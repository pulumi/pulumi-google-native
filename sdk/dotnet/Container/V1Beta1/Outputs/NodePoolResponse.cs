// *** WARNING: this file was generated by the Pulumi SDK Generator. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.GoogleNative.Container.V1Beta1.Outputs
{

    /// <summary>
    /// NodePool contains the name and configuration for a cluster's node pool. Node pools are a set of nodes (i.e. VM's), with a common configuration and specification, under the control of the cluster master. They may have a set of Kubernetes labels applied to them, which may be used to reference them during pod scheduling. They may also be resized up or down, to accommodate the workload. These upgrade settings control the level of parallelism and the level of disruption caused by an upgrade. maxUnavailable controls the number of nodes that can be simultaneously unavailable. maxSurge controls the number of additional nodes that can be added to the node pool temporarily for the time of the upgrade to increase the number of available nodes. (maxUnavailable + maxSurge) determines the level of parallelism (how many nodes are being upgraded at the same time). Note: upgrades inevitably introduce some disruption since workloads need to be moved from old nodes to new, upgraded ones. Even if maxUnavailable=0, this holds true. (Disruption stays within the limits of PodDisruptionBudget, if it is configured.) Consider a hypothetical node pool with 5 nodes having maxSurge=2, maxUnavailable=1. This means the upgrade process upgrades 3 nodes simultaneously. It creates 2 additional (upgraded) nodes, then it brings down 3 old (not yet upgraded) nodes at the same time. This ensures that there are always at least 4 nodes available.
    /// </summary>
    [OutputType]
    public sealed class NodePoolResponse
    {
        /// <summary>
        /// Autoscaler configuration for this NodePool. Autoscaler is enabled only if a valid configuration is present.
        /// </summary>
        public readonly Outputs.NodePoolAutoscalingResponse Autoscaling;
        /// <summary>
        /// Which conditions caused the current node pool state.
        /// </summary>
        public readonly ImmutableArray<Outputs.StatusConditionResponse> Conditions;
        /// <summary>
        /// The node configuration of the pool.
        /// </summary>
        public readonly Outputs.NodeConfigResponse Config;
        /// <summary>
        /// This checksum is computed by the server based on the value of node pool fields, and may be sent on update requests to ensure the client has an up-to-date value before proceeding.
        /// </summary>
        public readonly string Etag;
        /// <summary>
        /// The initial node count for the pool. You must ensure that your Compute Engine [resource quota](https://cloud.google.com/compute/quotas) is sufficient for this number of instances. You must also have available firewall and routes quota.
        /// </summary>
        public readonly int InitialNodeCount;
        /// <summary>
        /// [Output only] The resource URLs of the [managed instance groups](https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances) associated with this node pool. During the node pool blue-green upgrade operation, the URLs contain both blue and green resources.
        /// </summary>
        public readonly ImmutableArray<string> InstanceGroupUrls;
        /// <summary>
        /// The list of Google Compute Engine [zones](https://cloud.google.com/compute/docs/zones#available) in which the NodePool's nodes should be located. If this value is unspecified during node pool creation, the [Cluster.Locations](https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1/projects.locations.clusters#Cluster.FIELDS.locations) value will be used, instead. Warning: changing node pool locations will result in nodes being added and/or removed.
        /// </summary>
        public readonly ImmutableArray<string> Locations;
        /// <summary>
        /// NodeManagement configuration for this NodePool.
        /// </summary>
        public readonly Outputs.NodeManagementResponse Management;
        /// <summary>
        /// The constraint on the maximum number of pods that can be run simultaneously on a node in the node pool.
        /// </summary>
        public readonly Outputs.MaxPodsConstraintResponse MaxPodsConstraint;
        /// <summary>
        /// The name of the node pool.
        /// </summary>
        public readonly string Name;
        /// <summary>
        /// Networking configuration for this NodePool. If specified, it overrides the cluster-level defaults.
        /// </summary>
        public readonly Outputs.NodeNetworkConfigResponse NetworkConfig;
        /// <summary>
        /// Specifies the node placement policy.
        /// </summary>
        public readonly Outputs.PlacementPolicyResponse PlacementPolicy;
        /// <summary>
        /// [Output only] The pod CIDR block size per node in this node pool.
        /// </summary>
        public readonly int PodIpv4CidrSize;
        /// <summary>
        /// [Output only] Server-defined URL for the resource.
        /// </summary>
        public readonly string SelfLink;
        /// <summary>
        /// [Output only] The status of the nodes in this pool instance.
        /// </summary>
        public readonly string Status;
        /// <summary>
        /// [Output only] Deprecated. Use conditions instead. Additional information about the current status of this node pool instance, if available.
        /// </summary>
        public readonly string StatusMessage;
        /// <summary>
        /// [Output only] Update info contains relevant information during a node pool update.
        /// </summary>
        public readonly Outputs.UpdateInfoResponse UpdateInfo;
        /// <summary>
        /// Upgrade settings control disruption and speed of the upgrade.
        /// </summary>
        public readonly Outputs.UpgradeSettingsResponse UpgradeSettings;
        /// <summary>
        /// The version of the Kubernetes of this node.
        /// </summary>
        public readonly string Version;

        [OutputConstructor]
        private NodePoolResponse(
            Outputs.NodePoolAutoscalingResponse autoscaling,

            ImmutableArray<Outputs.StatusConditionResponse> conditions,

            Outputs.NodeConfigResponse config,

            string etag,

            int initialNodeCount,

            ImmutableArray<string> instanceGroupUrls,

            ImmutableArray<string> locations,

            Outputs.NodeManagementResponse management,

            Outputs.MaxPodsConstraintResponse maxPodsConstraint,

            string name,

            Outputs.NodeNetworkConfigResponse networkConfig,

            Outputs.PlacementPolicyResponse placementPolicy,

            int podIpv4CidrSize,

            string selfLink,

            string status,

            string statusMessage,

            Outputs.UpdateInfoResponse updateInfo,

            Outputs.UpgradeSettingsResponse upgradeSettings,

            string version)
        {
            Autoscaling = autoscaling;
            Conditions = conditions;
            Config = config;
            Etag = etag;
            InitialNodeCount = initialNodeCount;
            InstanceGroupUrls = instanceGroupUrls;
            Locations = locations;
            Management = management;
            MaxPodsConstraint = maxPodsConstraint;
            Name = name;
            NetworkConfig = networkConfig;
            PlacementPolicy = placementPolicy;
            PodIpv4CidrSize = podIpv4CidrSize;
            SelfLink = selfLink;
            Status = status;
            StatusMessage = statusMessage;
            UpdateInfo = updateInfo;
            UpgradeSettings = upgradeSettings;
            Version = version;
        }
    }
}
