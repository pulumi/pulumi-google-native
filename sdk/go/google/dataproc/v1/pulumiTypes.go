// Code generated by the Pulumi SDK Generator DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package v1

import (
	"context"
	"reflect"

	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// Specifies the type and number of accelerator cards attached to the instances of an instance. See GPUs on Compute Engine (https://cloud.google.com/compute/docs/gpus/).
type AcceleratorConfig struct {
	// The number of the accelerator cards of this type exposed to this instance.
	AcceleratorCount *int `pulumi:"acceleratorCount"`
	// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See Compute Engine AcceleratorTypes (https://cloud.google.com/compute/docs/reference/v1/acceleratorTypes).Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-k80 projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-k80 nvidia-tesla-k80Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
	AcceleratorTypeUri *string `pulumi:"acceleratorTypeUri"`
}

// AcceleratorConfigInput is an input type that accepts AcceleratorConfigArgs and AcceleratorConfigOutput values.
// You can construct a concrete instance of `AcceleratorConfigInput` via:
//
//	AcceleratorConfigArgs{...}
type AcceleratorConfigInput interface {
	pulumi.Input

	ToAcceleratorConfigOutput() AcceleratorConfigOutput
	ToAcceleratorConfigOutputWithContext(context.Context) AcceleratorConfigOutput
}

// Specifies the type and number of accelerator cards attached to the instances of an instance. See GPUs on Compute Engine (https://cloud.google.com/compute/docs/gpus/).
type AcceleratorConfigArgs struct {
	// The number of the accelerator cards of this type exposed to this instance.
	AcceleratorCount pulumi.IntPtrInput `pulumi:"acceleratorCount"`
	// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See Compute Engine AcceleratorTypes (https://cloud.google.com/compute/docs/reference/v1/acceleratorTypes).Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-k80 projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-k80 nvidia-tesla-k80Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
	AcceleratorTypeUri pulumi.StringPtrInput `pulumi:"acceleratorTypeUri"`
}

func (AcceleratorConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*AcceleratorConfig)(nil)).Elem()
}

func (i AcceleratorConfigArgs) ToAcceleratorConfigOutput() AcceleratorConfigOutput {
	return i.ToAcceleratorConfigOutputWithContext(context.Background())
}

func (i AcceleratorConfigArgs) ToAcceleratorConfigOutputWithContext(ctx context.Context) AcceleratorConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AcceleratorConfigOutput)
}

// AcceleratorConfigArrayInput is an input type that accepts AcceleratorConfigArray and AcceleratorConfigArrayOutput values.
// You can construct a concrete instance of `AcceleratorConfigArrayInput` via:
//
//	AcceleratorConfigArray{ AcceleratorConfigArgs{...} }
type AcceleratorConfigArrayInput interface {
	pulumi.Input

	ToAcceleratorConfigArrayOutput() AcceleratorConfigArrayOutput
	ToAcceleratorConfigArrayOutputWithContext(context.Context) AcceleratorConfigArrayOutput
}

type AcceleratorConfigArray []AcceleratorConfigInput

func (AcceleratorConfigArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]AcceleratorConfig)(nil)).Elem()
}

func (i AcceleratorConfigArray) ToAcceleratorConfigArrayOutput() AcceleratorConfigArrayOutput {
	return i.ToAcceleratorConfigArrayOutputWithContext(context.Background())
}

func (i AcceleratorConfigArray) ToAcceleratorConfigArrayOutputWithContext(ctx context.Context) AcceleratorConfigArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AcceleratorConfigArrayOutput)
}

// Specifies the type and number of accelerator cards attached to the instances of an instance. See GPUs on Compute Engine (https://cloud.google.com/compute/docs/gpus/).
type AcceleratorConfigOutput struct{ *pulumi.OutputState }

func (AcceleratorConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*AcceleratorConfig)(nil)).Elem()
}

func (o AcceleratorConfigOutput) ToAcceleratorConfigOutput() AcceleratorConfigOutput {
	return o
}

func (o AcceleratorConfigOutput) ToAcceleratorConfigOutputWithContext(ctx context.Context) AcceleratorConfigOutput {
	return o
}

// The number of the accelerator cards of this type exposed to this instance.
func (o AcceleratorConfigOutput) AcceleratorCount() pulumi.IntPtrOutput {
	return o.ApplyT(func(v AcceleratorConfig) *int { return v.AcceleratorCount }).(pulumi.IntPtrOutput)
}

// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See Compute Engine AcceleratorTypes (https://cloud.google.com/compute/docs/reference/v1/acceleratorTypes).Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-k80 projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-k80 nvidia-tesla-k80Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
func (o AcceleratorConfigOutput) AcceleratorTypeUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v AcceleratorConfig) *string { return v.AcceleratorTypeUri }).(pulumi.StringPtrOutput)
}

type AcceleratorConfigArrayOutput struct{ *pulumi.OutputState }

func (AcceleratorConfigArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]AcceleratorConfig)(nil)).Elem()
}

func (o AcceleratorConfigArrayOutput) ToAcceleratorConfigArrayOutput() AcceleratorConfigArrayOutput {
	return o
}

func (o AcceleratorConfigArrayOutput) ToAcceleratorConfigArrayOutputWithContext(ctx context.Context) AcceleratorConfigArrayOutput {
	return o
}

func (o AcceleratorConfigArrayOutput) Index(i pulumi.IntInput) AcceleratorConfigOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) AcceleratorConfig {
		return vs[0].([]AcceleratorConfig)[vs[1].(int)]
	}).(AcceleratorConfigOutput)
}

// Specifies the type and number of accelerator cards attached to the instances of an instance. See GPUs on Compute Engine (https://cloud.google.com/compute/docs/gpus/).
type AcceleratorConfigResponse struct {
	// The number of the accelerator cards of this type exposed to this instance.
	AcceleratorCount int `pulumi:"acceleratorCount"`
	// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See Compute Engine AcceleratorTypes (https://cloud.google.com/compute/docs/reference/v1/acceleratorTypes).Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-k80 projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-k80 nvidia-tesla-k80Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
	AcceleratorTypeUri string `pulumi:"acceleratorTypeUri"`
}

// Specifies the type and number of accelerator cards attached to the instances of an instance. See GPUs on Compute Engine (https://cloud.google.com/compute/docs/gpus/).
type AcceleratorConfigResponseOutput struct{ *pulumi.OutputState }

func (AcceleratorConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*AcceleratorConfigResponse)(nil)).Elem()
}

func (o AcceleratorConfigResponseOutput) ToAcceleratorConfigResponseOutput() AcceleratorConfigResponseOutput {
	return o
}

func (o AcceleratorConfigResponseOutput) ToAcceleratorConfigResponseOutputWithContext(ctx context.Context) AcceleratorConfigResponseOutput {
	return o
}

// The number of the accelerator cards of this type exposed to this instance.
func (o AcceleratorConfigResponseOutput) AcceleratorCount() pulumi.IntOutput {
	return o.ApplyT(func(v AcceleratorConfigResponse) int { return v.AcceleratorCount }).(pulumi.IntOutput)
}

// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See Compute Engine AcceleratorTypes (https://cloud.google.com/compute/docs/reference/v1/acceleratorTypes).Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-k80 projects/[project_id]/zones/[zone]/acceleratorTypes/nvidia-tesla-k80 nvidia-tesla-k80Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
func (o AcceleratorConfigResponseOutput) AcceleratorTypeUri() pulumi.StringOutput {
	return o.ApplyT(func(v AcceleratorConfigResponse) string { return v.AcceleratorTypeUri }).(pulumi.StringOutput)
}

type AcceleratorConfigResponseArrayOutput struct{ *pulumi.OutputState }

func (AcceleratorConfigResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]AcceleratorConfigResponse)(nil)).Elem()
}

func (o AcceleratorConfigResponseArrayOutput) ToAcceleratorConfigResponseArrayOutput() AcceleratorConfigResponseArrayOutput {
	return o
}

func (o AcceleratorConfigResponseArrayOutput) ToAcceleratorConfigResponseArrayOutputWithContext(ctx context.Context) AcceleratorConfigResponseArrayOutput {
	return o
}

func (o AcceleratorConfigResponseArrayOutput) Index(i pulumi.IntInput) AcceleratorConfigResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) AcceleratorConfigResponse {
		return vs[0].([]AcceleratorConfigResponse)[vs[1].(int)]
	}).(AcceleratorConfigResponseOutput)
}

// Autoscaling Policy config associated with the cluster.
type AutoscalingConfig struct {
	// Optional. The autoscaling policy used by the cluster.Only resource names including projectid and location (region) are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id] projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]Note that the policy must be in the same project and Dataproc region.
	PolicyUri *string `pulumi:"policyUri"`
}

// AutoscalingConfigInput is an input type that accepts AutoscalingConfigArgs and AutoscalingConfigOutput values.
// You can construct a concrete instance of `AutoscalingConfigInput` via:
//
//	AutoscalingConfigArgs{...}
type AutoscalingConfigInput interface {
	pulumi.Input

	ToAutoscalingConfigOutput() AutoscalingConfigOutput
	ToAutoscalingConfigOutputWithContext(context.Context) AutoscalingConfigOutput
}

// Autoscaling Policy config associated with the cluster.
type AutoscalingConfigArgs struct {
	// Optional. The autoscaling policy used by the cluster.Only resource names including projectid and location (region) are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id] projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]Note that the policy must be in the same project and Dataproc region.
	PolicyUri pulumi.StringPtrInput `pulumi:"policyUri"`
}

func (AutoscalingConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*AutoscalingConfig)(nil)).Elem()
}

func (i AutoscalingConfigArgs) ToAutoscalingConfigOutput() AutoscalingConfigOutput {
	return i.ToAutoscalingConfigOutputWithContext(context.Background())
}

func (i AutoscalingConfigArgs) ToAutoscalingConfigOutputWithContext(ctx context.Context) AutoscalingConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AutoscalingConfigOutput)
}

func (i AutoscalingConfigArgs) ToAutoscalingConfigPtrOutput() AutoscalingConfigPtrOutput {
	return i.ToAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i AutoscalingConfigArgs) ToAutoscalingConfigPtrOutputWithContext(ctx context.Context) AutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AutoscalingConfigOutput).ToAutoscalingConfigPtrOutputWithContext(ctx)
}

// AutoscalingConfigPtrInput is an input type that accepts AutoscalingConfigArgs, AutoscalingConfigPtr and AutoscalingConfigPtrOutput values.
// You can construct a concrete instance of `AutoscalingConfigPtrInput` via:
//
//	        AutoscalingConfigArgs{...}
//
//	or:
//
//	        nil
type AutoscalingConfigPtrInput interface {
	pulumi.Input

	ToAutoscalingConfigPtrOutput() AutoscalingConfigPtrOutput
	ToAutoscalingConfigPtrOutputWithContext(context.Context) AutoscalingConfigPtrOutput
}

type autoscalingConfigPtrType AutoscalingConfigArgs

func AutoscalingConfigPtr(v *AutoscalingConfigArgs) AutoscalingConfigPtrInput {
	return (*autoscalingConfigPtrType)(v)
}

func (*autoscalingConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**AutoscalingConfig)(nil)).Elem()
}

func (i *autoscalingConfigPtrType) ToAutoscalingConfigPtrOutput() AutoscalingConfigPtrOutput {
	return i.ToAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i *autoscalingConfigPtrType) ToAutoscalingConfigPtrOutputWithContext(ctx context.Context) AutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AutoscalingConfigPtrOutput)
}

// Autoscaling Policy config associated with the cluster.
type AutoscalingConfigOutput struct{ *pulumi.OutputState }

func (AutoscalingConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*AutoscalingConfig)(nil)).Elem()
}

func (o AutoscalingConfigOutput) ToAutoscalingConfigOutput() AutoscalingConfigOutput {
	return o
}

func (o AutoscalingConfigOutput) ToAutoscalingConfigOutputWithContext(ctx context.Context) AutoscalingConfigOutput {
	return o
}

func (o AutoscalingConfigOutput) ToAutoscalingConfigPtrOutput() AutoscalingConfigPtrOutput {
	return o.ToAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (o AutoscalingConfigOutput) ToAutoscalingConfigPtrOutputWithContext(ctx context.Context) AutoscalingConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v AutoscalingConfig) *AutoscalingConfig {
		return &v
	}).(AutoscalingConfigPtrOutput)
}

// Optional. The autoscaling policy used by the cluster.Only resource names including projectid and location (region) are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id] projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]Note that the policy must be in the same project and Dataproc region.
func (o AutoscalingConfigOutput) PolicyUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v AutoscalingConfig) *string { return v.PolicyUri }).(pulumi.StringPtrOutput)
}

type AutoscalingConfigPtrOutput struct{ *pulumi.OutputState }

func (AutoscalingConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**AutoscalingConfig)(nil)).Elem()
}

func (o AutoscalingConfigPtrOutput) ToAutoscalingConfigPtrOutput() AutoscalingConfigPtrOutput {
	return o
}

func (o AutoscalingConfigPtrOutput) ToAutoscalingConfigPtrOutputWithContext(ctx context.Context) AutoscalingConfigPtrOutput {
	return o
}

func (o AutoscalingConfigPtrOutput) Elem() AutoscalingConfigOutput {
	return o.ApplyT(func(v *AutoscalingConfig) AutoscalingConfig {
		if v != nil {
			return *v
		}
		var ret AutoscalingConfig
		return ret
	}).(AutoscalingConfigOutput)
}

// Optional. The autoscaling policy used by the cluster.Only resource names including projectid and location (region) are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id] projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]Note that the policy must be in the same project and Dataproc region.
func (o AutoscalingConfigPtrOutput) PolicyUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *AutoscalingConfig) *string {
		if v == nil {
			return nil
		}
		return v.PolicyUri
	}).(pulumi.StringPtrOutput)
}

// Autoscaling Policy config associated with the cluster.
type AutoscalingConfigResponse struct {
	// Optional. The autoscaling policy used by the cluster.Only resource names including projectid and location (region) are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id] projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]Note that the policy must be in the same project and Dataproc region.
	PolicyUri string `pulumi:"policyUri"`
}

// Autoscaling Policy config associated with the cluster.
type AutoscalingConfigResponseOutput struct{ *pulumi.OutputState }

func (AutoscalingConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*AutoscalingConfigResponse)(nil)).Elem()
}

func (o AutoscalingConfigResponseOutput) ToAutoscalingConfigResponseOutput() AutoscalingConfigResponseOutput {
	return o
}

func (o AutoscalingConfigResponseOutput) ToAutoscalingConfigResponseOutputWithContext(ctx context.Context) AutoscalingConfigResponseOutput {
	return o
}

// Optional. The autoscaling policy used by the cluster.Only resource names including projectid and location (region) are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id] projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]Note that the policy must be in the same project and Dataproc region.
func (o AutoscalingConfigResponseOutput) PolicyUri() pulumi.StringOutput {
	return o.ApplyT(func(v AutoscalingConfigResponse) string { return v.PolicyUri }).(pulumi.StringOutput)
}

// Node group identification and configuration information.
type AuxiliaryNodeGroup struct {
	// Node group configuration.
	NodeGroup NodeGroupType `pulumi:"nodeGroup"`
	// Optional. A node group ID. Generated if not specified.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of from 3 to 33 characters.
	NodeGroupId *string `pulumi:"nodeGroupId"`
}

// AuxiliaryNodeGroupInput is an input type that accepts AuxiliaryNodeGroupArgs and AuxiliaryNodeGroupOutput values.
// You can construct a concrete instance of `AuxiliaryNodeGroupInput` via:
//
//	AuxiliaryNodeGroupArgs{...}
type AuxiliaryNodeGroupInput interface {
	pulumi.Input

	ToAuxiliaryNodeGroupOutput() AuxiliaryNodeGroupOutput
	ToAuxiliaryNodeGroupOutputWithContext(context.Context) AuxiliaryNodeGroupOutput
}

// Node group identification and configuration information.
type AuxiliaryNodeGroupArgs struct {
	// Node group configuration.
	NodeGroup NodeGroupTypeInput `pulumi:"nodeGroup"`
	// Optional. A node group ID. Generated if not specified.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of from 3 to 33 characters.
	NodeGroupId pulumi.StringPtrInput `pulumi:"nodeGroupId"`
}

func (AuxiliaryNodeGroupArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*AuxiliaryNodeGroup)(nil)).Elem()
}

func (i AuxiliaryNodeGroupArgs) ToAuxiliaryNodeGroupOutput() AuxiliaryNodeGroupOutput {
	return i.ToAuxiliaryNodeGroupOutputWithContext(context.Background())
}

func (i AuxiliaryNodeGroupArgs) ToAuxiliaryNodeGroupOutputWithContext(ctx context.Context) AuxiliaryNodeGroupOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AuxiliaryNodeGroupOutput)
}

// AuxiliaryNodeGroupArrayInput is an input type that accepts AuxiliaryNodeGroupArray and AuxiliaryNodeGroupArrayOutput values.
// You can construct a concrete instance of `AuxiliaryNodeGroupArrayInput` via:
//
//	AuxiliaryNodeGroupArray{ AuxiliaryNodeGroupArgs{...} }
type AuxiliaryNodeGroupArrayInput interface {
	pulumi.Input

	ToAuxiliaryNodeGroupArrayOutput() AuxiliaryNodeGroupArrayOutput
	ToAuxiliaryNodeGroupArrayOutputWithContext(context.Context) AuxiliaryNodeGroupArrayOutput
}

type AuxiliaryNodeGroupArray []AuxiliaryNodeGroupInput

func (AuxiliaryNodeGroupArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]AuxiliaryNodeGroup)(nil)).Elem()
}

func (i AuxiliaryNodeGroupArray) ToAuxiliaryNodeGroupArrayOutput() AuxiliaryNodeGroupArrayOutput {
	return i.ToAuxiliaryNodeGroupArrayOutputWithContext(context.Background())
}

func (i AuxiliaryNodeGroupArray) ToAuxiliaryNodeGroupArrayOutputWithContext(ctx context.Context) AuxiliaryNodeGroupArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AuxiliaryNodeGroupArrayOutput)
}

// Node group identification and configuration information.
type AuxiliaryNodeGroupOutput struct{ *pulumi.OutputState }

func (AuxiliaryNodeGroupOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*AuxiliaryNodeGroup)(nil)).Elem()
}

func (o AuxiliaryNodeGroupOutput) ToAuxiliaryNodeGroupOutput() AuxiliaryNodeGroupOutput {
	return o
}

func (o AuxiliaryNodeGroupOutput) ToAuxiliaryNodeGroupOutputWithContext(ctx context.Context) AuxiliaryNodeGroupOutput {
	return o
}

// Node group configuration.
func (o AuxiliaryNodeGroupOutput) NodeGroup() NodeGroupTypeOutput {
	return o.ApplyT(func(v AuxiliaryNodeGroup) NodeGroupType { return v.NodeGroup }).(NodeGroupTypeOutput)
}

// Optional. A node group ID. Generated if not specified.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of from 3 to 33 characters.
func (o AuxiliaryNodeGroupOutput) NodeGroupId() pulumi.StringPtrOutput {
	return o.ApplyT(func(v AuxiliaryNodeGroup) *string { return v.NodeGroupId }).(pulumi.StringPtrOutput)
}

type AuxiliaryNodeGroupArrayOutput struct{ *pulumi.OutputState }

func (AuxiliaryNodeGroupArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]AuxiliaryNodeGroup)(nil)).Elem()
}

func (o AuxiliaryNodeGroupArrayOutput) ToAuxiliaryNodeGroupArrayOutput() AuxiliaryNodeGroupArrayOutput {
	return o
}

func (o AuxiliaryNodeGroupArrayOutput) ToAuxiliaryNodeGroupArrayOutputWithContext(ctx context.Context) AuxiliaryNodeGroupArrayOutput {
	return o
}

func (o AuxiliaryNodeGroupArrayOutput) Index(i pulumi.IntInput) AuxiliaryNodeGroupOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) AuxiliaryNodeGroup {
		return vs[0].([]AuxiliaryNodeGroup)[vs[1].(int)]
	}).(AuxiliaryNodeGroupOutput)
}

// Node group identification and configuration information.
type AuxiliaryNodeGroupResponse struct {
	// Node group configuration.
	NodeGroup NodeGroupResponse `pulumi:"nodeGroup"`
	// Optional. A node group ID. Generated if not specified.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of from 3 to 33 characters.
	NodeGroupId string `pulumi:"nodeGroupId"`
}

// Node group identification and configuration information.
type AuxiliaryNodeGroupResponseOutput struct{ *pulumi.OutputState }

func (AuxiliaryNodeGroupResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*AuxiliaryNodeGroupResponse)(nil)).Elem()
}

func (o AuxiliaryNodeGroupResponseOutput) ToAuxiliaryNodeGroupResponseOutput() AuxiliaryNodeGroupResponseOutput {
	return o
}

func (o AuxiliaryNodeGroupResponseOutput) ToAuxiliaryNodeGroupResponseOutputWithContext(ctx context.Context) AuxiliaryNodeGroupResponseOutput {
	return o
}

// Node group configuration.
func (o AuxiliaryNodeGroupResponseOutput) NodeGroup() NodeGroupResponseOutput {
	return o.ApplyT(func(v AuxiliaryNodeGroupResponse) NodeGroupResponse { return v.NodeGroup }).(NodeGroupResponseOutput)
}

// Optional. A node group ID. Generated if not specified.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of from 3 to 33 characters.
func (o AuxiliaryNodeGroupResponseOutput) NodeGroupId() pulumi.StringOutput {
	return o.ApplyT(func(v AuxiliaryNodeGroupResponse) string { return v.NodeGroupId }).(pulumi.StringOutput)
}

type AuxiliaryNodeGroupResponseArrayOutput struct{ *pulumi.OutputState }

func (AuxiliaryNodeGroupResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]AuxiliaryNodeGroupResponse)(nil)).Elem()
}

func (o AuxiliaryNodeGroupResponseArrayOutput) ToAuxiliaryNodeGroupResponseArrayOutput() AuxiliaryNodeGroupResponseArrayOutput {
	return o
}

func (o AuxiliaryNodeGroupResponseArrayOutput) ToAuxiliaryNodeGroupResponseArrayOutputWithContext(ctx context.Context) AuxiliaryNodeGroupResponseArrayOutput {
	return o
}

func (o AuxiliaryNodeGroupResponseArrayOutput) Index(i pulumi.IntInput) AuxiliaryNodeGroupResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) AuxiliaryNodeGroupResponse {
		return vs[0].([]AuxiliaryNodeGroupResponse)[vs[1].(int)]
	}).(AuxiliaryNodeGroupResponseOutput)
}

// Auxiliary services configuration for a Cluster.
type AuxiliaryServicesConfig struct {
	// Optional. The Hive Metastore configuration for this workload.
	MetastoreConfig *MetastoreConfig `pulumi:"metastoreConfig"`
	// Optional. The Spark History Server configuration for the workload.
	SparkHistoryServerConfig *SparkHistoryServerConfig `pulumi:"sparkHistoryServerConfig"`
}

// AuxiliaryServicesConfigInput is an input type that accepts AuxiliaryServicesConfigArgs and AuxiliaryServicesConfigOutput values.
// You can construct a concrete instance of `AuxiliaryServicesConfigInput` via:
//
//	AuxiliaryServicesConfigArgs{...}
type AuxiliaryServicesConfigInput interface {
	pulumi.Input

	ToAuxiliaryServicesConfigOutput() AuxiliaryServicesConfigOutput
	ToAuxiliaryServicesConfigOutputWithContext(context.Context) AuxiliaryServicesConfigOutput
}

// Auxiliary services configuration for a Cluster.
type AuxiliaryServicesConfigArgs struct {
	// Optional. The Hive Metastore configuration for this workload.
	MetastoreConfig MetastoreConfigPtrInput `pulumi:"metastoreConfig"`
	// Optional. The Spark History Server configuration for the workload.
	SparkHistoryServerConfig SparkHistoryServerConfigPtrInput `pulumi:"sparkHistoryServerConfig"`
}

func (AuxiliaryServicesConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*AuxiliaryServicesConfig)(nil)).Elem()
}

func (i AuxiliaryServicesConfigArgs) ToAuxiliaryServicesConfigOutput() AuxiliaryServicesConfigOutput {
	return i.ToAuxiliaryServicesConfigOutputWithContext(context.Background())
}

func (i AuxiliaryServicesConfigArgs) ToAuxiliaryServicesConfigOutputWithContext(ctx context.Context) AuxiliaryServicesConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AuxiliaryServicesConfigOutput)
}

func (i AuxiliaryServicesConfigArgs) ToAuxiliaryServicesConfigPtrOutput() AuxiliaryServicesConfigPtrOutput {
	return i.ToAuxiliaryServicesConfigPtrOutputWithContext(context.Background())
}

func (i AuxiliaryServicesConfigArgs) ToAuxiliaryServicesConfigPtrOutputWithContext(ctx context.Context) AuxiliaryServicesConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AuxiliaryServicesConfigOutput).ToAuxiliaryServicesConfigPtrOutputWithContext(ctx)
}

// AuxiliaryServicesConfigPtrInput is an input type that accepts AuxiliaryServicesConfigArgs, AuxiliaryServicesConfigPtr and AuxiliaryServicesConfigPtrOutput values.
// You can construct a concrete instance of `AuxiliaryServicesConfigPtrInput` via:
//
//	        AuxiliaryServicesConfigArgs{...}
//
//	or:
//
//	        nil
type AuxiliaryServicesConfigPtrInput interface {
	pulumi.Input

	ToAuxiliaryServicesConfigPtrOutput() AuxiliaryServicesConfigPtrOutput
	ToAuxiliaryServicesConfigPtrOutputWithContext(context.Context) AuxiliaryServicesConfigPtrOutput
}

type auxiliaryServicesConfigPtrType AuxiliaryServicesConfigArgs

func AuxiliaryServicesConfigPtr(v *AuxiliaryServicesConfigArgs) AuxiliaryServicesConfigPtrInput {
	return (*auxiliaryServicesConfigPtrType)(v)
}

func (*auxiliaryServicesConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**AuxiliaryServicesConfig)(nil)).Elem()
}

func (i *auxiliaryServicesConfigPtrType) ToAuxiliaryServicesConfigPtrOutput() AuxiliaryServicesConfigPtrOutput {
	return i.ToAuxiliaryServicesConfigPtrOutputWithContext(context.Background())
}

func (i *auxiliaryServicesConfigPtrType) ToAuxiliaryServicesConfigPtrOutputWithContext(ctx context.Context) AuxiliaryServicesConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AuxiliaryServicesConfigPtrOutput)
}

// Auxiliary services configuration for a Cluster.
type AuxiliaryServicesConfigOutput struct{ *pulumi.OutputState }

func (AuxiliaryServicesConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*AuxiliaryServicesConfig)(nil)).Elem()
}

func (o AuxiliaryServicesConfigOutput) ToAuxiliaryServicesConfigOutput() AuxiliaryServicesConfigOutput {
	return o
}

func (o AuxiliaryServicesConfigOutput) ToAuxiliaryServicesConfigOutputWithContext(ctx context.Context) AuxiliaryServicesConfigOutput {
	return o
}

func (o AuxiliaryServicesConfigOutput) ToAuxiliaryServicesConfigPtrOutput() AuxiliaryServicesConfigPtrOutput {
	return o.ToAuxiliaryServicesConfigPtrOutputWithContext(context.Background())
}

func (o AuxiliaryServicesConfigOutput) ToAuxiliaryServicesConfigPtrOutputWithContext(ctx context.Context) AuxiliaryServicesConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v AuxiliaryServicesConfig) *AuxiliaryServicesConfig {
		return &v
	}).(AuxiliaryServicesConfigPtrOutput)
}

// Optional. The Hive Metastore configuration for this workload.
func (o AuxiliaryServicesConfigOutput) MetastoreConfig() MetastoreConfigPtrOutput {
	return o.ApplyT(func(v AuxiliaryServicesConfig) *MetastoreConfig { return v.MetastoreConfig }).(MetastoreConfigPtrOutput)
}

// Optional. The Spark History Server configuration for the workload.
func (o AuxiliaryServicesConfigOutput) SparkHistoryServerConfig() SparkHistoryServerConfigPtrOutput {
	return o.ApplyT(func(v AuxiliaryServicesConfig) *SparkHistoryServerConfig { return v.SparkHistoryServerConfig }).(SparkHistoryServerConfigPtrOutput)
}

type AuxiliaryServicesConfigPtrOutput struct{ *pulumi.OutputState }

func (AuxiliaryServicesConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**AuxiliaryServicesConfig)(nil)).Elem()
}

func (o AuxiliaryServicesConfigPtrOutput) ToAuxiliaryServicesConfigPtrOutput() AuxiliaryServicesConfigPtrOutput {
	return o
}

func (o AuxiliaryServicesConfigPtrOutput) ToAuxiliaryServicesConfigPtrOutputWithContext(ctx context.Context) AuxiliaryServicesConfigPtrOutput {
	return o
}

func (o AuxiliaryServicesConfigPtrOutput) Elem() AuxiliaryServicesConfigOutput {
	return o.ApplyT(func(v *AuxiliaryServicesConfig) AuxiliaryServicesConfig {
		if v != nil {
			return *v
		}
		var ret AuxiliaryServicesConfig
		return ret
	}).(AuxiliaryServicesConfigOutput)
}

// Optional. The Hive Metastore configuration for this workload.
func (o AuxiliaryServicesConfigPtrOutput) MetastoreConfig() MetastoreConfigPtrOutput {
	return o.ApplyT(func(v *AuxiliaryServicesConfig) *MetastoreConfig {
		if v == nil {
			return nil
		}
		return v.MetastoreConfig
	}).(MetastoreConfigPtrOutput)
}

// Optional. The Spark History Server configuration for the workload.
func (o AuxiliaryServicesConfigPtrOutput) SparkHistoryServerConfig() SparkHistoryServerConfigPtrOutput {
	return o.ApplyT(func(v *AuxiliaryServicesConfig) *SparkHistoryServerConfig {
		if v == nil {
			return nil
		}
		return v.SparkHistoryServerConfig
	}).(SparkHistoryServerConfigPtrOutput)
}

// Auxiliary services configuration for a Cluster.
type AuxiliaryServicesConfigResponse struct {
	// Optional. The Hive Metastore configuration for this workload.
	MetastoreConfig MetastoreConfigResponse `pulumi:"metastoreConfig"`
	// Optional. The Spark History Server configuration for the workload.
	SparkHistoryServerConfig SparkHistoryServerConfigResponse `pulumi:"sparkHistoryServerConfig"`
}

// Auxiliary services configuration for a Cluster.
type AuxiliaryServicesConfigResponseOutput struct{ *pulumi.OutputState }

func (AuxiliaryServicesConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*AuxiliaryServicesConfigResponse)(nil)).Elem()
}

func (o AuxiliaryServicesConfigResponseOutput) ToAuxiliaryServicesConfigResponseOutput() AuxiliaryServicesConfigResponseOutput {
	return o
}

func (o AuxiliaryServicesConfigResponseOutput) ToAuxiliaryServicesConfigResponseOutputWithContext(ctx context.Context) AuxiliaryServicesConfigResponseOutput {
	return o
}

// Optional. The Hive Metastore configuration for this workload.
func (o AuxiliaryServicesConfigResponseOutput) MetastoreConfig() MetastoreConfigResponseOutput {
	return o.ApplyT(func(v AuxiliaryServicesConfigResponse) MetastoreConfigResponse { return v.MetastoreConfig }).(MetastoreConfigResponseOutput)
}

// Optional. The Spark History Server configuration for the workload.
func (o AuxiliaryServicesConfigResponseOutput) SparkHistoryServerConfig() SparkHistoryServerConfigResponseOutput {
	return o.ApplyT(func(v AuxiliaryServicesConfigResponse) SparkHistoryServerConfigResponse {
		return v.SparkHistoryServerConfig
	}).(SparkHistoryServerConfigResponseOutput)
}

// Basic algorithm for autoscaling.
type BasicAutoscalingAlgorithm struct {
	// Optional. Duration between scaling events. A scaling period starts after the update operation from the previous event has completed.Bounds: 2m, 1d. Default: 2m.
	CooldownPeriod *string `pulumi:"cooldownPeriod"`
	// Optional. Spark Standalone autoscaling configuration
	SparkStandaloneConfig *SparkStandaloneAutoscalingConfig `pulumi:"sparkStandaloneConfig"`
	// Optional. YARN autoscaling configuration.
	YarnConfig *BasicYarnAutoscalingConfig `pulumi:"yarnConfig"`
}

// BasicAutoscalingAlgorithmInput is an input type that accepts BasicAutoscalingAlgorithmArgs and BasicAutoscalingAlgorithmOutput values.
// You can construct a concrete instance of `BasicAutoscalingAlgorithmInput` via:
//
//	BasicAutoscalingAlgorithmArgs{...}
type BasicAutoscalingAlgorithmInput interface {
	pulumi.Input

	ToBasicAutoscalingAlgorithmOutput() BasicAutoscalingAlgorithmOutput
	ToBasicAutoscalingAlgorithmOutputWithContext(context.Context) BasicAutoscalingAlgorithmOutput
}

// Basic algorithm for autoscaling.
type BasicAutoscalingAlgorithmArgs struct {
	// Optional. Duration between scaling events. A scaling period starts after the update operation from the previous event has completed.Bounds: 2m, 1d. Default: 2m.
	CooldownPeriod pulumi.StringPtrInput `pulumi:"cooldownPeriod"`
	// Optional. Spark Standalone autoscaling configuration
	SparkStandaloneConfig SparkStandaloneAutoscalingConfigPtrInput `pulumi:"sparkStandaloneConfig"`
	// Optional. YARN autoscaling configuration.
	YarnConfig BasicYarnAutoscalingConfigPtrInput `pulumi:"yarnConfig"`
}

func (BasicAutoscalingAlgorithmArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*BasicAutoscalingAlgorithm)(nil)).Elem()
}

func (i BasicAutoscalingAlgorithmArgs) ToBasicAutoscalingAlgorithmOutput() BasicAutoscalingAlgorithmOutput {
	return i.ToBasicAutoscalingAlgorithmOutputWithContext(context.Background())
}

func (i BasicAutoscalingAlgorithmArgs) ToBasicAutoscalingAlgorithmOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BasicAutoscalingAlgorithmOutput)
}

func (i BasicAutoscalingAlgorithmArgs) ToBasicAutoscalingAlgorithmPtrOutput() BasicAutoscalingAlgorithmPtrOutput {
	return i.ToBasicAutoscalingAlgorithmPtrOutputWithContext(context.Background())
}

func (i BasicAutoscalingAlgorithmArgs) ToBasicAutoscalingAlgorithmPtrOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BasicAutoscalingAlgorithmOutput).ToBasicAutoscalingAlgorithmPtrOutputWithContext(ctx)
}

// BasicAutoscalingAlgorithmPtrInput is an input type that accepts BasicAutoscalingAlgorithmArgs, BasicAutoscalingAlgorithmPtr and BasicAutoscalingAlgorithmPtrOutput values.
// You can construct a concrete instance of `BasicAutoscalingAlgorithmPtrInput` via:
//
//	        BasicAutoscalingAlgorithmArgs{...}
//
//	or:
//
//	        nil
type BasicAutoscalingAlgorithmPtrInput interface {
	pulumi.Input

	ToBasicAutoscalingAlgorithmPtrOutput() BasicAutoscalingAlgorithmPtrOutput
	ToBasicAutoscalingAlgorithmPtrOutputWithContext(context.Context) BasicAutoscalingAlgorithmPtrOutput
}

type basicAutoscalingAlgorithmPtrType BasicAutoscalingAlgorithmArgs

func BasicAutoscalingAlgorithmPtr(v *BasicAutoscalingAlgorithmArgs) BasicAutoscalingAlgorithmPtrInput {
	return (*basicAutoscalingAlgorithmPtrType)(v)
}

func (*basicAutoscalingAlgorithmPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**BasicAutoscalingAlgorithm)(nil)).Elem()
}

func (i *basicAutoscalingAlgorithmPtrType) ToBasicAutoscalingAlgorithmPtrOutput() BasicAutoscalingAlgorithmPtrOutput {
	return i.ToBasicAutoscalingAlgorithmPtrOutputWithContext(context.Background())
}

func (i *basicAutoscalingAlgorithmPtrType) ToBasicAutoscalingAlgorithmPtrOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BasicAutoscalingAlgorithmPtrOutput)
}

// Basic algorithm for autoscaling.
type BasicAutoscalingAlgorithmOutput struct{ *pulumi.OutputState }

func (BasicAutoscalingAlgorithmOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*BasicAutoscalingAlgorithm)(nil)).Elem()
}

func (o BasicAutoscalingAlgorithmOutput) ToBasicAutoscalingAlgorithmOutput() BasicAutoscalingAlgorithmOutput {
	return o
}

func (o BasicAutoscalingAlgorithmOutput) ToBasicAutoscalingAlgorithmOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmOutput {
	return o
}

func (o BasicAutoscalingAlgorithmOutput) ToBasicAutoscalingAlgorithmPtrOutput() BasicAutoscalingAlgorithmPtrOutput {
	return o.ToBasicAutoscalingAlgorithmPtrOutputWithContext(context.Background())
}

func (o BasicAutoscalingAlgorithmOutput) ToBasicAutoscalingAlgorithmPtrOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v BasicAutoscalingAlgorithm) *BasicAutoscalingAlgorithm {
		return &v
	}).(BasicAutoscalingAlgorithmPtrOutput)
}

// Optional. Duration between scaling events. A scaling period starts after the update operation from the previous event has completed.Bounds: 2m, 1d. Default: 2m.
func (o BasicAutoscalingAlgorithmOutput) CooldownPeriod() pulumi.StringPtrOutput {
	return o.ApplyT(func(v BasicAutoscalingAlgorithm) *string { return v.CooldownPeriod }).(pulumi.StringPtrOutput)
}

// Optional. Spark Standalone autoscaling configuration
func (o BasicAutoscalingAlgorithmOutput) SparkStandaloneConfig() SparkStandaloneAutoscalingConfigPtrOutput {
	return o.ApplyT(func(v BasicAutoscalingAlgorithm) *SparkStandaloneAutoscalingConfig { return v.SparkStandaloneConfig }).(SparkStandaloneAutoscalingConfigPtrOutput)
}

// Optional. YARN autoscaling configuration.
func (o BasicAutoscalingAlgorithmOutput) YarnConfig() BasicYarnAutoscalingConfigPtrOutput {
	return o.ApplyT(func(v BasicAutoscalingAlgorithm) *BasicYarnAutoscalingConfig { return v.YarnConfig }).(BasicYarnAutoscalingConfigPtrOutput)
}

type BasicAutoscalingAlgorithmPtrOutput struct{ *pulumi.OutputState }

func (BasicAutoscalingAlgorithmPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**BasicAutoscalingAlgorithm)(nil)).Elem()
}

func (o BasicAutoscalingAlgorithmPtrOutput) ToBasicAutoscalingAlgorithmPtrOutput() BasicAutoscalingAlgorithmPtrOutput {
	return o
}

func (o BasicAutoscalingAlgorithmPtrOutput) ToBasicAutoscalingAlgorithmPtrOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmPtrOutput {
	return o
}

func (o BasicAutoscalingAlgorithmPtrOutput) Elem() BasicAutoscalingAlgorithmOutput {
	return o.ApplyT(func(v *BasicAutoscalingAlgorithm) BasicAutoscalingAlgorithm {
		if v != nil {
			return *v
		}
		var ret BasicAutoscalingAlgorithm
		return ret
	}).(BasicAutoscalingAlgorithmOutput)
}

// Optional. Duration between scaling events. A scaling period starts after the update operation from the previous event has completed.Bounds: 2m, 1d. Default: 2m.
func (o BasicAutoscalingAlgorithmPtrOutput) CooldownPeriod() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *BasicAutoscalingAlgorithm) *string {
		if v == nil {
			return nil
		}
		return v.CooldownPeriod
	}).(pulumi.StringPtrOutput)
}

// Optional. Spark Standalone autoscaling configuration
func (o BasicAutoscalingAlgorithmPtrOutput) SparkStandaloneConfig() SparkStandaloneAutoscalingConfigPtrOutput {
	return o.ApplyT(func(v *BasicAutoscalingAlgorithm) *SparkStandaloneAutoscalingConfig {
		if v == nil {
			return nil
		}
		return v.SparkStandaloneConfig
	}).(SparkStandaloneAutoscalingConfigPtrOutput)
}

// Optional. YARN autoscaling configuration.
func (o BasicAutoscalingAlgorithmPtrOutput) YarnConfig() BasicYarnAutoscalingConfigPtrOutput {
	return o.ApplyT(func(v *BasicAutoscalingAlgorithm) *BasicYarnAutoscalingConfig {
		if v == nil {
			return nil
		}
		return v.YarnConfig
	}).(BasicYarnAutoscalingConfigPtrOutput)
}

// Basic algorithm for autoscaling.
type BasicAutoscalingAlgorithmResponse struct {
	// Optional. Duration between scaling events. A scaling period starts after the update operation from the previous event has completed.Bounds: 2m, 1d. Default: 2m.
	CooldownPeriod string `pulumi:"cooldownPeriod"`
	// Optional. Spark Standalone autoscaling configuration
	SparkStandaloneConfig SparkStandaloneAutoscalingConfigResponse `pulumi:"sparkStandaloneConfig"`
	// Optional. YARN autoscaling configuration.
	YarnConfig BasicYarnAutoscalingConfigResponse `pulumi:"yarnConfig"`
}

// Basic algorithm for autoscaling.
type BasicAutoscalingAlgorithmResponseOutput struct{ *pulumi.OutputState }

func (BasicAutoscalingAlgorithmResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*BasicAutoscalingAlgorithmResponse)(nil)).Elem()
}

func (o BasicAutoscalingAlgorithmResponseOutput) ToBasicAutoscalingAlgorithmResponseOutput() BasicAutoscalingAlgorithmResponseOutput {
	return o
}

func (o BasicAutoscalingAlgorithmResponseOutput) ToBasicAutoscalingAlgorithmResponseOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmResponseOutput {
	return o
}

// Optional. Duration between scaling events. A scaling period starts after the update operation from the previous event has completed.Bounds: 2m, 1d. Default: 2m.
func (o BasicAutoscalingAlgorithmResponseOutput) CooldownPeriod() pulumi.StringOutput {
	return o.ApplyT(func(v BasicAutoscalingAlgorithmResponse) string { return v.CooldownPeriod }).(pulumi.StringOutput)
}

// Optional. Spark Standalone autoscaling configuration
func (o BasicAutoscalingAlgorithmResponseOutput) SparkStandaloneConfig() SparkStandaloneAutoscalingConfigResponseOutput {
	return o.ApplyT(func(v BasicAutoscalingAlgorithmResponse) SparkStandaloneAutoscalingConfigResponse {
		return v.SparkStandaloneConfig
	}).(SparkStandaloneAutoscalingConfigResponseOutput)
}

// Optional. YARN autoscaling configuration.
func (o BasicAutoscalingAlgorithmResponseOutput) YarnConfig() BasicYarnAutoscalingConfigResponseOutput {
	return o.ApplyT(func(v BasicAutoscalingAlgorithmResponse) BasicYarnAutoscalingConfigResponse { return v.YarnConfig }).(BasicYarnAutoscalingConfigResponseOutput)
}

// Basic autoscaling configurations for YARN.
type BasicYarnAutoscalingConfig struct {
	// Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations.Bounds: 0s, 1d.
	GracefulDecommissionTimeout string `pulumi:"gracefulDecommissionTimeout"`
	// Fraction of average YARN pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
	ScaleDownFactor float64 `pulumi:"scaleDownFactor"`
	// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleDownMinWorkerFraction *float64 `pulumi:"scaleDownMinWorkerFraction"`
	// Fraction of average YARN pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
	ScaleUpFactor float64 `pulumi:"scaleUpFactor"`
	// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleUpMinWorkerFraction *float64 `pulumi:"scaleUpMinWorkerFraction"`
}

// BasicYarnAutoscalingConfigInput is an input type that accepts BasicYarnAutoscalingConfigArgs and BasicYarnAutoscalingConfigOutput values.
// You can construct a concrete instance of `BasicYarnAutoscalingConfigInput` via:
//
//	BasicYarnAutoscalingConfigArgs{...}
type BasicYarnAutoscalingConfigInput interface {
	pulumi.Input

	ToBasicYarnAutoscalingConfigOutput() BasicYarnAutoscalingConfigOutput
	ToBasicYarnAutoscalingConfigOutputWithContext(context.Context) BasicYarnAutoscalingConfigOutput
}

// Basic autoscaling configurations for YARN.
type BasicYarnAutoscalingConfigArgs struct {
	// Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations.Bounds: 0s, 1d.
	GracefulDecommissionTimeout pulumi.StringInput `pulumi:"gracefulDecommissionTimeout"`
	// Fraction of average YARN pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
	ScaleDownFactor pulumi.Float64Input `pulumi:"scaleDownFactor"`
	// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleDownMinWorkerFraction pulumi.Float64PtrInput `pulumi:"scaleDownMinWorkerFraction"`
	// Fraction of average YARN pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
	ScaleUpFactor pulumi.Float64Input `pulumi:"scaleUpFactor"`
	// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleUpMinWorkerFraction pulumi.Float64PtrInput `pulumi:"scaleUpMinWorkerFraction"`
}

func (BasicYarnAutoscalingConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*BasicYarnAutoscalingConfig)(nil)).Elem()
}

func (i BasicYarnAutoscalingConfigArgs) ToBasicYarnAutoscalingConfigOutput() BasicYarnAutoscalingConfigOutput {
	return i.ToBasicYarnAutoscalingConfigOutputWithContext(context.Background())
}

func (i BasicYarnAutoscalingConfigArgs) ToBasicYarnAutoscalingConfigOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BasicYarnAutoscalingConfigOutput)
}

func (i BasicYarnAutoscalingConfigArgs) ToBasicYarnAutoscalingConfigPtrOutput() BasicYarnAutoscalingConfigPtrOutput {
	return i.ToBasicYarnAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i BasicYarnAutoscalingConfigArgs) ToBasicYarnAutoscalingConfigPtrOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BasicYarnAutoscalingConfigOutput).ToBasicYarnAutoscalingConfigPtrOutputWithContext(ctx)
}

// BasicYarnAutoscalingConfigPtrInput is an input type that accepts BasicYarnAutoscalingConfigArgs, BasicYarnAutoscalingConfigPtr and BasicYarnAutoscalingConfigPtrOutput values.
// You can construct a concrete instance of `BasicYarnAutoscalingConfigPtrInput` via:
//
//	        BasicYarnAutoscalingConfigArgs{...}
//
//	or:
//
//	        nil
type BasicYarnAutoscalingConfigPtrInput interface {
	pulumi.Input

	ToBasicYarnAutoscalingConfigPtrOutput() BasicYarnAutoscalingConfigPtrOutput
	ToBasicYarnAutoscalingConfigPtrOutputWithContext(context.Context) BasicYarnAutoscalingConfigPtrOutput
}

type basicYarnAutoscalingConfigPtrType BasicYarnAutoscalingConfigArgs

func BasicYarnAutoscalingConfigPtr(v *BasicYarnAutoscalingConfigArgs) BasicYarnAutoscalingConfigPtrInput {
	return (*basicYarnAutoscalingConfigPtrType)(v)
}

func (*basicYarnAutoscalingConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**BasicYarnAutoscalingConfig)(nil)).Elem()
}

func (i *basicYarnAutoscalingConfigPtrType) ToBasicYarnAutoscalingConfigPtrOutput() BasicYarnAutoscalingConfigPtrOutput {
	return i.ToBasicYarnAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i *basicYarnAutoscalingConfigPtrType) ToBasicYarnAutoscalingConfigPtrOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BasicYarnAutoscalingConfigPtrOutput)
}

// Basic autoscaling configurations for YARN.
type BasicYarnAutoscalingConfigOutput struct{ *pulumi.OutputState }

func (BasicYarnAutoscalingConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*BasicYarnAutoscalingConfig)(nil)).Elem()
}

func (o BasicYarnAutoscalingConfigOutput) ToBasicYarnAutoscalingConfigOutput() BasicYarnAutoscalingConfigOutput {
	return o
}

func (o BasicYarnAutoscalingConfigOutput) ToBasicYarnAutoscalingConfigOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigOutput {
	return o
}

func (o BasicYarnAutoscalingConfigOutput) ToBasicYarnAutoscalingConfigPtrOutput() BasicYarnAutoscalingConfigPtrOutput {
	return o.ToBasicYarnAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (o BasicYarnAutoscalingConfigOutput) ToBasicYarnAutoscalingConfigPtrOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v BasicYarnAutoscalingConfig) *BasicYarnAutoscalingConfig {
		return &v
	}).(BasicYarnAutoscalingConfigPtrOutput)
}

// Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations.Bounds: 0s, 1d.
func (o BasicYarnAutoscalingConfigOutput) GracefulDecommissionTimeout() pulumi.StringOutput {
	return o.ApplyT(func(v BasicYarnAutoscalingConfig) string { return v.GracefulDecommissionTimeout }).(pulumi.StringOutput)
}

// Fraction of average YARN pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
func (o BasicYarnAutoscalingConfigOutput) ScaleDownFactor() pulumi.Float64Output {
	return o.ApplyT(func(v BasicYarnAutoscalingConfig) float64 { return v.ScaleDownFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o BasicYarnAutoscalingConfigOutput) ScaleDownMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v BasicYarnAutoscalingConfig) *float64 { return v.ScaleDownMinWorkerFraction }).(pulumi.Float64PtrOutput)
}

// Fraction of average YARN pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
func (o BasicYarnAutoscalingConfigOutput) ScaleUpFactor() pulumi.Float64Output {
	return o.ApplyT(func(v BasicYarnAutoscalingConfig) float64 { return v.ScaleUpFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o BasicYarnAutoscalingConfigOutput) ScaleUpMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v BasicYarnAutoscalingConfig) *float64 { return v.ScaleUpMinWorkerFraction }).(pulumi.Float64PtrOutput)
}

type BasicYarnAutoscalingConfigPtrOutput struct{ *pulumi.OutputState }

func (BasicYarnAutoscalingConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**BasicYarnAutoscalingConfig)(nil)).Elem()
}

func (o BasicYarnAutoscalingConfigPtrOutput) ToBasicYarnAutoscalingConfigPtrOutput() BasicYarnAutoscalingConfigPtrOutput {
	return o
}

func (o BasicYarnAutoscalingConfigPtrOutput) ToBasicYarnAutoscalingConfigPtrOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigPtrOutput {
	return o
}

func (o BasicYarnAutoscalingConfigPtrOutput) Elem() BasicYarnAutoscalingConfigOutput {
	return o.ApplyT(func(v *BasicYarnAutoscalingConfig) BasicYarnAutoscalingConfig {
		if v != nil {
			return *v
		}
		var ret BasicYarnAutoscalingConfig
		return ret
	}).(BasicYarnAutoscalingConfigOutput)
}

// Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations.Bounds: 0s, 1d.
func (o BasicYarnAutoscalingConfigPtrOutput) GracefulDecommissionTimeout() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *BasicYarnAutoscalingConfig) *string {
		if v == nil {
			return nil
		}
		return &v.GracefulDecommissionTimeout
	}).(pulumi.StringPtrOutput)
}

// Fraction of average YARN pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
func (o BasicYarnAutoscalingConfigPtrOutput) ScaleDownFactor() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *BasicYarnAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return &v.ScaleDownFactor
	}).(pulumi.Float64PtrOutput)
}

// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o BasicYarnAutoscalingConfigPtrOutput) ScaleDownMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *BasicYarnAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return v.ScaleDownMinWorkerFraction
	}).(pulumi.Float64PtrOutput)
}

// Fraction of average YARN pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
func (o BasicYarnAutoscalingConfigPtrOutput) ScaleUpFactor() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *BasicYarnAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return &v.ScaleUpFactor
	}).(pulumi.Float64PtrOutput)
}

// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o BasicYarnAutoscalingConfigPtrOutput) ScaleUpMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *BasicYarnAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return v.ScaleUpMinWorkerFraction
	}).(pulumi.Float64PtrOutput)
}

// Basic autoscaling configurations for YARN.
type BasicYarnAutoscalingConfigResponse struct {
	// Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations.Bounds: 0s, 1d.
	GracefulDecommissionTimeout string `pulumi:"gracefulDecommissionTimeout"`
	// Fraction of average YARN pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
	ScaleDownFactor float64 `pulumi:"scaleDownFactor"`
	// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleDownMinWorkerFraction float64 `pulumi:"scaleDownMinWorkerFraction"`
	// Fraction of average YARN pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
	ScaleUpFactor float64 `pulumi:"scaleUpFactor"`
	// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleUpMinWorkerFraction float64 `pulumi:"scaleUpMinWorkerFraction"`
}

// Basic autoscaling configurations for YARN.
type BasicYarnAutoscalingConfigResponseOutput struct{ *pulumi.OutputState }

func (BasicYarnAutoscalingConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*BasicYarnAutoscalingConfigResponse)(nil)).Elem()
}

func (o BasicYarnAutoscalingConfigResponseOutput) ToBasicYarnAutoscalingConfigResponseOutput() BasicYarnAutoscalingConfigResponseOutput {
	return o
}

func (o BasicYarnAutoscalingConfigResponseOutput) ToBasicYarnAutoscalingConfigResponseOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigResponseOutput {
	return o
}

// Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations.Bounds: 0s, 1d.
func (o BasicYarnAutoscalingConfigResponseOutput) GracefulDecommissionTimeout() pulumi.StringOutput {
	return o.ApplyT(func(v BasicYarnAutoscalingConfigResponse) string { return v.GracefulDecommissionTimeout }).(pulumi.StringOutput)
}

// Fraction of average YARN pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
func (o BasicYarnAutoscalingConfigResponseOutput) ScaleDownFactor() pulumi.Float64Output {
	return o.ApplyT(func(v BasicYarnAutoscalingConfigResponse) float64 { return v.ScaleDownFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o BasicYarnAutoscalingConfigResponseOutput) ScaleDownMinWorkerFraction() pulumi.Float64Output {
	return o.ApplyT(func(v BasicYarnAutoscalingConfigResponse) float64 { return v.ScaleDownMinWorkerFraction }).(pulumi.Float64Output)
}

// Fraction of average YARN pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
func (o BasicYarnAutoscalingConfigResponseOutput) ScaleUpFactor() pulumi.Float64Output {
	return o.ApplyT(func(v BasicYarnAutoscalingConfigResponse) float64 { return v.ScaleUpFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o BasicYarnAutoscalingConfigResponseOutput) ScaleUpMinWorkerFraction() pulumi.Float64Output {
	return o.ApplyT(func(v BasicYarnAutoscalingConfigResponse) float64 { return v.ScaleUpMinWorkerFraction }).(pulumi.Float64Output)
}

// Associates members, or principals, with a role.
type Binding struct {
	// The condition that is associated with this binding.If the condition evaluates to true, then this binding applies to the current request.If the condition evaluates to false, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding.To learn which resources support conditions in their IAM policies, see the IAM documentation (https://cloud.google.com/iam/help/conditions/resource-policies).
	Condition *Expr `pulumi:"condition"`
	// Specifies the principals requesting access for a Google Cloud resource. members can have the following values: allUsers: A special identifier that represents anyone who is on the internet; with or without a Google account. allAuthenticatedUsers: A special identifier that represents anyone who is authenticated with a Google account or a service account. Does not include identities that come from external identity providers (IdPs) through identity federation. user:{emailid}: An email address that represents a specific Google account. For example, alice@example.com . serviceAccount:{emailid}: An email address that represents a Google service account. For example, my-other-app@appspot.gserviceaccount.com. serviceAccount:{projectid}.svc.id.goog[{namespace}/{kubernetes-sa}]: An identifier for a Kubernetes service account (https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts). For example, my-project.svc.id.goog[my-namespace/my-kubernetes-sa]. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. domain:{domain}: The G Suite domain (primary) that represents all the users of that domain. For example, google.com or example.com. deleted:user:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a user that has been recently deleted. For example, alice@example.com?uid=123456789012345678901. If the user is recovered, this value reverts to user:{emailid} and the recovered user retains the role in the binding. deleted:serviceAccount:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901. If the service account is undeleted, this value reverts to serviceAccount:{emailid} and the undeleted service account retains the role in the binding. deleted:group:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, admins@example.com?uid=123456789012345678901. If the group is recovered, this value reverts to group:{emailid} and the recovered group retains the role in the binding.
	Members []string `pulumi:"members"`
	// Role that is assigned to the list of members, or principals. For example, roles/viewer, roles/editor, or roles/owner.
	Role *string `pulumi:"role"`
}

// BindingInput is an input type that accepts BindingArgs and BindingOutput values.
// You can construct a concrete instance of `BindingInput` via:
//
//	BindingArgs{...}
type BindingInput interface {
	pulumi.Input

	ToBindingOutput() BindingOutput
	ToBindingOutputWithContext(context.Context) BindingOutput
}

// Associates members, or principals, with a role.
type BindingArgs struct {
	// The condition that is associated with this binding.If the condition evaluates to true, then this binding applies to the current request.If the condition evaluates to false, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding.To learn which resources support conditions in their IAM policies, see the IAM documentation (https://cloud.google.com/iam/help/conditions/resource-policies).
	Condition ExprPtrInput `pulumi:"condition"`
	// Specifies the principals requesting access for a Google Cloud resource. members can have the following values: allUsers: A special identifier that represents anyone who is on the internet; with or without a Google account. allAuthenticatedUsers: A special identifier that represents anyone who is authenticated with a Google account or a service account. Does not include identities that come from external identity providers (IdPs) through identity federation. user:{emailid}: An email address that represents a specific Google account. For example, alice@example.com . serviceAccount:{emailid}: An email address that represents a Google service account. For example, my-other-app@appspot.gserviceaccount.com. serviceAccount:{projectid}.svc.id.goog[{namespace}/{kubernetes-sa}]: An identifier for a Kubernetes service account (https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts). For example, my-project.svc.id.goog[my-namespace/my-kubernetes-sa]. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. domain:{domain}: The G Suite domain (primary) that represents all the users of that domain. For example, google.com or example.com. deleted:user:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a user that has been recently deleted. For example, alice@example.com?uid=123456789012345678901. If the user is recovered, this value reverts to user:{emailid} and the recovered user retains the role in the binding. deleted:serviceAccount:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901. If the service account is undeleted, this value reverts to serviceAccount:{emailid} and the undeleted service account retains the role in the binding. deleted:group:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, admins@example.com?uid=123456789012345678901. If the group is recovered, this value reverts to group:{emailid} and the recovered group retains the role in the binding.
	Members pulumi.StringArrayInput `pulumi:"members"`
	// Role that is assigned to the list of members, or principals. For example, roles/viewer, roles/editor, or roles/owner.
	Role pulumi.StringPtrInput `pulumi:"role"`
}

func (BindingArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*Binding)(nil)).Elem()
}

func (i BindingArgs) ToBindingOutput() BindingOutput {
	return i.ToBindingOutputWithContext(context.Background())
}

func (i BindingArgs) ToBindingOutputWithContext(ctx context.Context) BindingOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BindingOutput)
}

// BindingArrayInput is an input type that accepts BindingArray and BindingArrayOutput values.
// You can construct a concrete instance of `BindingArrayInput` via:
//
//	BindingArray{ BindingArgs{...} }
type BindingArrayInput interface {
	pulumi.Input

	ToBindingArrayOutput() BindingArrayOutput
	ToBindingArrayOutputWithContext(context.Context) BindingArrayOutput
}

type BindingArray []BindingInput

func (BindingArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]Binding)(nil)).Elem()
}

func (i BindingArray) ToBindingArrayOutput() BindingArrayOutput {
	return i.ToBindingArrayOutputWithContext(context.Background())
}

func (i BindingArray) ToBindingArrayOutputWithContext(ctx context.Context) BindingArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BindingArrayOutput)
}

// Associates members, or principals, with a role.
type BindingOutput struct{ *pulumi.OutputState }

func (BindingOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*Binding)(nil)).Elem()
}

func (o BindingOutput) ToBindingOutput() BindingOutput {
	return o
}

func (o BindingOutput) ToBindingOutputWithContext(ctx context.Context) BindingOutput {
	return o
}

// The condition that is associated with this binding.If the condition evaluates to true, then this binding applies to the current request.If the condition evaluates to false, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding.To learn which resources support conditions in their IAM policies, see the IAM documentation (https://cloud.google.com/iam/help/conditions/resource-policies).
func (o BindingOutput) Condition() ExprPtrOutput {
	return o.ApplyT(func(v Binding) *Expr { return v.Condition }).(ExprPtrOutput)
}

// Specifies the principals requesting access for a Google Cloud resource. members can have the following values: allUsers: A special identifier that represents anyone who is on the internet; with or without a Google account. allAuthenticatedUsers: A special identifier that represents anyone who is authenticated with a Google account or a service account. Does not include identities that come from external identity providers (IdPs) through identity federation. user:{emailid}: An email address that represents a specific Google account. For example, alice@example.com . serviceAccount:{emailid}: An email address that represents a Google service account. For example, my-other-app@appspot.gserviceaccount.com. serviceAccount:{projectid}.svc.id.goog[{namespace}/{kubernetes-sa}]: An identifier for a Kubernetes service account (https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts). For example, my-project.svc.id.goog[my-namespace/my-kubernetes-sa]. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. domain:{domain}: The G Suite domain (primary) that represents all the users of that domain. For example, google.com or example.com. deleted:user:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a user that has been recently deleted. For example, alice@example.com?uid=123456789012345678901. If the user is recovered, this value reverts to user:{emailid} and the recovered user retains the role in the binding. deleted:serviceAccount:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901. If the service account is undeleted, this value reverts to serviceAccount:{emailid} and the undeleted service account retains the role in the binding. deleted:group:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, admins@example.com?uid=123456789012345678901. If the group is recovered, this value reverts to group:{emailid} and the recovered group retains the role in the binding.
func (o BindingOutput) Members() pulumi.StringArrayOutput {
	return o.ApplyT(func(v Binding) []string { return v.Members }).(pulumi.StringArrayOutput)
}

// Role that is assigned to the list of members, or principals. For example, roles/viewer, roles/editor, or roles/owner.
func (o BindingOutput) Role() pulumi.StringPtrOutput {
	return o.ApplyT(func(v Binding) *string { return v.Role }).(pulumi.StringPtrOutput)
}

type BindingArrayOutput struct{ *pulumi.OutputState }

func (BindingArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]Binding)(nil)).Elem()
}

func (o BindingArrayOutput) ToBindingArrayOutput() BindingArrayOutput {
	return o
}

func (o BindingArrayOutput) ToBindingArrayOutputWithContext(ctx context.Context) BindingArrayOutput {
	return o
}

func (o BindingArrayOutput) Index(i pulumi.IntInput) BindingOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) Binding {
		return vs[0].([]Binding)[vs[1].(int)]
	}).(BindingOutput)
}

// Associates members, or principals, with a role.
type BindingResponse struct {
	// The condition that is associated with this binding.If the condition evaluates to true, then this binding applies to the current request.If the condition evaluates to false, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding.To learn which resources support conditions in their IAM policies, see the IAM documentation (https://cloud.google.com/iam/help/conditions/resource-policies).
	Condition ExprResponse `pulumi:"condition"`
	// Specifies the principals requesting access for a Google Cloud resource. members can have the following values: allUsers: A special identifier that represents anyone who is on the internet; with or without a Google account. allAuthenticatedUsers: A special identifier that represents anyone who is authenticated with a Google account or a service account. Does not include identities that come from external identity providers (IdPs) through identity federation. user:{emailid}: An email address that represents a specific Google account. For example, alice@example.com . serviceAccount:{emailid}: An email address that represents a Google service account. For example, my-other-app@appspot.gserviceaccount.com. serviceAccount:{projectid}.svc.id.goog[{namespace}/{kubernetes-sa}]: An identifier for a Kubernetes service account (https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts). For example, my-project.svc.id.goog[my-namespace/my-kubernetes-sa]. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. domain:{domain}: The G Suite domain (primary) that represents all the users of that domain. For example, google.com or example.com. deleted:user:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a user that has been recently deleted. For example, alice@example.com?uid=123456789012345678901. If the user is recovered, this value reverts to user:{emailid} and the recovered user retains the role in the binding. deleted:serviceAccount:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901. If the service account is undeleted, this value reverts to serviceAccount:{emailid} and the undeleted service account retains the role in the binding. deleted:group:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, admins@example.com?uid=123456789012345678901. If the group is recovered, this value reverts to group:{emailid} and the recovered group retains the role in the binding.
	Members []string `pulumi:"members"`
	// Role that is assigned to the list of members, or principals. For example, roles/viewer, roles/editor, or roles/owner.
	Role string `pulumi:"role"`
}

// Associates members, or principals, with a role.
type BindingResponseOutput struct{ *pulumi.OutputState }

func (BindingResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*BindingResponse)(nil)).Elem()
}

func (o BindingResponseOutput) ToBindingResponseOutput() BindingResponseOutput {
	return o
}

func (o BindingResponseOutput) ToBindingResponseOutputWithContext(ctx context.Context) BindingResponseOutput {
	return o
}

// The condition that is associated with this binding.If the condition evaluates to true, then this binding applies to the current request.If the condition evaluates to false, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding.To learn which resources support conditions in their IAM policies, see the IAM documentation (https://cloud.google.com/iam/help/conditions/resource-policies).
func (o BindingResponseOutput) Condition() ExprResponseOutput {
	return o.ApplyT(func(v BindingResponse) ExprResponse { return v.Condition }).(ExprResponseOutput)
}

// Specifies the principals requesting access for a Google Cloud resource. members can have the following values: allUsers: A special identifier that represents anyone who is on the internet; with or without a Google account. allAuthenticatedUsers: A special identifier that represents anyone who is authenticated with a Google account or a service account. Does not include identities that come from external identity providers (IdPs) through identity federation. user:{emailid}: An email address that represents a specific Google account. For example, alice@example.com . serviceAccount:{emailid}: An email address that represents a Google service account. For example, my-other-app@appspot.gserviceaccount.com. serviceAccount:{projectid}.svc.id.goog[{namespace}/{kubernetes-sa}]: An identifier for a Kubernetes service account (https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts). For example, my-project.svc.id.goog[my-namespace/my-kubernetes-sa]. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. domain:{domain}: The G Suite domain (primary) that represents all the users of that domain. For example, google.com or example.com. deleted:user:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a user that has been recently deleted. For example, alice@example.com?uid=123456789012345678901. If the user is recovered, this value reverts to user:{emailid} and the recovered user retains the role in the binding. deleted:serviceAccount:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901. If the service account is undeleted, this value reverts to serviceAccount:{emailid} and the undeleted service account retains the role in the binding. deleted:group:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, admins@example.com?uid=123456789012345678901. If the group is recovered, this value reverts to group:{emailid} and the recovered group retains the role in the binding.
func (o BindingResponseOutput) Members() pulumi.StringArrayOutput {
	return o.ApplyT(func(v BindingResponse) []string { return v.Members }).(pulumi.StringArrayOutput)
}

// Role that is assigned to the list of members, or principals. For example, roles/viewer, roles/editor, or roles/owner.
func (o BindingResponseOutput) Role() pulumi.StringOutput {
	return o.ApplyT(func(v BindingResponse) string { return v.Role }).(pulumi.StringOutput)
}

type BindingResponseArrayOutput struct{ *pulumi.OutputState }

func (BindingResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]BindingResponse)(nil)).Elem()
}

func (o BindingResponseArrayOutput) ToBindingResponseArrayOutput() BindingResponseArrayOutput {
	return o
}

func (o BindingResponseArrayOutput) ToBindingResponseArrayOutputWithContext(ctx context.Context) BindingResponseArrayOutput {
	return o
}

func (o BindingResponseArrayOutput) Index(i pulumi.IntInput) BindingResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) BindingResponse {
		return vs[0].([]BindingResponse)[vs[1].(int)]
	}).(BindingResponseOutput)
}

// The cluster config.
type ClusterConfig struct {
	// Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
	AutoscalingConfig *AutoscalingConfig `pulumi:"autoscalingConfig"`
	// Optional. The node group settings.
	AuxiliaryNodeGroups []AuxiliaryNodeGroup `pulumi:"auxiliaryNodeGroups"`
	// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	ConfigBucket *string `pulumi:"configBucket"`
	// Optional. The config for Dataproc metrics.
	DataprocMetricConfig *DataprocMetricConfig `pulumi:"dataprocMetricConfig"`
	// Optional. Encryption settings for the cluster.
	EncryptionConfig *EncryptionConfig `pulumi:"encryptionConfig"`
	// Optional. Port/endpoint configuration for this cluster
	EndpointConfig *EndpointConfig `pulumi:"endpointConfig"`
	// Optional. The shared Compute Engine config settings for all instances in a cluster.
	GceClusterConfig *GceClusterConfig `pulumi:"gceClusterConfig"`
	// Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. These config settings are mutually exclusive with Compute Engine-based options, such as gce_cluster_config, master_config, worker_config, secondary_worker_config, and autoscaling_config.
	GkeClusterConfig *GkeClusterConfig `pulumi:"gkeClusterConfig"`
	// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
	InitializationActions []NodeInitializationAction `pulumi:"initializationActions"`
	// Optional. Lifecycle setting for the cluster.
	LifecycleConfig *LifecycleConfig `pulumi:"lifecycleConfig"`
	// Optional. The Compute Engine config settings for the cluster's master instance.
	MasterConfig *InstanceGroupConfig `pulumi:"masterConfig"`
	// Optional. Metastore configuration.
	MetastoreConfig *MetastoreConfig `pulumi:"metastoreConfig"`
	// Optional. The Compute Engine config settings for a cluster's secondary worker instances
	SecondaryWorkerConfig *InstanceGroupConfig `pulumi:"secondaryWorkerConfig"`
	// Optional. Security settings for the cluster.
	SecurityConfig *SecurityConfig `pulumi:"securityConfig"`
	// Optional. The config settings for cluster software.
	SoftwareConfig *SoftwareConfig `pulumi:"softwareConfig"`
	// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	TempBucket *string `pulumi:"tempBucket"`
	// Optional. The Compute Engine config settings for the cluster's worker instances.
	WorkerConfig *InstanceGroupConfig `pulumi:"workerConfig"`
}

// ClusterConfigInput is an input type that accepts ClusterConfigArgs and ClusterConfigOutput values.
// You can construct a concrete instance of `ClusterConfigInput` via:
//
//	ClusterConfigArgs{...}
type ClusterConfigInput interface {
	pulumi.Input

	ToClusterConfigOutput() ClusterConfigOutput
	ToClusterConfigOutputWithContext(context.Context) ClusterConfigOutput
}

// The cluster config.
type ClusterConfigArgs struct {
	// Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
	AutoscalingConfig AutoscalingConfigPtrInput `pulumi:"autoscalingConfig"`
	// Optional. The node group settings.
	AuxiliaryNodeGroups AuxiliaryNodeGroupArrayInput `pulumi:"auxiliaryNodeGroups"`
	// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	ConfigBucket pulumi.StringPtrInput `pulumi:"configBucket"`
	// Optional. The config for Dataproc metrics.
	DataprocMetricConfig DataprocMetricConfigPtrInput `pulumi:"dataprocMetricConfig"`
	// Optional. Encryption settings for the cluster.
	EncryptionConfig EncryptionConfigPtrInput `pulumi:"encryptionConfig"`
	// Optional. Port/endpoint configuration for this cluster
	EndpointConfig EndpointConfigPtrInput `pulumi:"endpointConfig"`
	// Optional. The shared Compute Engine config settings for all instances in a cluster.
	GceClusterConfig GceClusterConfigPtrInput `pulumi:"gceClusterConfig"`
	// Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. These config settings are mutually exclusive with Compute Engine-based options, such as gce_cluster_config, master_config, worker_config, secondary_worker_config, and autoscaling_config.
	GkeClusterConfig GkeClusterConfigPtrInput `pulumi:"gkeClusterConfig"`
	// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
	InitializationActions NodeInitializationActionArrayInput `pulumi:"initializationActions"`
	// Optional. Lifecycle setting for the cluster.
	LifecycleConfig LifecycleConfigPtrInput `pulumi:"lifecycleConfig"`
	// Optional. The Compute Engine config settings for the cluster's master instance.
	MasterConfig InstanceGroupConfigPtrInput `pulumi:"masterConfig"`
	// Optional. Metastore configuration.
	MetastoreConfig MetastoreConfigPtrInput `pulumi:"metastoreConfig"`
	// Optional. The Compute Engine config settings for a cluster's secondary worker instances
	SecondaryWorkerConfig InstanceGroupConfigPtrInput `pulumi:"secondaryWorkerConfig"`
	// Optional. Security settings for the cluster.
	SecurityConfig SecurityConfigPtrInput `pulumi:"securityConfig"`
	// Optional. The config settings for cluster software.
	SoftwareConfig SoftwareConfigPtrInput `pulumi:"softwareConfig"`
	// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	TempBucket pulumi.StringPtrInput `pulumi:"tempBucket"`
	// Optional. The Compute Engine config settings for the cluster's worker instances.
	WorkerConfig InstanceGroupConfigPtrInput `pulumi:"workerConfig"`
}

func (ClusterConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterConfig)(nil)).Elem()
}

func (i ClusterConfigArgs) ToClusterConfigOutput() ClusterConfigOutput {
	return i.ToClusterConfigOutputWithContext(context.Background())
}

func (i ClusterConfigArgs) ToClusterConfigOutputWithContext(ctx context.Context) ClusterConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterConfigOutput)
}

func (i ClusterConfigArgs) ToClusterConfigPtrOutput() ClusterConfigPtrOutput {
	return i.ToClusterConfigPtrOutputWithContext(context.Background())
}

func (i ClusterConfigArgs) ToClusterConfigPtrOutputWithContext(ctx context.Context) ClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterConfigOutput).ToClusterConfigPtrOutputWithContext(ctx)
}

// ClusterConfigPtrInput is an input type that accepts ClusterConfigArgs, ClusterConfigPtr and ClusterConfigPtrOutput values.
// You can construct a concrete instance of `ClusterConfigPtrInput` via:
//
//	        ClusterConfigArgs{...}
//
//	or:
//
//	        nil
type ClusterConfigPtrInput interface {
	pulumi.Input

	ToClusterConfigPtrOutput() ClusterConfigPtrOutput
	ToClusterConfigPtrOutputWithContext(context.Context) ClusterConfigPtrOutput
}

type clusterConfigPtrType ClusterConfigArgs

func ClusterConfigPtr(v *ClusterConfigArgs) ClusterConfigPtrInput {
	return (*clusterConfigPtrType)(v)
}

func (*clusterConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ClusterConfig)(nil)).Elem()
}

func (i *clusterConfigPtrType) ToClusterConfigPtrOutput() ClusterConfigPtrOutput {
	return i.ToClusterConfigPtrOutputWithContext(context.Background())
}

func (i *clusterConfigPtrType) ToClusterConfigPtrOutputWithContext(ctx context.Context) ClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterConfigPtrOutput)
}

// The cluster config.
type ClusterConfigOutput struct{ *pulumi.OutputState }

func (ClusterConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterConfig)(nil)).Elem()
}

func (o ClusterConfigOutput) ToClusterConfigOutput() ClusterConfigOutput {
	return o
}

func (o ClusterConfigOutput) ToClusterConfigOutputWithContext(ctx context.Context) ClusterConfigOutput {
	return o
}

func (o ClusterConfigOutput) ToClusterConfigPtrOutput() ClusterConfigPtrOutput {
	return o.ToClusterConfigPtrOutputWithContext(context.Background())
}

func (o ClusterConfigOutput) ToClusterConfigPtrOutputWithContext(ctx context.Context) ClusterConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ClusterConfig) *ClusterConfig {
		return &v
	}).(ClusterConfigPtrOutput)
}

// Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
func (o ClusterConfigOutput) AutoscalingConfig() AutoscalingConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *AutoscalingConfig { return v.AutoscalingConfig }).(AutoscalingConfigPtrOutput)
}

// Optional. The node group settings.
func (o ClusterConfigOutput) AuxiliaryNodeGroups() AuxiliaryNodeGroupArrayOutput {
	return o.ApplyT(func(v ClusterConfig) []AuxiliaryNodeGroup { return v.AuxiliaryNodeGroups }).(AuxiliaryNodeGroupArrayOutput)
}

// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ClusterConfigOutput) ConfigBucket() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *string { return v.ConfigBucket }).(pulumi.StringPtrOutput)
}

// Optional. The config for Dataproc metrics.
func (o ClusterConfigOutput) DataprocMetricConfig() DataprocMetricConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *DataprocMetricConfig { return v.DataprocMetricConfig }).(DataprocMetricConfigPtrOutput)
}

// Optional. Encryption settings for the cluster.
func (o ClusterConfigOutput) EncryptionConfig() EncryptionConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *EncryptionConfig { return v.EncryptionConfig }).(EncryptionConfigPtrOutput)
}

// Optional. Port/endpoint configuration for this cluster
func (o ClusterConfigOutput) EndpointConfig() EndpointConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *EndpointConfig { return v.EndpointConfig }).(EndpointConfigPtrOutput)
}

// Optional. The shared Compute Engine config settings for all instances in a cluster.
func (o ClusterConfigOutput) GceClusterConfig() GceClusterConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *GceClusterConfig { return v.GceClusterConfig }).(GceClusterConfigPtrOutput)
}

// Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. These config settings are mutually exclusive with Compute Engine-based options, such as gce_cluster_config, master_config, worker_config, secondary_worker_config, and autoscaling_config.
func (o ClusterConfigOutput) GkeClusterConfig() GkeClusterConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *GkeClusterConfig { return v.GkeClusterConfig }).(GkeClusterConfigPtrOutput)
}

// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
func (o ClusterConfigOutput) InitializationActions() NodeInitializationActionArrayOutput {
	return o.ApplyT(func(v ClusterConfig) []NodeInitializationAction { return v.InitializationActions }).(NodeInitializationActionArrayOutput)
}

// Optional. Lifecycle setting for the cluster.
func (o ClusterConfigOutput) LifecycleConfig() LifecycleConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *LifecycleConfig { return v.LifecycleConfig }).(LifecycleConfigPtrOutput)
}

// Optional. The Compute Engine config settings for the cluster's master instance.
func (o ClusterConfigOutput) MasterConfig() InstanceGroupConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *InstanceGroupConfig { return v.MasterConfig }).(InstanceGroupConfigPtrOutput)
}

// Optional. Metastore configuration.
func (o ClusterConfigOutput) MetastoreConfig() MetastoreConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *MetastoreConfig { return v.MetastoreConfig }).(MetastoreConfigPtrOutput)
}

// Optional. The Compute Engine config settings for a cluster's secondary worker instances
func (o ClusterConfigOutput) SecondaryWorkerConfig() InstanceGroupConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *InstanceGroupConfig { return v.SecondaryWorkerConfig }).(InstanceGroupConfigPtrOutput)
}

// Optional. Security settings for the cluster.
func (o ClusterConfigOutput) SecurityConfig() SecurityConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *SecurityConfig { return v.SecurityConfig }).(SecurityConfigPtrOutput)
}

// Optional. The config settings for cluster software.
func (o ClusterConfigOutput) SoftwareConfig() SoftwareConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *SoftwareConfig { return v.SoftwareConfig }).(SoftwareConfigPtrOutput)
}

// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ClusterConfigOutput) TempBucket() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *string { return v.TempBucket }).(pulumi.StringPtrOutput)
}

// Optional. The Compute Engine config settings for the cluster's worker instances.
func (o ClusterConfigOutput) WorkerConfig() InstanceGroupConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *InstanceGroupConfig { return v.WorkerConfig }).(InstanceGroupConfigPtrOutput)
}

type ClusterConfigPtrOutput struct{ *pulumi.OutputState }

func (ClusterConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ClusterConfig)(nil)).Elem()
}

func (o ClusterConfigPtrOutput) ToClusterConfigPtrOutput() ClusterConfigPtrOutput {
	return o
}

func (o ClusterConfigPtrOutput) ToClusterConfigPtrOutputWithContext(ctx context.Context) ClusterConfigPtrOutput {
	return o
}

func (o ClusterConfigPtrOutput) Elem() ClusterConfigOutput {
	return o.ApplyT(func(v *ClusterConfig) ClusterConfig {
		if v != nil {
			return *v
		}
		var ret ClusterConfig
		return ret
	}).(ClusterConfigOutput)
}

// Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
func (o ClusterConfigPtrOutput) AutoscalingConfig() AutoscalingConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *AutoscalingConfig {
		if v == nil {
			return nil
		}
		return v.AutoscalingConfig
	}).(AutoscalingConfigPtrOutput)
}

// Optional. The node group settings.
func (o ClusterConfigPtrOutput) AuxiliaryNodeGroups() AuxiliaryNodeGroupArrayOutput {
	return o.ApplyT(func(v *ClusterConfig) []AuxiliaryNodeGroup {
		if v == nil {
			return nil
		}
		return v.AuxiliaryNodeGroups
	}).(AuxiliaryNodeGroupArrayOutput)
}

// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ClusterConfigPtrOutput) ConfigBucket() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.ConfigBucket
	}).(pulumi.StringPtrOutput)
}

// Optional. The config for Dataproc metrics.
func (o ClusterConfigPtrOutput) DataprocMetricConfig() DataprocMetricConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *DataprocMetricConfig {
		if v == nil {
			return nil
		}
		return v.DataprocMetricConfig
	}).(DataprocMetricConfigPtrOutput)
}

// Optional. Encryption settings for the cluster.
func (o ClusterConfigPtrOutput) EncryptionConfig() EncryptionConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *EncryptionConfig {
		if v == nil {
			return nil
		}
		return v.EncryptionConfig
	}).(EncryptionConfigPtrOutput)
}

// Optional. Port/endpoint configuration for this cluster
func (o ClusterConfigPtrOutput) EndpointConfig() EndpointConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *EndpointConfig {
		if v == nil {
			return nil
		}
		return v.EndpointConfig
	}).(EndpointConfigPtrOutput)
}

// Optional. The shared Compute Engine config settings for all instances in a cluster.
func (o ClusterConfigPtrOutput) GceClusterConfig() GceClusterConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *GceClusterConfig {
		if v == nil {
			return nil
		}
		return v.GceClusterConfig
	}).(GceClusterConfigPtrOutput)
}

// Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. These config settings are mutually exclusive with Compute Engine-based options, such as gce_cluster_config, master_config, worker_config, secondary_worker_config, and autoscaling_config.
func (o ClusterConfigPtrOutput) GkeClusterConfig() GkeClusterConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *GkeClusterConfig {
		if v == nil {
			return nil
		}
		return v.GkeClusterConfig
	}).(GkeClusterConfigPtrOutput)
}

// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
func (o ClusterConfigPtrOutput) InitializationActions() NodeInitializationActionArrayOutput {
	return o.ApplyT(func(v *ClusterConfig) []NodeInitializationAction {
		if v == nil {
			return nil
		}
		return v.InitializationActions
	}).(NodeInitializationActionArrayOutput)
}

// Optional. Lifecycle setting for the cluster.
func (o ClusterConfigPtrOutput) LifecycleConfig() LifecycleConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *LifecycleConfig {
		if v == nil {
			return nil
		}
		return v.LifecycleConfig
	}).(LifecycleConfigPtrOutput)
}

// Optional. The Compute Engine config settings for the cluster's master instance.
func (o ClusterConfigPtrOutput) MasterConfig() InstanceGroupConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *InstanceGroupConfig {
		if v == nil {
			return nil
		}
		return v.MasterConfig
	}).(InstanceGroupConfigPtrOutput)
}

// Optional. Metastore configuration.
func (o ClusterConfigPtrOutput) MetastoreConfig() MetastoreConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *MetastoreConfig {
		if v == nil {
			return nil
		}
		return v.MetastoreConfig
	}).(MetastoreConfigPtrOutput)
}

// Optional. The Compute Engine config settings for a cluster's secondary worker instances
func (o ClusterConfigPtrOutput) SecondaryWorkerConfig() InstanceGroupConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *InstanceGroupConfig {
		if v == nil {
			return nil
		}
		return v.SecondaryWorkerConfig
	}).(InstanceGroupConfigPtrOutput)
}

// Optional. Security settings for the cluster.
func (o ClusterConfigPtrOutput) SecurityConfig() SecurityConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *SecurityConfig {
		if v == nil {
			return nil
		}
		return v.SecurityConfig
	}).(SecurityConfigPtrOutput)
}

// Optional. The config settings for cluster software.
func (o ClusterConfigPtrOutput) SoftwareConfig() SoftwareConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *SoftwareConfig {
		if v == nil {
			return nil
		}
		return v.SoftwareConfig
	}).(SoftwareConfigPtrOutput)
}

// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ClusterConfigPtrOutput) TempBucket() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.TempBucket
	}).(pulumi.StringPtrOutput)
}

// Optional. The Compute Engine config settings for the cluster's worker instances.
func (o ClusterConfigPtrOutput) WorkerConfig() InstanceGroupConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *InstanceGroupConfig {
		if v == nil {
			return nil
		}
		return v.WorkerConfig
	}).(InstanceGroupConfigPtrOutput)
}

// The cluster config.
type ClusterConfigResponse struct {
	// Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
	AutoscalingConfig AutoscalingConfigResponse `pulumi:"autoscalingConfig"`
	// Optional. The node group settings.
	AuxiliaryNodeGroups []AuxiliaryNodeGroupResponse `pulumi:"auxiliaryNodeGroups"`
	// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	ConfigBucket string `pulumi:"configBucket"`
	// Optional. The config for Dataproc metrics.
	DataprocMetricConfig DataprocMetricConfigResponse `pulumi:"dataprocMetricConfig"`
	// Optional. Encryption settings for the cluster.
	EncryptionConfig EncryptionConfigResponse `pulumi:"encryptionConfig"`
	// Optional. Port/endpoint configuration for this cluster
	EndpointConfig EndpointConfigResponse `pulumi:"endpointConfig"`
	// Optional. The shared Compute Engine config settings for all instances in a cluster.
	GceClusterConfig GceClusterConfigResponse `pulumi:"gceClusterConfig"`
	// Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. These config settings are mutually exclusive with Compute Engine-based options, such as gce_cluster_config, master_config, worker_config, secondary_worker_config, and autoscaling_config.
	GkeClusterConfig GkeClusterConfigResponse `pulumi:"gkeClusterConfig"`
	// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
	InitializationActions []NodeInitializationActionResponse `pulumi:"initializationActions"`
	// Optional. Lifecycle setting for the cluster.
	LifecycleConfig LifecycleConfigResponse `pulumi:"lifecycleConfig"`
	// Optional. The Compute Engine config settings for the cluster's master instance.
	MasterConfig InstanceGroupConfigResponse `pulumi:"masterConfig"`
	// Optional. Metastore configuration.
	MetastoreConfig MetastoreConfigResponse `pulumi:"metastoreConfig"`
	// Optional. The Compute Engine config settings for a cluster's secondary worker instances
	SecondaryWorkerConfig InstanceGroupConfigResponse `pulumi:"secondaryWorkerConfig"`
	// Optional. Security settings for the cluster.
	SecurityConfig SecurityConfigResponse `pulumi:"securityConfig"`
	// Optional. The config settings for cluster software.
	SoftwareConfig SoftwareConfigResponse `pulumi:"softwareConfig"`
	// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	TempBucket string `pulumi:"tempBucket"`
	// Optional. The Compute Engine config settings for the cluster's worker instances.
	WorkerConfig InstanceGroupConfigResponse `pulumi:"workerConfig"`
}

// The cluster config.
type ClusterConfigResponseOutput struct{ *pulumi.OutputState }

func (ClusterConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterConfigResponse)(nil)).Elem()
}

func (o ClusterConfigResponseOutput) ToClusterConfigResponseOutput() ClusterConfigResponseOutput {
	return o
}

func (o ClusterConfigResponseOutput) ToClusterConfigResponseOutputWithContext(ctx context.Context) ClusterConfigResponseOutput {
	return o
}

// Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
func (o ClusterConfigResponseOutput) AutoscalingConfig() AutoscalingConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) AutoscalingConfigResponse { return v.AutoscalingConfig }).(AutoscalingConfigResponseOutput)
}

// Optional. The node group settings.
func (o ClusterConfigResponseOutput) AuxiliaryNodeGroups() AuxiliaryNodeGroupResponseArrayOutput {
	return o.ApplyT(func(v ClusterConfigResponse) []AuxiliaryNodeGroupResponse { return v.AuxiliaryNodeGroups }).(AuxiliaryNodeGroupResponseArrayOutput)
}

// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ClusterConfigResponseOutput) ConfigBucket() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterConfigResponse) string { return v.ConfigBucket }).(pulumi.StringOutput)
}

// Optional. The config for Dataproc metrics.
func (o ClusterConfigResponseOutput) DataprocMetricConfig() DataprocMetricConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) DataprocMetricConfigResponse { return v.DataprocMetricConfig }).(DataprocMetricConfigResponseOutput)
}

// Optional. Encryption settings for the cluster.
func (o ClusterConfigResponseOutput) EncryptionConfig() EncryptionConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) EncryptionConfigResponse { return v.EncryptionConfig }).(EncryptionConfigResponseOutput)
}

// Optional. Port/endpoint configuration for this cluster
func (o ClusterConfigResponseOutput) EndpointConfig() EndpointConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) EndpointConfigResponse { return v.EndpointConfig }).(EndpointConfigResponseOutput)
}

// Optional. The shared Compute Engine config settings for all instances in a cluster.
func (o ClusterConfigResponseOutput) GceClusterConfig() GceClusterConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) GceClusterConfigResponse { return v.GceClusterConfig }).(GceClusterConfigResponseOutput)
}

// Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. These config settings are mutually exclusive with Compute Engine-based options, such as gce_cluster_config, master_config, worker_config, secondary_worker_config, and autoscaling_config.
func (o ClusterConfigResponseOutput) GkeClusterConfig() GkeClusterConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) GkeClusterConfigResponse { return v.GkeClusterConfig }).(GkeClusterConfigResponseOutput)
}

// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
func (o ClusterConfigResponseOutput) InitializationActions() NodeInitializationActionResponseArrayOutput {
	return o.ApplyT(func(v ClusterConfigResponse) []NodeInitializationActionResponse { return v.InitializationActions }).(NodeInitializationActionResponseArrayOutput)
}

// Optional. Lifecycle setting for the cluster.
func (o ClusterConfigResponseOutput) LifecycleConfig() LifecycleConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) LifecycleConfigResponse { return v.LifecycleConfig }).(LifecycleConfigResponseOutput)
}

// Optional. The Compute Engine config settings for the cluster's master instance.
func (o ClusterConfigResponseOutput) MasterConfig() InstanceGroupConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) InstanceGroupConfigResponse { return v.MasterConfig }).(InstanceGroupConfigResponseOutput)
}

// Optional. Metastore configuration.
func (o ClusterConfigResponseOutput) MetastoreConfig() MetastoreConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) MetastoreConfigResponse { return v.MetastoreConfig }).(MetastoreConfigResponseOutput)
}

// Optional. The Compute Engine config settings for a cluster's secondary worker instances
func (o ClusterConfigResponseOutput) SecondaryWorkerConfig() InstanceGroupConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) InstanceGroupConfigResponse { return v.SecondaryWorkerConfig }).(InstanceGroupConfigResponseOutput)
}

// Optional. Security settings for the cluster.
func (o ClusterConfigResponseOutput) SecurityConfig() SecurityConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) SecurityConfigResponse { return v.SecurityConfig }).(SecurityConfigResponseOutput)
}

// Optional. The config settings for cluster software.
func (o ClusterConfigResponseOutput) SoftwareConfig() SoftwareConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) SoftwareConfigResponse { return v.SoftwareConfig }).(SoftwareConfigResponseOutput)
}

// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ClusterConfigResponseOutput) TempBucket() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterConfigResponse) string { return v.TempBucket }).(pulumi.StringOutput)
}

// Optional. The Compute Engine config settings for the cluster's worker instances.
func (o ClusterConfigResponseOutput) WorkerConfig() InstanceGroupConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) InstanceGroupConfigResponse { return v.WorkerConfig }).(InstanceGroupConfigResponseOutput)
}

// Contains cluster daemon metrics, such as HDFS and YARN stats.Beta Feature: This report is available for testing purposes only. It may be changed before final release.
type ClusterMetricsResponse struct {
	// The HDFS metrics.
	HdfsMetrics map[string]string `pulumi:"hdfsMetrics"`
	// YARN metrics.
	YarnMetrics map[string]string `pulumi:"yarnMetrics"`
}

// Contains cluster daemon metrics, such as HDFS and YARN stats.Beta Feature: This report is available for testing purposes only. It may be changed before final release.
type ClusterMetricsResponseOutput struct{ *pulumi.OutputState }

func (ClusterMetricsResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterMetricsResponse)(nil)).Elem()
}

func (o ClusterMetricsResponseOutput) ToClusterMetricsResponseOutput() ClusterMetricsResponseOutput {
	return o
}

func (o ClusterMetricsResponseOutput) ToClusterMetricsResponseOutputWithContext(ctx context.Context) ClusterMetricsResponseOutput {
	return o
}

// The HDFS metrics.
func (o ClusterMetricsResponseOutput) HdfsMetrics() pulumi.StringMapOutput {
	return o.ApplyT(func(v ClusterMetricsResponse) map[string]string { return v.HdfsMetrics }).(pulumi.StringMapOutput)
}

// YARN metrics.
func (o ClusterMetricsResponseOutput) YarnMetrics() pulumi.StringMapOutput {
	return o.ApplyT(func(v ClusterMetricsResponse) map[string]string { return v.YarnMetrics }).(pulumi.StringMapOutput)
}

// A selector that chooses target cluster for jobs based on metadata.
type ClusterSelector struct {
	// The cluster labels. Cluster must have all labels to match.
	ClusterLabels map[string]string `pulumi:"clusterLabels"`
	// Optional. The zone where workflow process executes. This parameter does not affect the selection of the cluster.If unspecified, the zone of the first cluster matching the selector is used.
	Zone *string `pulumi:"zone"`
}

// ClusterSelectorInput is an input type that accepts ClusterSelectorArgs and ClusterSelectorOutput values.
// You can construct a concrete instance of `ClusterSelectorInput` via:
//
//	ClusterSelectorArgs{...}
type ClusterSelectorInput interface {
	pulumi.Input

	ToClusterSelectorOutput() ClusterSelectorOutput
	ToClusterSelectorOutputWithContext(context.Context) ClusterSelectorOutput
}

// A selector that chooses target cluster for jobs based on metadata.
type ClusterSelectorArgs struct {
	// The cluster labels. Cluster must have all labels to match.
	ClusterLabels pulumi.StringMapInput `pulumi:"clusterLabels"`
	// Optional. The zone where workflow process executes. This parameter does not affect the selection of the cluster.If unspecified, the zone of the first cluster matching the selector is used.
	Zone pulumi.StringPtrInput `pulumi:"zone"`
}

func (ClusterSelectorArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterSelector)(nil)).Elem()
}

func (i ClusterSelectorArgs) ToClusterSelectorOutput() ClusterSelectorOutput {
	return i.ToClusterSelectorOutputWithContext(context.Background())
}

func (i ClusterSelectorArgs) ToClusterSelectorOutputWithContext(ctx context.Context) ClusterSelectorOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterSelectorOutput)
}

func (i ClusterSelectorArgs) ToClusterSelectorPtrOutput() ClusterSelectorPtrOutput {
	return i.ToClusterSelectorPtrOutputWithContext(context.Background())
}

func (i ClusterSelectorArgs) ToClusterSelectorPtrOutputWithContext(ctx context.Context) ClusterSelectorPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterSelectorOutput).ToClusterSelectorPtrOutputWithContext(ctx)
}

// ClusterSelectorPtrInput is an input type that accepts ClusterSelectorArgs, ClusterSelectorPtr and ClusterSelectorPtrOutput values.
// You can construct a concrete instance of `ClusterSelectorPtrInput` via:
//
//	        ClusterSelectorArgs{...}
//
//	or:
//
//	        nil
type ClusterSelectorPtrInput interface {
	pulumi.Input

	ToClusterSelectorPtrOutput() ClusterSelectorPtrOutput
	ToClusterSelectorPtrOutputWithContext(context.Context) ClusterSelectorPtrOutput
}

type clusterSelectorPtrType ClusterSelectorArgs

func ClusterSelectorPtr(v *ClusterSelectorArgs) ClusterSelectorPtrInput {
	return (*clusterSelectorPtrType)(v)
}

func (*clusterSelectorPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ClusterSelector)(nil)).Elem()
}

func (i *clusterSelectorPtrType) ToClusterSelectorPtrOutput() ClusterSelectorPtrOutput {
	return i.ToClusterSelectorPtrOutputWithContext(context.Background())
}

func (i *clusterSelectorPtrType) ToClusterSelectorPtrOutputWithContext(ctx context.Context) ClusterSelectorPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterSelectorPtrOutput)
}

// A selector that chooses target cluster for jobs based on metadata.
type ClusterSelectorOutput struct{ *pulumi.OutputState }

func (ClusterSelectorOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterSelector)(nil)).Elem()
}

func (o ClusterSelectorOutput) ToClusterSelectorOutput() ClusterSelectorOutput {
	return o
}

func (o ClusterSelectorOutput) ToClusterSelectorOutputWithContext(ctx context.Context) ClusterSelectorOutput {
	return o
}

func (o ClusterSelectorOutput) ToClusterSelectorPtrOutput() ClusterSelectorPtrOutput {
	return o.ToClusterSelectorPtrOutputWithContext(context.Background())
}

func (o ClusterSelectorOutput) ToClusterSelectorPtrOutputWithContext(ctx context.Context) ClusterSelectorPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ClusterSelector) *ClusterSelector {
		return &v
	}).(ClusterSelectorPtrOutput)
}

// The cluster labels. Cluster must have all labels to match.
func (o ClusterSelectorOutput) ClusterLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v ClusterSelector) map[string]string { return v.ClusterLabels }).(pulumi.StringMapOutput)
}

// Optional. The zone where workflow process executes. This parameter does not affect the selection of the cluster.If unspecified, the zone of the first cluster matching the selector is used.
func (o ClusterSelectorOutput) Zone() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ClusterSelector) *string { return v.Zone }).(pulumi.StringPtrOutput)
}

type ClusterSelectorPtrOutput struct{ *pulumi.OutputState }

func (ClusterSelectorPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ClusterSelector)(nil)).Elem()
}

func (o ClusterSelectorPtrOutput) ToClusterSelectorPtrOutput() ClusterSelectorPtrOutput {
	return o
}

func (o ClusterSelectorPtrOutput) ToClusterSelectorPtrOutputWithContext(ctx context.Context) ClusterSelectorPtrOutput {
	return o
}

func (o ClusterSelectorPtrOutput) Elem() ClusterSelectorOutput {
	return o.ApplyT(func(v *ClusterSelector) ClusterSelector {
		if v != nil {
			return *v
		}
		var ret ClusterSelector
		return ret
	}).(ClusterSelectorOutput)
}

// The cluster labels. Cluster must have all labels to match.
func (o ClusterSelectorPtrOutput) ClusterLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *ClusterSelector) map[string]string {
		if v == nil {
			return nil
		}
		return v.ClusterLabels
	}).(pulumi.StringMapOutput)
}

// Optional. The zone where workflow process executes. This parameter does not affect the selection of the cluster.If unspecified, the zone of the first cluster matching the selector is used.
func (o ClusterSelectorPtrOutput) Zone() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ClusterSelector) *string {
		if v == nil {
			return nil
		}
		return v.Zone
	}).(pulumi.StringPtrOutput)
}

// A selector that chooses target cluster for jobs based on metadata.
type ClusterSelectorResponse struct {
	// The cluster labels. Cluster must have all labels to match.
	ClusterLabels map[string]string `pulumi:"clusterLabels"`
	// Optional. The zone where workflow process executes. This parameter does not affect the selection of the cluster.If unspecified, the zone of the first cluster matching the selector is used.
	Zone string `pulumi:"zone"`
}

// A selector that chooses target cluster for jobs based on metadata.
type ClusterSelectorResponseOutput struct{ *pulumi.OutputState }

func (ClusterSelectorResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterSelectorResponse)(nil)).Elem()
}

func (o ClusterSelectorResponseOutput) ToClusterSelectorResponseOutput() ClusterSelectorResponseOutput {
	return o
}

func (o ClusterSelectorResponseOutput) ToClusterSelectorResponseOutputWithContext(ctx context.Context) ClusterSelectorResponseOutput {
	return o
}

// The cluster labels. Cluster must have all labels to match.
func (o ClusterSelectorResponseOutput) ClusterLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v ClusterSelectorResponse) map[string]string { return v.ClusterLabels }).(pulumi.StringMapOutput)
}

// Optional. The zone where workflow process executes. This parameter does not affect the selection of the cluster.If unspecified, the zone of the first cluster matching the selector is used.
func (o ClusterSelectorResponseOutput) Zone() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterSelectorResponse) string { return v.Zone }).(pulumi.StringOutput)
}

// The status of a cluster and its instances.
type ClusterStatusResponse struct {
	// Optional. Output only. Details of cluster's state.
	Detail string `pulumi:"detail"`
	// The cluster's state.
	State string `pulumi:"state"`
	// Time when this state was entered (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	StateStartTime string `pulumi:"stateStartTime"`
	// Additional state information that includes status reported by the agent.
	Substate string `pulumi:"substate"`
}

// The status of a cluster and its instances.
type ClusterStatusResponseOutput struct{ *pulumi.OutputState }

func (ClusterStatusResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterStatusResponse)(nil)).Elem()
}

func (o ClusterStatusResponseOutput) ToClusterStatusResponseOutput() ClusterStatusResponseOutput {
	return o
}

func (o ClusterStatusResponseOutput) ToClusterStatusResponseOutputWithContext(ctx context.Context) ClusterStatusResponseOutput {
	return o
}

// Optional. Output only. Details of cluster's state.
func (o ClusterStatusResponseOutput) Detail() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterStatusResponse) string { return v.Detail }).(pulumi.StringOutput)
}

// The cluster's state.
func (o ClusterStatusResponseOutput) State() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterStatusResponse) string { return v.State }).(pulumi.StringOutput)
}

// Time when this state was entered (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o ClusterStatusResponseOutput) StateStartTime() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterStatusResponse) string { return v.StateStartTime }).(pulumi.StringOutput)
}

// Additional state information that includes status reported by the agent.
func (o ClusterStatusResponseOutput) Substate() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterStatusResponse) string { return v.Substate }).(pulumi.StringOutput)
}

type ClusterStatusResponseArrayOutput struct{ *pulumi.OutputState }

func (ClusterStatusResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]ClusterStatusResponse)(nil)).Elem()
}

func (o ClusterStatusResponseArrayOutput) ToClusterStatusResponseArrayOutput() ClusterStatusResponseArrayOutput {
	return o
}

func (o ClusterStatusResponseArrayOutput) ToClusterStatusResponseArrayOutputWithContext(ctx context.Context) ClusterStatusResponseArrayOutput {
	return o
}

func (o ClusterStatusResponseArrayOutput) Index(i pulumi.IntInput) ClusterStatusResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) ClusterStatusResponse {
		return vs[0].([]ClusterStatusResponse)[vs[1].(int)]
	}).(ClusterStatusResponseOutput)
}

// Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs)
type ConfidentialInstanceConfig struct {
	// Optional. Defines whether the instance should have confidential compute enabled.
	EnableConfidentialCompute *bool `pulumi:"enableConfidentialCompute"`
}

// ConfidentialInstanceConfigInput is an input type that accepts ConfidentialInstanceConfigArgs and ConfidentialInstanceConfigOutput values.
// You can construct a concrete instance of `ConfidentialInstanceConfigInput` via:
//
//	ConfidentialInstanceConfigArgs{...}
type ConfidentialInstanceConfigInput interface {
	pulumi.Input

	ToConfidentialInstanceConfigOutput() ConfidentialInstanceConfigOutput
	ToConfidentialInstanceConfigOutputWithContext(context.Context) ConfidentialInstanceConfigOutput
}

// Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs)
type ConfidentialInstanceConfigArgs struct {
	// Optional. Defines whether the instance should have confidential compute enabled.
	EnableConfidentialCompute pulumi.BoolPtrInput `pulumi:"enableConfidentialCompute"`
}

func (ConfidentialInstanceConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ConfidentialInstanceConfig)(nil)).Elem()
}

func (i ConfidentialInstanceConfigArgs) ToConfidentialInstanceConfigOutput() ConfidentialInstanceConfigOutput {
	return i.ToConfidentialInstanceConfigOutputWithContext(context.Background())
}

func (i ConfidentialInstanceConfigArgs) ToConfidentialInstanceConfigOutputWithContext(ctx context.Context) ConfidentialInstanceConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ConfidentialInstanceConfigOutput)
}

func (i ConfidentialInstanceConfigArgs) ToConfidentialInstanceConfigPtrOutput() ConfidentialInstanceConfigPtrOutput {
	return i.ToConfidentialInstanceConfigPtrOutputWithContext(context.Background())
}

func (i ConfidentialInstanceConfigArgs) ToConfidentialInstanceConfigPtrOutputWithContext(ctx context.Context) ConfidentialInstanceConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ConfidentialInstanceConfigOutput).ToConfidentialInstanceConfigPtrOutputWithContext(ctx)
}

// ConfidentialInstanceConfigPtrInput is an input type that accepts ConfidentialInstanceConfigArgs, ConfidentialInstanceConfigPtr and ConfidentialInstanceConfigPtrOutput values.
// You can construct a concrete instance of `ConfidentialInstanceConfigPtrInput` via:
//
//	        ConfidentialInstanceConfigArgs{...}
//
//	or:
//
//	        nil
type ConfidentialInstanceConfigPtrInput interface {
	pulumi.Input

	ToConfidentialInstanceConfigPtrOutput() ConfidentialInstanceConfigPtrOutput
	ToConfidentialInstanceConfigPtrOutputWithContext(context.Context) ConfidentialInstanceConfigPtrOutput
}

type confidentialInstanceConfigPtrType ConfidentialInstanceConfigArgs

func ConfidentialInstanceConfigPtr(v *ConfidentialInstanceConfigArgs) ConfidentialInstanceConfigPtrInput {
	return (*confidentialInstanceConfigPtrType)(v)
}

func (*confidentialInstanceConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ConfidentialInstanceConfig)(nil)).Elem()
}

func (i *confidentialInstanceConfigPtrType) ToConfidentialInstanceConfigPtrOutput() ConfidentialInstanceConfigPtrOutput {
	return i.ToConfidentialInstanceConfigPtrOutputWithContext(context.Background())
}

func (i *confidentialInstanceConfigPtrType) ToConfidentialInstanceConfigPtrOutputWithContext(ctx context.Context) ConfidentialInstanceConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ConfidentialInstanceConfigPtrOutput)
}

// Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs)
type ConfidentialInstanceConfigOutput struct{ *pulumi.OutputState }

func (ConfidentialInstanceConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ConfidentialInstanceConfig)(nil)).Elem()
}

func (o ConfidentialInstanceConfigOutput) ToConfidentialInstanceConfigOutput() ConfidentialInstanceConfigOutput {
	return o
}

func (o ConfidentialInstanceConfigOutput) ToConfidentialInstanceConfigOutputWithContext(ctx context.Context) ConfidentialInstanceConfigOutput {
	return o
}

func (o ConfidentialInstanceConfigOutput) ToConfidentialInstanceConfigPtrOutput() ConfidentialInstanceConfigPtrOutput {
	return o.ToConfidentialInstanceConfigPtrOutputWithContext(context.Background())
}

func (o ConfidentialInstanceConfigOutput) ToConfidentialInstanceConfigPtrOutputWithContext(ctx context.Context) ConfidentialInstanceConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ConfidentialInstanceConfig) *ConfidentialInstanceConfig {
		return &v
	}).(ConfidentialInstanceConfigPtrOutput)
}

// Optional. Defines whether the instance should have confidential compute enabled.
func (o ConfidentialInstanceConfigOutput) EnableConfidentialCompute() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v ConfidentialInstanceConfig) *bool { return v.EnableConfidentialCompute }).(pulumi.BoolPtrOutput)
}

type ConfidentialInstanceConfigPtrOutput struct{ *pulumi.OutputState }

func (ConfidentialInstanceConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ConfidentialInstanceConfig)(nil)).Elem()
}

func (o ConfidentialInstanceConfigPtrOutput) ToConfidentialInstanceConfigPtrOutput() ConfidentialInstanceConfigPtrOutput {
	return o
}

func (o ConfidentialInstanceConfigPtrOutput) ToConfidentialInstanceConfigPtrOutputWithContext(ctx context.Context) ConfidentialInstanceConfigPtrOutput {
	return o
}

func (o ConfidentialInstanceConfigPtrOutput) Elem() ConfidentialInstanceConfigOutput {
	return o.ApplyT(func(v *ConfidentialInstanceConfig) ConfidentialInstanceConfig {
		if v != nil {
			return *v
		}
		var ret ConfidentialInstanceConfig
		return ret
	}).(ConfidentialInstanceConfigOutput)
}

// Optional. Defines whether the instance should have confidential compute enabled.
func (o ConfidentialInstanceConfigPtrOutput) EnableConfidentialCompute() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *ConfidentialInstanceConfig) *bool {
		if v == nil {
			return nil
		}
		return v.EnableConfidentialCompute
	}).(pulumi.BoolPtrOutput)
}

// Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs)
type ConfidentialInstanceConfigResponse struct {
	// Optional. Defines whether the instance should have confidential compute enabled.
	EnableConfidentialCompute bool `pulumi:"enableConfidentialCompute"`
}

// Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs)
type ConfidentialInstanceConfigResponseOutput struct{ *pulumi.OutputState }

func (ConfidentialInstanceConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ConfidentialInstanceConfigResponse)(nil)).Elem()
}

func (o ConfidentialInstanceConfigResponseOutput) ToConfidentialInstanceConfigResponseOutput() ConfidentialInstanceConfigResponseOutput {
	return o
}

func (o ConfidentialInstanceConfigResponseOutput) ToConfidentialInstanceConfigResponseOutputWithContext(ctx context.Context) ConfidentialInstanceConfigResponseOutput {
	return o
}

// Optional. Defines whether the instance should have confidential compute enabled.
func (o ConfidentialInstanceConfigResponseOutput) EnableConfidentialCompute() pulumi.BoolOutput {
	return o.ApplyT(func(v ConfidentialInstanceConfigResponse) bool { return v.EnableConfidentialCompute }).(pulumi.BoolOutput)
}

// Dataproc metric config.
type DataprocMetricConfig struct {
	// Metrics sources to enable.
	Metrics []Metric `pulumi:"metrics"`
}

// DataprocMetricConfigInput is an input type that accepts DataprocMetricConfigArgs and DataprocMetricConfigOutput values.
// You can construct a concrete instance of `DataprocMetricConfigInput` via:
//
//	DataprocMetricConfigArgs{...}
type DataprocMetricConfigInput interface {
	pulumi.Input

	ToDataprocMetricConfigOutput() DataprocMetricConfigOutput
	ToDataprocMetricConfigOutputWithContext(context.Context) DataprocMetricConfigOutput
}

// Dataproc metric config.
type DataprocMetricConfigArgs struct {
	// Metrics sources to enable.
	Metrics MetricArrayInput `pulumi:"metrics"`
}

func (DataprocMetricConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*DataprocMetricConfig)(nil)).Elem()
}

func (i DataprocMetricConfigArgs) ToDataprocMetricConfigOutput() DataprocMetricConfigOutput {
	return i.ToDataprocMetricConfigOutputWithContext(context.Background())
}

func (i DataprocMetricConfigArgs) ToDataprocMetricConfigOutputWithContext(ctx context.Context) DataprocMetricConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DataprocMetricConfigOutput)
}

func (i DataprocMetricConfigArgs) ToDataprocMetricConfigPtrOutput() DataprocMetricConfigPtrOutput {
	return i.ToDataprocMetricConfigPtrOutputWithContext(context.Background())
}

func (i DataprocMetricConfigArgs) ToDataprocMetricConfigPtrOutputWithContext(ctx context.Context) DataprocMetricConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DataprocMetricConfigOutput).ToDataprocMetricConfigPtrOutputWithContext(ctx)
}

// DataprocMetricConfigPtrInput is an input type that accepts DataprocMetricConfigArgs, DataprocMetricConfigPtr and DataprocMetricConfigPtrOutput values.
// You can construct a concrete instance of `DataprocMetricConfigPtrInput` via:
//
//	        DataprocMetricConfigArgs{...}
//
//	or:
//
//	        nil
type DataprocMetricConfigPtrInput interface {
	pulumi.Input

	ToDataprocMetricConfigPtrOutput() DataprocMetricConfigPtrOutput
	ToDataprocMetricConfigPtrOutputWithContext(context.Context) DataprocMetricConfigPtrOutput
}

type dataprocMetricConfigPtrType DataprocMetricConfigArgs

func DataprocMetricConfigPtr(v *DataprocMetricConfigArgs) DataprocMetricConfigPtrInput {
	return (*dataprocMetricConfigPtrType)(v)
}

func (*dataprocMetricConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**DataprocMetricConfig)(nil)).Elem()
}

func (i *dataprocMetricConfigPtrType) ToDataprocMetricConfigPtrOutput() DataprocMetricConfigPtrOutput {
	return i.ToDataprocMetricConfigPtrOutputWithContext(context.Background())
}

func (i *dataprocMetricConfigPtrType) ToDataprocMetricConfigPtrOutputWithContext(ctx context.Context) DataprocMetricConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DataprocMetricConfigPtrOutput)
}

// Dataproc metric config.
type DataprocMetricConfigOutput struct{ *pulumi.OutputState }

func (DataprocMetricConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*DataprocMetricConfig)(nil)).Elem()
}

func (o DataprocMetricConfigOutput) ToDataprocMetricConfigOutput() DataprocMetricConfigOutput {
	return o
}

func (o DataprocMetricConfigOutput) ToDataprocMetricConfigOutputWithContext(ctx context.Context) DataprocMetricConfigOutput {
	return o
}

func (o DataprocMetricConfigOutput) ToDataprocMetricConfigPtrOutput() DataprocMetricConfigPtrOutput {
	return o.ToDataprocMetricConfigPtrOutputWithContext(context.Background())
}

func (o DataprocMetricConfigOutput) ToDataprocMetricConfigPtrOutputWithContext(ctx context.Context) DataprocMetricConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v DataprocMetricConfig) *DataprocMetricConfig {
		return &v
	}).(DataprocMetricConfigPtrOutput)
}

// Metrics sources to enable.
func (o DataprocMetricConfigOutput) Metrics() MetricArrayOutput {
	return o.ApplyT(func(v DataprocMetricConfig) []Metric { return v.Metrics }).(MetricArrayOutput)
}

type DataprocMetricConfigPtrOutput struct{ *pulumi.OutputState }

func (DataprocMetricConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**DataprocMetricConfig)(nil)).Elem()
}

func (o DataprocMetricConfigPtrOutput) ToDataprocMetricConfigPtrOutput() DataprocMetricConfigPtrOutput {
	return o
}

func (o DataprocMetricConfigPtrOutput) ToDataprocMetricConfigPtrOutputWithContext(ctx context.Context) DataprocMetricConfigPtrOutput {
	return o
}

func (o DataprocMetricConfigPtrOutput) Elem() DataprocMetricConfigOutput {
	return o.ApplyT(func(v *DataprocMetricConfig) DataprocMetricConfig {
		if v != nil {
			return *v
		}
		var ret DataprocMetricConfig
		return ret
	}).(DataprocMetricConfigOutput)
}

// Metrics sources to enable.
func (o DataprocMetricConfigPtrOutput) Metrics() MetricArrayOutput {
	return o.ApplyT(func(v *DataprocMetricConfig) []Metric {
		if v == nil {
			return nil
		}
		return v.Metrics
	}).(MetricArrayOutput)
}

// Dataproc metric config.
type DataprocMetricConfigResponse struct {
	// Metrics sources to enable.
	Metrics []MetricResponse `pulumi:"metrics"`
}

// Dataproc metric config.
type DataprocMetricConfigResponseOutput struct{ *pulumi.OutputState }

func (DataprocMetricConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*DataprocMetricConfigResponse)(nil)).Elem()
}

func (o DataprocMetricConfigResponseOutput) ToDataprocMetricConfigResponseOutput() DataprocMetricConfigResponseOutput {
	return o
}

func (o DataprocMetricConfigResponseOutput) ToDataprocMetricConfigResponseOutputWithContext(ctx context.Context) DataprocMetricConfigResponseOutput {
	return o
}

// Metrics sources to enable.
func (o DataprocMetricConfigResponseOutput) Metrics() MetricResponseArrayOutput {
	return o.ApplyT(func(v DataprocMetricConfigResponse) []MetricResponse { return v.Metrics }).(MetricResponseArrayOutput)
}

// Specifies the config of disk options for a group of VM instances.
type DiskConfig struct {
	// Optional. Size in GB of the boot disk (default is 500GB).
	BootDiskSizeGb *int `pulumi:"bootDiskSizeGb"`
	// Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See Disk types (https://cloud.google.com/compute/docs/disks#disk-types).
	BootDiskType *string `pulumi:"bootDiskType"`
	// Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See local SSD performance (https://cloud.google.com/compute/docs/disks/local-ssd#performance).
	LocalSsdInterface *string `pulumi:"localSsdInterface"`
	// Optional. Number of attached SSDs, from 0 to 8 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and HDFS (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.Note: Local SSD options may vary by machine type and number of vCPUs selected.
	NumLocalSsds *int `pulumi:"numLocalSsds"`
}

// DiskConfigInput is an input type that accepts DiskConfigArgs and DiskConfigOutput values.
// You can construct a concrete instance of `DiskConfigInput` via:
//
//	DiskConfigArgs{...}
type DiskConfigInput interface {
	pulumi.Input

	ToDiskConfigOutput() DiskConfigOutput
	ToDiskConfigOutputWithContext(context.Context) DiskConfigOutput
}

// Specifies the config of disk options for a group of VM instances.
type DiskConfigArgs struct {
	// Optional. Size in GB of the boot disk (default is 500GB).
	BootDiskSizeGb pulumi.IntPtrInput `pulumi:"bootDiskSizeGb"`
	// Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See Disk types (https://cloud.google.com/compute/docs/disks#disk-types).
	BootDiskType pulumi.StringPtrInput `pulumi:"bootDiskType"`
	// Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See local SSD performance (https://cloud.google.com/compute/docs/disks/local-ssd#performance).
	LocalSsdInterface pulumi.StringPtrInput `pulumi:"localSsdInterface"`
	// Optional. Number of attached SSDs, from 0 to 8 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and HDFS (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.Note: Local SSD options may vary by machine type and number of vCPUs selected.
	NumLocalSsds pulumi.IntPtrInput `pulumi:"numLocalSsds"`
}

func (DiskConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*DiskConfig)(nil)).Elem()
}

func (i DiskConfigArgs) ToDiskConfigOutput() DiskConfigOutput {
	return i.ToDiskConfigOutputWithContext(context.Background())
}

func (i DiskConfigArgs) ToDiskConfigOutputWithContext(ctx context.Context) DiskConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DiskConfigOutput)
}

func (i DiskConfigArgs) ToDiskConfigPtrOutput() DiskConfigPtrOutput {
	return i.ToDiskConfigPtrOutputWithContext(context.Background())
}

func (i DiskConfigArgs) ToDiskConfigPtrOutputWithContext(ctx context.Context) DiskConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DiskConfigOutput).ToDiskConfigPtrOutputWithContext(ctx)
}

// DiskConfigPtrInput is an input type that accepts DiskConfigArgs, DiskConfigPtr and DiskConfigPtrOutput values.
// You can construct a concrete instance of `DiskConfigPtrInput` via:
//
//	        DiskConfigArgs{...}
//
//	or:
//
//	        nil
type DiskConfigPtrInput interface {
	pulumi.Input

	ToDiskConfigPtrOutput() DiskConfigPtrOutput
	ToDiskConfigPtrOutputWithContext(context.Context) DiskConfigPtrOutput
}

type diskConfigPtrType DiskConfigArgs

func DiskConfigPtr(v *DiskConfigArgs) DiskConfigPtrInput {
	return (*diskConfigPtrType)(v)
}

func (*diskConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**DiskConfig)(nil)).Elem()
}

func (i *diskConfigPtrType) ToDiskConfigPtrOutput() DiskConfigPtrOutput {
	return i.ToDiskConfigPtrOutputWithContext(context.Background())
}

func (i *diskConfigPtrType) ToDiskConfigPtrOutputWithContext(ctx context.Context) DiskConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DiskConfigPtrOutput)
}

// Specifies the config of disk options for a group of VM instances.
type DiskConfigOutput struct{ *pulumi.OutputState }

func (DiskConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*DiskConfig)(nil)).Elem()
}

func (o DiskConfigOutput) ToDiskConfigOutput() DiskConfigOutput {
	return o
}

func (o DiskConfigOutput) ToDiskConfigOutputWithContext(ctx context.Context) DiskConfigOutput {
	return o
}

func (o DiskConfigOutput) ToDiskConfigPtrOutput() DiskConfigPtrOutput {
	return o.ToDiskConfigPtrOutputWithContext(context.Background())
}

func (o DiskConfigOutput) ToDiskConfigPtrOutputWithContext(ctx context.Context) DiskConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v DiskConfig) *DiskConfig {
		return &v
	}).(DiskConfigPtrOutput)
}

// Optional. Size in GB of the boot disk (default is 500GB).
func (o DiskConfigOutput) BootDiskSizeGb() pulumi.IntPtrOutput {
	return o.ApplyT(func(v DiskConfig) *int { return v.BootDiskSizeGb }).(pulumi.IntPtrOutput)
}

// Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See Disk types (https://cloud.google.com/compute/docs/disks#disk-types).
func (o DiskConfigOutput) BootDiskType() pulumi.StringPtrOutput {
	return o.ApplyT(func(v DiskConfig) *string { return v.BootDiskType }).(pulumi.StringPtrOutput)
}

// Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See local SSD performance (https://cloud.google.com/compute/docs/disks/local-ssd#performance).
func (o DiskConfigOutput) LocalSsdInterface() pulumi.StringPtrOutput {
	return o.ApplyT(func(v DiskConfig) *string { return v.LocalSsdInterface }).(pulumi.StringPtrOutput)
}

// Optional. Number of attached SSDs, from 0 to 8 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and HDFS (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.Note: Local SSD options may vary by machine type and number of vCPUs selected.
func (o DiskConfigOutput) NumLocalSsds() pulumi.IntPtrOutput {
	return o.ApplyT(func(v DiskConfig) *int { return v.NumLocalSsds }).(pulumi.IntPtrOutput)
}

type DiskConfigPtrOutput struct{ *pulumi.OutputState }

func (DiskConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**DiskConfig)(nil)).Elem()
}

func (o DiskConfigPtrOutput) ToDiskConfigPtrOutput() DiskConfigPtrOutput {
	return o
}

func (o DiskConfigPtrOutput) ToDiskConfigPtrOutputWithContext(ctx context.Context) DiskConfigPtrOutput {
	return o
}

func (o DiskConfigPtrOutput) Elem() DiskConfigOutput {
	return o.ApplyT(func(v *DiskConfig) DiskConfig {
		if v != nil {
			return *v
		}
		var ret DiskConfig
		return ret
	}).(DiskConfigOutput)
}

// Optional. Size in GB of the boot disk (default is 500GB).
func (o DiskConfigPtrOutput) BootDiskSizeGb() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *DiskConfig) *int {
		if v == nil {
			return nil
		}
		return v.BootDiskSizeGb
	}).(pulumi.IntPtrOutput)
}

// Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See Disk types (https://cloud.google.com/compute/docs/disks#disk-types).
func (o DiskConfigPtrOutput) BootDiskType() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *DiskConfig) *string {
		if v == nil {
			return nil
		}
		return v.BootDiskType
	}).(pulumi.StringPtrOutput)
}

// Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See local SSD performance (https://cloud.google.com/compute/docs/disks/local-ssd#performance).
func (o DiskConfigPtrOutput) LocalSsdInterface() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *DiskConfig) *string {
		if v == nil {
			return nil
		}
		return v.LocalSsdInterface
	}).(pulumi.StringPtrOutput)
}

// Optional. Number of attached SSDs, from 0 to 8 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and HDFS (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.Note: Local SSD options may vary by machine type and number of vCPUs selected.
func (o DiskConfigPtrOutput) NumLocalSsds() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *DiskConfig) *int {
		if v == nil {
			return nil
		}
		return v.NumLocalSsds
	}).(pulumi.IntPtrOutput)
}

// Specifies the config of disk options for a group of VM instances.
type DiskConfigResponse struct {
	// Optional. Size in GB of the boot disk (default is 500GB).
	BootDiskSizeGb int `pulumi:"bootDiskSizeGb"`
	// Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See Disk types (https://cloud.google.com/compute/docs/disks#disk-types).
	BootDiskType string `pulumi:"bootDiskType"`
	// Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See local SSD performance (https://cloud.google.com/compute/docs/disks/local-ssd#performance).
	LocalSsdInterface string `pulumi:"localSsdInterface"`
	// Optional. Number of attached SSDs, from 0 to 8 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and HDFS (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.Note: Local SSD options may vary by machine type and number of vCPUs selected.
	NumLocalSsds int `pulumi:"numLocalSsds"`
}

// Specifies the config of disk options for a group of VM instances.
type DiskConfigResponseOutput struct{ *pulumi.OutputState }

func (DiskConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*DiskConfigResponse)(nil)).Elem()
}

func (o DiskConfigResponseOutput) ToDiskConfigResponseOutput() DiskConfigResponseOutput {
	return o
}

func (o DiskConfigResponseOutput) ToDiskConfigResponseOutputWithContext(ctx context.Context) DiskConfigResponseOutput {
	return o
}

// Optional. Size in GB of the boot disk (default is 500GB).
func (o DiskConfigResponseOutput) BootDiskSizeGb() pulumi.IntOutput {
	return o.ApplyT(func(v DiskConfigResponse) int { return v.BootDiskSizeGb }).(pulumi.IntOutput)
}

// Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See Disk types (https://cloud.google.com/compute/docs/disks#disk-types).
func (o DiskConfigResponseOutput) BootDiskType() pulumi.StringOutput {
	return o.ApplyT(func(v DiskConfigResponse) string { return v.BootDiskType }).(pulumi.StringOutput)
}

// Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See local SSD performance (https://cloud.google.com/compute/docs/disks/local-ssd#performance).
func (o DiskConfigResponseOutput) LocalSsdInterface() pulumi.StringOutput {
	return o.ApplyT(func(v DiskConfigResponse) string { return v.LocalSsdInterface }).(pulumi.StringOutput)
}

// Optional. Number of attached SSDs, from 0 to 8 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and HDFS (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.Note: Local SSD options may vary by machine type and number of vCPUs selected.
func (o DiskConfigResponseOutput) NumLocalSsds() pulumi.IntOutput {
	return o.ApplyT(func(v DiskConfigResponse) int { return v.NumLocalSsds }).(pulumi.IntOutput)
}

// Driver scheduling configuration.
type DriverSchedulingConfig struct {
	// The amount of memory in MB the driver is requesting.
	MemoryMb int `pulumi:"memoryMb"`
	// The number of vCPUs the driver is requesting.
	Vcores int `pulumi:"vcores"`
}

// DriverSchedulingConfigInput is an input type that accepts DriverSchedulingConfigArgs and DriverSchedulingConfigOutput values.
// You can construct a concrete instance of `DriverSchedulingConfigInput` via:
//
//	DriverSchedulingConfigArgs{...}
type DriverSchedulingConfigInput interface {
	pulumi.Input

	ToDriverSchedulingConfigOutput() DriverSchedulingConfigOutput
	ToDriverSchedulingConfigOutputWithContext(context.Context) DriverSchedulingConfigOutput
}

// Driver scheduling configuration.
type DriverSchedulingConfigArgs struct {
	// The amount of memory in MB the driver is requesting.
	MemoryMb pulumi.IntInput `pulumi:"memoryMb"`
	// The number of vCPUs the driver is requesting.
	Vcores pulumi.IntInput `pulumi:"vcores"`
}

func (DriverSchedulingConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*DriverSchedulingConfig)(nil)).Elem()
}

func (i DriverSchedulingConfigArgs) ToDriverSchedulingConfigOutput() DriverSchedulingConfigOutput {
	return i.ToDriverSchedulingConfigOutputWithContext(context.Background())
}

func (i DriverSchedulingConfigArgs) ToDriverSchedulingConfigOutputWithContext(ctx context.Context) DriverSchedulingConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DriverSchedulingConfigOutput)
}

func (i DriverSchedulingConfigArgs) ToDriverSchedulingConfigPtrOutput() DriverSchedulingConfigPtrOutput {
	return i.ToDriverSchedulingConfigPtrOutputWithContext(context.Background())
}

func (i DriverSchedulingConfigArgs) ToDriverSchedulingConfigPtrOutputWithContext(ctx context.Context) DriverSchedulingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DriverSchedulingConfigOutput).ToDriverSchedulingConfigPtrOutputWithContext(ctx)
}

// DriverSchedulingConfigPtrInput is an input type that accepts DriverSchedulingConfigArgs, DriverSchedulingConfigPtr and DriverSchedulingConfigPtrOutput values.
// You can construct a concrete instance of `DriverSchedulingConfigPtrInput` via:
//
//	        DriverSchedulingConfigArgs{...}
//
//	or:
//
//	        nil
type DriverSchedulingConfigPtrInput interface {
	pulumi.Input

	ToDriverSchedulingConfigPtrOutput() DriverSchedulingConfigPtrOutput
	ToDriverSchedulingConfigPtrOutputWithContext(context.Context) DriverSchedulingConfigPtrOutput
}

type driverSchedulingConfigPtrType DriverSchedulingConfigArgs

func DriverSchedulingConfigPtr(v *DriverSchedulingConfigArgs) DriverSchedulingConfigPtrInput {
	return (*driverSchedulingConfigPtrType)(v)
}

func (*driverSchedulingConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**DriverSchedulingConfig)(nil)).Elem()
}

func (i *driverSchedulingConfigPtrType) ToDriverSchedulingConfigPtrOutput() DriverSchedulingConfigPtrOutput {
	return i.ToDriverSchedulingConfigPtrOutputWithContext(context.Background())
}

func (i *driverSchedulingConfigPtrType) ToDriverSchedulingConfigPtrOutputWithContext(ctx context.Context) DriverSchedulingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DriverSchedulingConfigPtrOutput)
}

// Driver scheduling configuration.
type DriverSchedulingConfigOutput struct{ *pulumi.OutputState }

func (DriverSchedulingConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*DriverSchedulingConfig)(nil)).Elem()
}

func (o DriverSchedulingConfigOutput) ToDriverSchedulingConfigOutput() DriverSchedulingConfigOutput {
	return o
}

func (o DriverSchedulingConfigOutput) ToDriverSchedulingConfigOutputWithContext(ctx context.Context) DriverSchedulingConfigOutput {
	return o
}

func (o DriverSchedulingConfigOutput) ToDriverSchedulingConfigPtrOutput() DriverSchedulingConfigPtrOutput {
	return o.ToDriverSchedulingConfigPtrOutputWithContext(context.Background())
}

func (o DriverSchedulingConfigOutput) ToDriverSchedulingConfigPtrOutputWithContext(ctx context.Context) DriverSchedulingConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v DriverSchedulingConfig) *DriverSchedulingConfig {
		return &v
	}).(DriverSchedulingConfigPtrOutput)
}

// The amount of memory in MB the driver is requesting.
func (o DriverSchedulingConfigOutput) MemoryMb() pulumi.IntOutput {
	return o.ApplyT(func(v DriverSchedulingConfig) int { return v.MemoryMb }).(pulumi.IntOutput)
}

// The number of vCPUs the driver is requesting.
func (o DriverSchedulingConfigOutput) Vcores() pulumi.IntOutput {
	return o.ApplyT(func(v DriverSchedulingConfig) int { return v.Vcores }).(pulumi.IntOutput)
}

type DriverSchedulingConfigPtrOutput struct{ *pulumi.OutputState }

func (DriverSchedulingConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**DriverSchedulingConfig)(nil)).Elem()
}

func (o DriverSchedulingConfigPtrOutput) ToDriverSchedulingConfigPtrOutput() DriverSchedulingConfigPtrOutput {
	return o
}

func (o DriverSchedulingConfigPtrOutput) ToDriverSchedulingConfigPtrOutputWithContext(ctx context.Context) DriverSchedulingConfigPtrOutput {
	return o
}

func (o DriverSchedulingConfigPtrOutput) Elem() DriverSchedulingConfigOutput {
	return o.ApplyT(func(v *DriverSchedulingConfig) DriverSchedulingConfig {
		if v != nil {
			return *v
		}
		var ret DriverSchedulingConfig
		return ret
	}).(DriverSchedulingConfigOutput)
}

// The amount of memory in MB the driver is requesting.
func (o DriverSchedulingConfigPtrOutput) MemoryMb() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *DriverSchedulingConfig) *int {
		if v == nil {
			return nil
		}
		return &v.MemoryMb
	}).(pulumi.IntPtrOutput)
}

// The number of vCPUs the driver is requesting.
func (o DriverSchedulingConfigPtrOutput) Vcores() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *DriverSchedulingConfig) *int {
		if v == nil {
			return nil
		}
		return &v.Vcores
	}).(pulumi.IntPtrOutput)
}

// Driver scheduling configuration.
type DriverSchedulingConfigResponse struct {
	// The amount of memory in MB the driver is requesting.
	MemoryMb int `pulumi:"memoryMb"`
	// The number of vCPUs the driver is requesting.
	Vcores int `pulumi:"vcores"`
}

// Driver scheduling configuration.
type DriverSchedulingConfigResponseOutput struct{ *pulumi.OutputState }

func (DriverSchedulingConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*DriverSchedulingConfigResponse)(nil)).Elem()
}

func (o DriverSchedulingConfigResponseOutput) ToDriverSchedulingConfigResponseOutput() DriverSchedulingConfigResponseOutput {
	return o
}

func (o DriverSchedulingConfigResponseOutput) ToDriverSchedulingConfigResponseOutputWithContext(ctx context.Context) DriverSchedulingConfigResponseOutput {
	return o
}

// The amount of memory in MB the driver is requesting.
func (o DriverSchedulingConfigResponseOutput) MemoryMb() pulumi.IntOutput {
	return o.ApplyT(func(v DriverSchedulingConfigResponse) int { return v.MemoryMb }).(pulumi.IntOutput)
}

// The number of vCPUs the driver is requesting.
func (o DriverSchedulingConfigResponseOutput) Vcores() pulumi.IntOutput {
	return o.ApplyT(func(v DriverSchedulingConfigResponse) int { return v.Vcores }).(pulumi.IntOutput)
}

// Encryption settings for the cluster.
type EncryptionConfig struct {
	// Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
	GcePdKmsKeyName *string `pulumi:"gcePdKmsKeyName"`
	// Optional. The Cloud KMS key name to use for encrypting customer core content and cluster PD disk for all instances in the cluster.
	KmsKey *string `pulumi:"kmsKey"`
}

// EncryptionConfigInput is an input type that accepts EncryptionConfigArgs and EncryptionConfigOutput values.
// You can construct a concrete instance of `EncryptionConfigInput` via:
//
//	EncryptionConfigArgs{...}
type EncryptionConfigInput interface {
	pulumi.Input

	ToEncryptionConfigOutput() EncryptionConfigOutput
	ToEncryptionConfigOutputWithContext(context.Context) EncryptionConfigOutput
}

// Encryption settings for the cluster.
type EncryptionConfigArgs struct {
	// Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
	GcePdKmsKeyName pulumi.StringPtrInput `pulumi:"gcePdKmsKeyName"`
	// Optional. The Cloud KMS key name to use for encrypting customer core content and cluster PD disk for all instances in the cluster.
	KmsKey pulumi.StringPtrInput `pulumi:"kmsKey"`
}

func (EncryptionConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*EncryptionConfig)(nil)).Elem()
}

func (i EncryptionConfigArgs) ToEncryptionConfigOutput() EncryptionConfigOutput {
	return i.ToEncryptionConfigOutputWithContext(context.Background())
}

func (i EncryptionConfigArgs) ToEncryptionConfigOutputWithContext(ctx context.Context) EncryptionConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EncryptionConfigOutput)
}

func (i EncryptionConfigArgs) ToEncryptionConfigPtrOutput() EncryptionConfigPtrOutput {
	return i.ToEncryptionConfigPtrOutputWithContext(context.Background())
}

func (i EncryptionConfigArgs) ToEncryptionConfigPtrOutputWithContext(ctx context.Context) EncryptionConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EncryptionConfigOutput).ToEncryptionConfigPtrOutputWithContext(ctx)
}

// EncryptionConfigPtrInput is an input type that accepts EncryptionConfigArgs, EncryptionConfigPtr and EncryptionConfigPtrOutput values.
// You can construct a concrete instance of `EncryptionConfigPtrInput` via:
//
//	        EncryptionConfigArgs{...}
//
//	or:
//
//	        nil
type EncryptionConfigPtrInput interface {
	pulumi.Input

	ToEncryptionConfigPtrOutput() EncryptionConfigPtrOutput
	ToEncryptionConfigPtrOutputWithContext(context.Context) EncryptionConfigPtrOutput
}

type encryptionConfigPtrType EncryptionConfigArgs

func EncryptionConfigPtr(v *EncryptionConfigArgs) EncryptionConfigPtrInput {
	return (*encryptionConfigPtrType)(v)
}

func (*encryptionConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**EncryptionConfig)(nil)).Elem()
}

func (i *encryptionConfigPtrType) ToEncryptionConfigPtrOutput() EncryptionConfigPtrOutput {
	return i.ToEncryptionConfigPtrOutputWithContext(context.Background())
}

func (i *encryptionConfigPtrType) ToEncryptionConfigPtrOutputWithContext(ctx context.Context) EncryptionConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EncryptionConfigPtrOutput)
}

// Encryption settings for the cluster.
type EncryptionConfigOutput struct{ *pulumi.OutputState }

func (EncryptionConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*EncryptionConfig)(nil)).Elem()
}

func (o EncryptionConfigOutput) ToEncryptionConfigOutput() EncryptionConfigOutput {
	return o
}

func (o EncryptionConfigOutput) ToEncryptionConfigOutputWithContext(ctx context.Context) EncryptionConfigOutput {
	return o
}

func (o EncryptionConfigOutput) ToEncryptionConfigPtrOutput() EncryptionConfigPtrOutput {
	return o.ToEncryptionConfigPtrOutputWithContext(context.Background())
}

func (o EncryptionConfigOutput) ToEncryptionConfigPtrOutputWithContext(ctx context.Context) EncryptionConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v EncryptionConfig) *EncryptionConfig {
		return &v
	}).(EncryptionConfigPtrOutput)
}

// Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
func (o EncryptionConfigOutput) GcePdKmsKeyName() pulumi.StringPtrOutput {
	return o.ApplyT(func(v EncryptionConfig) *string { return v.GcePdKmsKeyName }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud KMS key name to use for encrypting customer core content and cluster PD disk for all instances in the cluster.
func (o EncryptionConfigOutput) KmsKey() pulumi.StringPtrOutput {
	return o.ApplyT(func(v EncryptionConfig) *string { return v.KmsKey }).(pulumi.StringPtrOutput)
}

type EncryptionConfigPtrOutput struct{ *pulumi.OutputState }

func (EncryptionConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**EncryptionConfig)(nil)).Elem()
}

func (o EncryptionConfigPtrOutput) ToEncryptionConfigPtrOutput() EncryptionConfigPtrOutput {
	return o
}

func (o EncryptionConfigPtrOutput) ToEncryptionConfigPtrOutputWithContext(ctx context.Context) EncryptionConfigPtrOutput {
	return o
}

func (o EncryptionConfigPtrOutput) Elem() EncryptionConfigOutput {
	return o.ApplyT(func(v *EncryptionConfig) EncryptionConfig {
		if v != nil {
			return *v
		}
		var ret EncryptionConfig
		return ret
	}).(EncryptionConfigOutput)
}

// Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
func (o EncryptionConfigPtrOutput) GcePdKmsKeyName() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *EncryptionConfig) *string {
		if v == nil {
			return nil
		}
		return v.GcePdKmsKeyName
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud KMS key name to use for encrypting customer core content and cluster PD disk for all instances in the cluster.
func (o EncryptionConfigPtrOutput) KmsKey() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *EncryptionConfig) *string {
		if v == nil {
			return nil
		}
		return v.KmsKey
	}).(pulumi.StringPtrOutput)
}

// Encryption settings for the cluster.
type EncryptionConfigResponse struct {
	// Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
	GcePdKmsKeyName string `pulumi:"gcePdKmsKeyName"`
	// Optional. The Cloud KMS key name to use for encrypting customer core content and cluster PD disk for all instances in the cluster.
	KmsKey string `pulumi:"kmsKey"`
}

// Encryption settings for the cluster.
type EncryptionConfigResponseOutput struct{ *pulumi.OutputState }

func (EncryptionConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*EncryptionConfigResponse)(nil)).Elem()
}

func (o EncryptionConfigResponseOutput) ToEncryptionConfigResponseOutput() EncryptionConfigResponseOutput {
	return o
}

func (o EncryptionConfigResponseOutput) ToEncryptionConfigResponseOutputWithContext(ctx context.Context) EncryptionConfigResponseOutput {
	return o
}

// Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
func (o EncryptionConfigResponseOutput) GcePdKmsKeyName() pulumi.StringOutput {
	return o.ApplyT(func(v EncryptionConfigResponse) string { return v.GcePdKmsKeyName }).(pulumi.StringOutput)
}

// Optional. The Cloud KMS key name to use for encrypting customer core content and cluster PD disk for all instances in the cluster.
func (o EncryptionConfigResponseOutput) KmsKey() pulumi.StringOutput {
	return o.ApplyT(func(v EncryptionConfigResponse) string { return v.KmsKey }).(pulumi.StringOutput)
}

// Endpoint config for this cluster
type EndpointConfig struct {
	// Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
	EnableHttpPortAccess *bool `pulumi:"enableHttpPortAccess"`
}

// EndpointConfigInput is an input type that accepts EndpointConfigArgs and EndpointConfigOutput values.
// You can construct a concrete instance of `EndpointConfigInput` via:
//
//	EndpointConfigArgs{...}
type EndpointConfigInput interface {
	pulumi.Input

	ToEndpointConfigOutput() EndpointConfigOutput
	ToEndpointConfigOutputWithContext(context.Context) EndpointConfigOutput
}

// Endpoint config for this cluster
type EndpointConfigArgs struct {
	// Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
	EnableHttpPortAccess pulumi.BoolPtrInput `pulumi:"enableHttpPortAccess"`
}

func (EndpointConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*EndpointConfig)(nil)).Elem()
}

func (i EndpointConfigArgs) ToEndpointConfigOutput() EndpointConfigOutput {
	return i.ToEndpointConfigOutputWithContext(context.Background())
}

func (i EndpointConfigArgs) ToEndpointConfigOutputWithContext(ctx context.Context) EndpointConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EndpointConfigOutput)
}

func (i EndpointConfigArgs) ToEndpointConfigPtrOutput() EndpointConfigPtrOutput {
	return i.ToEndpointConfigPtrOutputWithContext(context.Background())
}

func (i EndpointConfigArgs) ToEndpointConfigPtrOutputWithContext(ctx context.Context) EndpointConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EndpointConfigOutput).ToEndpointConfigPtrOutputWithContext(ctx)
}

// EndpointConfigPtrInput is an input type that accepts EndpointConfigArgs, EndpointConfigPtr and EndpointConfigPtrOutput values.
// You can construct a concrete instance of `EndpointConfigPtrInput` via:
//
//	        EndpointConfigArgs{...}
//
//	or:
//
//	        nil
type EndpointConfigPtrInput interface {
	pulumi.Input

	ToEndpointConfigPtrOutput() EndpointConfigPtrOutput
	ToEndpointConfigPtrOutputWithContext(context.Context) EndpointConfigPtrOutput
}

type endpointConfigPtrType EndpointConfigArgs

func EndpointConfigPtr(v *EndpointConfigArgs) EndpointConfigPtrInput {
	return (*endpointConfigPtrType)(v)
}

func (*endpointConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**EndpointConfig)(nil)).Elem()
}

func (i *endpointConfigPtrType) ToEndpointConfigPtrOutput() EndpointConfigPtrOutput {
	return i.ToEndpointConfigPtrOutputWithContext(context.Background())
}

func (i *endpointConfigPtrType) ToEndpointConfigPtrOutputWithContext(ctx context.Context) EndpointConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EndpointConfigPtrOutput)
}

// Endpoint config for this cluster
type EndpointConfigOutput struct{ *pulumi.OutputState }

func (EndpointConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*EndpointConfig)(nil)).Elem()
}

func (o EndpointConfigOutput) ToEndpointConfigOutput() EndpointConfigOutput {
	return o
}

func (o EndpointConfigOutput) ToEndpointConfigOutputWithContext(ctx context.Context) EndpointConfigOutput {
	return o
}

func (o EndpointConfigOutput) ToEndpointConfigPtrOutput() EndpointConfigPtrOutput {
	return o.ToEndpointConfigPtrOutputWithContext(context.Background())
}

func (o EndpointConfigOutput) ToEndpointConfigPtrOutputWithContext(ctx context.Context) EndpointConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v EndpointConfig) *EndpointConfig {
		return &v
	}).(EndpointConfigPtrOutput)
}

// Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
func (o EndpointConfigOutput) EnableHttpPortAccess() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v EndpointConfig) *bool { return v.EnableHttpPortAccess }).(pulumi.BoolPtrOutput)
}

type EndpointConfigPtrOutput struct{ *pulumi.OutputState }

func (EndpointConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**EndpointConfig)(nil)).Elem()
}

func (o EndpointConfigPtrOutput) ToEndpointConfigPtrOutput() EndpointConfigPtrOutput {
	return o
}

func (o EndpointConfigPtrOutput) ToEndpointConfigPtrOutputWithContext(ctx context.Context) EndpointConfigPtrOutput {
	return o
}

func (o EndpointConfigPtrOutput) Elem() EndpointConfigOutput {
	return o.ApplyT(func(v *EndpointConfig) EndpointConfig {
		if v != nil {
			return *v
		}
		var ret EndpointConfig
		return ret
	}).(EndpointConfigOutput)
}

// Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
func (o EndpointConfigPtrOutput) EnableHttpPortAccess() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *EndpointConfig) *bool {
		if v == nil {
			return nil
		}
		return v.EnableHttpPortAccess
	}).(pulumi.BoolPtrOutput)
}

// Endpoint config for this cluster
type EndpointConfigResponse struct {
	// Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
	EnableHttpPortAccess bool `pulumi:"enableHttpPortAccess"`
	// The map of port descriptions to URLs. Will only be populated if enable_http_port_access is true.
	HttpPorts map[string]string `pulumi:"httpPorts"`
}

// Endpoint config for this cluster
type EndpointConfigResponseOutput struct{ *pulumi.OutputState }

func (EndpointConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*EndpointConfigResponse)(nil)).Elem()
}

func (o EndpointConfigResponseOutput) ToEndpointConfigResponseOutput() EndpointConfigResponseOutput {
	return o
}

func (o EndpointConfigResponseOutput) ToEndpointConfigResponseOutputWithContext(ctx context.Context) EndpointConfigResponseOutput {
	return o
}

// Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
func (o EndpointConfigResponseOutput) EnableHttpPortAccess() pulumi.BoolOutput {
	return o.ApplyT(func(v EndpointConfigResponse) bool { return v.EnableHttpPortAccess }).(pulumi.BoolOutput)
}

// The map of port descriptions to URLs. Will only be populated if enable_http_port_access is true.
func (o EndpointConfigResponseOutput) HttpPorts() pulumi.StringMapOutput {
	return o.ApplyT(func(v EndpointConfigResponse) map[string]string { return v.HttpPorts }).(pulumi.StringMapOutput)
}

// Environment configuration for a workload.
type EnvironmentConfig struct {
	// Optional. Execution configuration for a workload.
	ExecutionConfig *ExecutionConfig `pulumi:"executionConfig"`
	// Optional. Peripherals configuration that workload has access to.
	PeripheralsConfig *PeripheralsConfig `pulumi:"peripheralsConfig"`
}

// EnvironmentConfigInput is an input type that accepts EnvironmentConfigArgs and EnvironmentConfigOutput values.
// You can construct a concrete instance of `EnvironmentConfigInput` via:
//
//	EnvironmentConfigArgs{...}
type EnvironmentConfigInput interface {
	pulumi.Input

	ToEnvironmentConfigOutput() EnvironmentConfigOutput
	ToEnvironmentConfigOutputWithContext(context.Context) EnvironmentConfigOutput
}

// Environment configuration for a workload.
type EnvironmentConfigArgs struct {
	// Optional. Execution configuration for a workload.
	ExecutionConfig ExecutionConfigPtrInput `pulumi:"executionConfig"`
	// Optional. Peripherals configuration that workload has access to.
	PeripheralsConfig PeripheralsConfigPtrInput `pulumi:"peripheralsConfig"`
}

func (EnvironmentConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*EnvironmentConfig)(nil)).Elem()
}

func (i EnvironmentConfigArgs) ToEnvironmentConfigOutput() EnvironmentConfigOutput {
	return i.ToEnvironmentConfigOutputWithContext(context.Background())
}

func (i EnvironmentConfigArgs) ToEnvironmentConfigOutputWithContext(ctx context.Context) EnvironmentConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EnvironmentConfigOutput)
}

func (i EnvironmentConfigArgs) ToEnvironmentConfigPtrOutput() EnvironmentConfigPtrOutput {
	return i.ToEnvironmentConfigPtrOutputWithContext(context.Background())
}

func (i EnvironmentConfigArgs) ToEnvironmentConfigPtrOutputWithContext(ctx context.Context) EnvironmentConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EnvironmentConfigOutput).ToEnvironmentConfigPtrOutputWithContext(ctx)
}

// EnvironmentConfigPtrInput is an input type that accepts EnvironmentConfigArgs, EnvironmentConfigPtr and EnvironmentConfigPtrOutput values.
// You can construct a concrete instance of `EnvironmentConfigPtrInput` via:
//
//	        EnvironmentConfigArgs{...}
//
//	or:
//
//	        nil
type EnvironmentConfigPtrInput interface {
	pulumi.Input

	ToEnvironmentConfigPtrOutput() EnvironmentConfigPtrOutput
	ToEnvironmentConfigPtrOutputWithContext(context.Context) EnvironmentConfigPtrOutput
}

type environmentConfigPtrType EnvironmentConfigArgs

func EnvironmentConfigPtr(v *EnvironmentConfigArgs) EnvironmentConfigPtrInput {
	return (*environmentConfigPtrType)(v)
}

func (*environmentConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**EnvironmentConfig)(nil)).Elem()
}

func (i *environmentConfigPtrType) ToEnvironmentConfigPtrOutput() EnvironmentConfigPtrOutput {
	return i.ToEnvironmentConfigPtrOutputWithContext(context.Background())
}

func (i *environmentConfigPtrType) ToEnvironmentConfigPtrOutputWithContext(ctx context.Context) EnvironmentConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EnvironmentConfigPtrOutput)
}

// Environment configuration for a workload.
type EnvironmentConfigOutput struct{ *pulumi.OutputState }

func (EnvironmentConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*EnvironmentConfig)(nil)).Elem()
}

func (o EnvironmentConfigOutput) ToEnvironmentConfigOutput() EnvironmentConfigOutput {
	return o
}

func (o EnvironmentConfigOutput) ToEnvironmentConfigOutputWithContext(ctx context.Context) EnvironmentConfigOutput {
	return o
}

func (o EnvironmentConfigOutput) ToEnvironmentConfigPtrOutput() EnvironmentConfigPtrOutput {
	return o.ToEnvironmentConfigPtrOutputWithContext(context.Background())
}

func (o EnvironmentConfigOutput) ToEnvironmentConfigPtrOutputWithContext(ctx context.Context) EnvironmentConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v EnvironmentConfig) *EnvironmentConfig {
		return &v
	}).(EnvironmentConfigPtrOutput)
}

// Optional. Execution configuration for a workload.
func (o EnvironmentConfigOutput) ExecutionConfig() ExecutionConfigPtrOutput {
	return o.ApplyT(func(v EnvironmentConfig) *ExecutionConfig { return v.ExecutionConfig }).(ExecutionConfigPtrOutput)
}

// Optional. Peripherals configuration that workload has access to.
func (o EnvironmentConfigOutput) PeripheralsConfig() PeripheralsConfigPtrOutput {
	return o.ApplyT(func(v EnvironmentConfig) *PeripheralsConfig { return v.PeripheralsConfig }).(PeripheralsConfigPtrOutput)
}

type EnvironmentConfigPtrOutput struct{ *pulumi.OutputState }

func (EnvironmentConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**EnvironmentConfig)(nil)).Elem()
}

func (o EnvironmentConfigPtrOutput) ToEnvironmentConfigPtrOutput() EnvironmentConfigPtrOutput {
	return o
}

func (o EnvironmentConfigPtrOutput) ToEnvironmentConfigPtrOutputWithContext(ctx context.Context) EnvironmentConfigPtrOutput {
	return o
}

func (o EnvironmentConfigPtrOutput) Elem() EnvironmentConfigOutput {
	return o.ApplyT(func(v *EnvironmentConfig) EnvironmentConfig {
		if v != nil {
			return *v
		}
		var ret EnvironmentConfig
		return ret
	}).(EnvironmentConfigOutput)
}

// Optional. Execution configuration for a workload.
func (o EnvironmentConfigPtrOutput) ExecutionConfig() ExecutionConfigPtrOutput {
	return o.ApplyT(func(v *EnvironmentConfig) *ExecutionConfig {
		if v == nil {
			return nil
		}
		return v.ExecutionConfig
	}).(ExecutionConfigPtrOutput)
}

// Optional. Peripherals configuration that workload has access to.
func (o EnvironmentConfigPtrOutput) PeripheralsConfig() PeripheralsConfigPtrOutput {
	return o.ApplyT(func(v *EnvironmentConfig) *PeripheralsConfig {
		if v == nil {
			return nil
		}
		return v.PeripheralsConfig
	}).(PeripheralsConfigPtrOutput)
}

// Environment configuration for a workload.
type EnvironmentConfigResponse struct {
	// Optional. Execution configuration for a workload.
	ExecutionConfig ExecutionConfigResponse `pulumi:"executionConfig"`
	// Optional. Peripherals configuration that workload has access to.
	PeripheralsConfig PeripheralsConfigResponse `pulumi:"peripheralsConfig"`
}

// Environment configuration for a workload.
type EnvironmentConfigResponseOutput struct{ *pulumi.OutputState }

func (EnvironmentConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*EnvironmentConfigResponse)(nil)).Elem()
}

func (o EnvironmentConfigResponseOutput) ToEnvironmentConfigResponseOutput() EnvironmentConfigResponseOutput {
	return o
}

func (o EnvironmentConfigResponseOutput) ToEnvironmentConfigResponseOutputWithContext(ctx context.Context) EnvironmentConfigResponseOutput {
	return o
}

// Optional. Execution configuration for a workload.
func (o EnvironmentConfigResponseOutput) ExecutionConfig() ExecutionConfigResponseOutput {
	return o.ApplyT(func(v EnvironmentConfigResponse) ExecutionConfigResponse { return v.ExecutionConfig }).(ExecutionConfigResponseOutput)
}

// Optional. Peripherals configuration that workload has access to.
func (o EnvironmentConfigResponseOutput) PeripheralsConfig() PeripheralsConfigResponseOutput {
	return o.ApplyT(func(v EnvironmentConfigResponse) PeripheralsConfigResponse { return v.PeripheralsConfig }).(PeripheralsConfigResponseOutput)
}

// Execution configuration for a workload.
type ExecutionConfig struct {
	// Optional. The duration to keep the session alive while it's idling. Exceeding this threshold causes the session to terminate. This field cannot be set on a batch workload. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). Defaults to 4 hours if not set. If both ttl and idle_ttl are specified for an interactive session, the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.
	IdleTtl *string `pulumi:"idleTtl"`
	// Optional. The Cloud KMS key to use for encryption.
	KmsKey *string `pulumi:"kmsKey"`
	// Optional. Tags used for network traffic control.
	NetworkTags []string `pulumi:"networkTags"`
	// Optional. Network URI to connect workload to.
	NetworkUri *string `pulumi:"networkUri"`
	// Optional. Service account that used to execute workload.
	ServiceAccount *string `pulumi:"serviceAccount"`
	// Optional. A Cloud Storage bucket used to stage workload dependencies, config files, and store workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running, and then create and manage project-level, per-location staging and temporary buckets. This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	StagingBucket *string `pulumi:"stagingBucket"`
	// Optional. Subnetwork URI to connect workload to.
	SubnetworkUri *string `pulumi:"subnetworkUri"`
	// Optional. The duration after which the workload will be terminated. When the workload exceeds this duration, it will be unconditionally terminated without waiting for ongoing work to finish. If ttl is not specified for a batch workload, the workload will be allowed to run until it exits naturally (or runs forever without exiting). If ttl is not specified for an interactive session, it defaults to 24h. If ttl is not specified for a batch that uses 2.1+ runtime version, it defaults to 4h. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). If both ttl and idle_ttl are specified (for an interactive session), the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.
	Ttl *string `pulumi:"ttl"`
}

// ExecutionConfigInput is an input type that accepts ExecutionConfigArgs and ExecutionConfigOutput values.
// You can construct a concrete instance of `ExecutionConfigInput` via:
//
//	ExecutionConfigArgs{...}
type ExecutionConfigInput interface {
	pulumi.Input

	ToExecutionConfigOutput() ExecutionConfigOutput
	ToExecutionConfigOutputWithContext(context.Context) ExecutionConfigOutput
}

// Execution configuration for a workload.
type ExecutionConfigArgs struct {
	// Optional. The duration to keep the session alive while it's idling. Exceeding this threshold causes the session to terminate. This field cannot be set on a batch workload. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). Defaults to 4 hours if not set. If both ttl and idle_ttl are specified for an interactive session, the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.
	IdleTtl pulumi.StringPtrInput `pulumi:"idleTtl"`
	// Optional. The Cloud KMS key to use for encryption.
	KmsKey pulumi.StringPtrInput `pulumi:"kmsKey"`
	// Optional. Tags used for network traffic control.
	NetworkTags pulumi.StringArrayInput `pulumi:"networkTags"`
	// Optional. Network URI to connect workload to.
	NetworkUri pulumi.StringPtrInput `pulumi:"networkUri"`
	// Optional. Service account that used to execute workload.
	ServiceAccount pulumi.StringPtrInput `pulumi:"serviceAccount"`
	// Optional. A Cloud Storage bucket used to stage workload dependencies, config files, and store workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running, and then create and manage project-level, per-location staging and temporary buckets. This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	StagingBucket pulumi.StringPtrInput `pulumi:"stagingBucket"`
	// Optional. Subnetwork URI to connect workload to.
	SubnetworkUri pulumi.StringPtrInput `pulumi:"subnetworkUri"`
	// Optional. The duration after which the workload will be terminated. When the workload exceeds this duration, it will be unconditionally terminated without waiting for ongoing work to finish. If ttl is not specified for a batch workload, the workload will be allowed to run until it exits naturally (or runs forever without exiting). If ttl is not specified for an interactive session, it defaults to 24h. If ttl is not specified for a batch that uses 2.1+ runtime version, it defaults to 4h. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). If both ttl and idle_ttl are specified (for an interactive session), the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.
	Ttl pulumi.StringPtrInput `pulumi:"ttl"`
}

func (ExecutionConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ExecutionConfig)(nil)).Elem()
}

func (i ExecutionConfigArgs) ToExecutionConfigOutput() ExecutionConfigOutput {
	return i.ToExecutionConfigOutputWithContext(context.Background())
}

func (i ExecutionConfigArgs) ToExecutionConfigOutputWithContext(ctx context.Context) ExecutionConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ExecutionConfigOutput)
}

func (i ExecutionConfigArgs) ToExecutionConfigPtrOutput() ExecutionConfigPtrOutput {
	return i.ToExecutionConfigPtrOutputWithContext(context.Background())
}

func (i ExecutionConfigArgs) ToExecutionConfigPtrOutputWithContext(ctx context.Context) ExecutionConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ExecutionConfigOutput).ToExecutionConfigPtrOutputWithContext(ctx)
}

// ExecutionConfigPtrInput is an input type that accepts ExecutionConfigArgs, ExecutionConfigPtr and ExecutionConfigPtrOutput values.
// You can construct a concrete instance of `ExecutionConfigPtrInput` via:
//
//	        ExecutionConfigArgs{...}
//
//	or:
//
//	        nil
type ExecutionConfigPtrInput interface {
	pulumi.Input

	ToExecutionConfigPtrOutput() ExecutionConfigPtrOutput
	ToExecutionConfigPtrOutputWithContext(context.Context) ExecutionConfigPtrOutput
}

type executionConfigPtrType ExecutionConfigArgs

func ExecutionConfigPtr(v *ExecutionConfigArgs) ExecutionConfigPtrInput {
	return (*executionConfigPtrType)(v)
}

func (*executionConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ExecutionConfig)(nil)).Elem()
}

func (i *executionConfigPtrType) ToExecutionConfigPtrOutput() ExecutionConfigPtrOutput {
	return i.ToExecutionConfigPtrOutputWithContext(context.Background())
}

func (i *executionConfigPtrType) ToExecutionConfigPtrOutputWithContext(ctx context.Context) ExecutionConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ExecutionConfigPtrOutput)
}

// Execution configuration for a workload.
type ExecutionConfigOutput struct{ *pulumi.OutputState }

func (ExecutionConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ExecutionConfig)(nil)).Elem()
}

func (o ExecutionConfigOutput) ToExecutionConfigOutput() ExecutionConfigOutput {
	return o
}

func (o ExecutionConfigOutput) ToExecutionConfigOutputWithContext(ctx context.Context) ExecutionConfigOutput {
	return o
}

func (o ExecutionConfigOutput) ToExecutionConfigPtrOutput() ExecutionConfigPtrOutput {
	return o.ToExecutionConfigPtrOutputWithContext(context.Background())
}

func (o ExecutionConfigOutput) ToExecutionConfigPtrOutputWithContext(ctx context.Context) ExecutionConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ExecutionConfig) *ExecutionConfig {
		return &v
	}).(ExecutionConfigPtrOutput)
}

// Optional. The duration to keep the session alive while it's idling. Exceeding this threshold causes the session to terminate. This field cannot be set on a batch workload. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). Defaults to 4 hours if not set. If both ttl and idle_ttl are specified for an interactive session, the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.
func (o ExecutionConfigOutput) IdleTtl() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ExecutionConfig) *string { return v.IdleTtl }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud KMS key to use for encryption.
func (o ExecutionConfigOutput) KmsKey() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ExecutionConfig) *string { return v.KmsKey }).(pulumi.StringPtrOutput)
}

// Optional. Tags used for network traffic control.
func (o ExecutionConfigOutput) NetworkTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v ExecutionConfig) []string { return v.NetworkTags }).(pulumi.StringArrayOutput)
}

// Optional. Network URI to connect workload to.
func (o ExecutionConfigOutput) NetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ExecutionConfig) *string { return v.NetworkUri }).(pulumi.StringPtrOutput)
}

// Optional. Service account that used to execute workload.
func (o ExecutionConfigOutput) ServiceAccount() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ExecutionConfig) *string { return v.ServiceAccount }).(pulumi.StringPtrOutput)
}

// Optional. A Cloud Storage bucket used to stage workload dependencies, config files, and store workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running, and then create and manage project-level, per-location staging and temporary buckets. This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ExecutionConfigOutput) StagingBucket() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ExecutionConfig) *string { return v.StagingBucket }).(pulumi.StringPtrOutput)
}

// Optional. Subnetwork URI to connect workload to.
func (o ExecutionConfigOutput) SubnetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ExecutionConfig) *string { return v.SubnetworkUri }).(pulumi.StringPtrOutput)
}

// Optional. The duration after which the workload will be terminated. When the workload exceeds this duration, it will be unconditionally terminated without waiting for ongoing work to finish. If ttl is not specified for a batch workload, the workload will be allowed to run until it exits naturally (or runs forever without exiting). If ttl is not specified for an interactive session, it defaults to 24h. If ttl is not specified for a batch that uses 2.1+ runtime version, it defaults to 4h. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). If both ttl and idle_ttl are specified (for an interactive session), the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.
func (o ExecutionConfigOutput) Ttl() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ExecutionConfig) *string { return v.Ttl }).(pulumi.StringPtrOutput)
}

type ExecutionConfigPtrOutput struct{ *pulumi.OutputState }

func (ExecutionConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ExecutionConfig)(nil)).Elem()
}

func (o ExecutionConfigPtrOutput) ToExecutionConfigPtrOutput() ExecutionConfigPtrOutput {
	return o
}

func (o ExecutionConfigPtrOutput) ToExecutionConfigPtrOutputWithContext(ctx context.Context) ExecutionConfigPtrOutput {
	return o
}

func (o ExecutionConfigPtrOutput) Elem() ExecutionConfigOutput {
	return o.ApplyT(func(v *ExecutionConfig) ExecutionConfig {
		if v != nil {
			return *v
		}
		var ret ExecutionConfig
		return ret
	}).(ExecutionConfigOutput)
}

// Optional. The duration to keep the session alive while it's idling. Exceeding this threshold causes the session to terminate. This field cannot be set on a batch workload. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). Defaults to 4 hours if not set. If both ttl and idle_ttl are specified for an interactive session, the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.
func (o ExecutionConfigPtrOutput) IdleTtl() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ExecutionConfig) *string {
		if v == nil {
			return nil
		}
		return v.IdleTtl
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud KMS key to use for encryption.
func (o ExecutionConfigPtrOutput) KmsKey() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ExecutionConfig) *string {
		if v == nil {
			return nil
		}
		return v.KmsKey
	}).(pulumi.StringPtrOutput)
}

// Optional. Tags used for network traffic control.
func (o ExecutionConfigPtrOutput) NetworkTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *ExecutionConfig) []string {
		if v == nil {
			return nil
		}
		return v.NetworkTags
	}).(pulumi.StringArrayOutput)
}

// Optional. Network URI to connect workload to.
func (o ExecutionConfigPtrOutput) NetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ExecutionConfig) *string {
		if v == nil {
			return nil
		}
		return v.NetworkUri
	}).(pulumi.StringPtrOutput)
}

// Optional. Service account that used to execute workload.
func (o ExecutionConfigPtrOutput) ServiceAccount() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ExecutionConfig) *string {
		if v == nil {
			return nil
		}
		return v.ServiceAccount
	}).(pulumi.StringPtrOutput)
}

// Optional. A Cloud Storage bucket used to stage workload dependencies, config files, and store workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running, and then create and manage project-level, per-location staging and temporary buckets. This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ExecutionConfigPtrOutput) StagingBucket() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ExecutionConfig) *string {
		if v == nil {
			return nil
		}
		return v.StagingBucket
	}).(pulumi.StringPtrOutput)
}

// Optional. Subnetwork URI to connect workload to.
func (o ExecutionConfigPtrOutput) SubnetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ExecutionConfig) *string {
		if v == nil {
			return nil
		}
		return v.SubnetworkUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The duration after which the workload will be terminated. When the workload exceeds this duration, it will be unconditionally terminated without waiting for ongoing work to finish. If ttl is not specified for a batch workload, the workload will be allowed to run until it exits naturally (or runs forever without exiting). If ttl is not specified for an interactive session, it defaults to 24h. If ttl is not specified for a batch that uses 2.1+ runtime version, it defaults to 4h. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). If both ttl and idle_ttl are specified (for an interactive session), the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.
func (o ExecutionConfigPtrOutput) Ttl() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ExecutionConfig) *string {
		if v == nil {
			return nil
		}
		return v.Ttl
	}).(pulumi.StringPtrOutput)
}

// Execution configuration for a workload.
type ExecutionConfigResponse struct {
	// Optional. The duration to keep the session alive while it's idling. Exceeding this threshold causes the session to terminate. This field cannot be set on a batch workload. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). Defaults to 4 hours if not set. If both ttl and idle_ttl are specified for an interactive session, the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.
	IdleTtl string `pulumi:"idleTtl"`
	// Optional. The Cloud KMS key to use for encryption.
	KmsKey string `pulumi:"kmsKey"`
	// Optional. Tags used for network traffic control.
	NetworkTags []string `pulumi:"networkTags"`
	// Optional. Network URI to connect workload to.
	NetworkUri string `pulumi:"networkUri"`
	// Optional. Service account that used to execute workload.
	ServiceAccount string `pulumi:"serviceAccount"`
	// Optional. A Cloud Storage bucket used to stage workload dependencies, config files, and store workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running, and then create and manage project-level, per-location staging and temporary buckets. This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	StagingBucket string `pulumi:"stagingBucket"`
	// Optional. Subnetwork URI to connect workload to.
	SubnetworkUri string `pulumi:"subnetworkUri"`
	// Optional. The duration after which the workload will be terminated. When the workload exceeds this duration, it will be unconditionally terminated without waiting for ongoing work to finish. If ttl is not specified for a batch workload, the workload will be allowed to run until it exits naturally (or runs forever without exiting). If ttl is not specified for an interactive session, it defaults to 24h. If ttl is not specified for a batch that uses 2.1+ runtime version, it defaults to 4h. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). If both ttl and idle_ttl are specified (for an interactive session), the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.
	Ttl string `pulumi:"ttl"`
}

// Execution configuration for a workload.
type ExecutionConfigResponseOutput struct{ *pulumi.OutputState }

func (ExecutionConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ExecutionConfigResponse)(nil)).Elem()
}

func (o ExecutionConfigResponseOutput) ToExecutionConfigResponseOutput() ExecutionConfigResponseOutput {
	return o
}

func (o ExecutionConfigResponseOutput) ToExecutionConfigResponseOutputWithContext(ctx context.Context) ExecutionConfigResponseOutput {
	return o
}

// Optional. The duration to keep the session alive while it's idling. Exceeding this threshold causes the session to terminate. This field cannot be set on a batch workload. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). Defaults to 4 hours if not set. If both ttl and idle_ttl are specified for an interactive session, the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.
func (o ExecutionConfigResponseOutput) IdleTtl() pulumi.StringOutput {
	return o.ApplyT(func(v ExecutionConfigResponse) string { return v.IdleTtl }).(pulumi.StringOutput)
}

// Optional. The Cloud KMS key to use for encryption.
func (o ExecutionConfigResponseOutput) KmsKey() pulumi.StringOutput {
	return o.ApplyT(func(v ExecutionConfigResponse) string { return v.KmsKey }).(pulumi.StringOutput)
}

// Optional. Tags used for network traffic control.
func (o ExecutionConfigResponseOutput) NetworkTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v ExecutionConfigResponse) []string { return v.NetworkTags }).(pulumi.StringArrayOutput)
}

// Optional. Network URI to connect workload to.
func (o ExecutionConfigResponseOutput) NetworkUri() pulumi.StringOutput {
	return o.ApplyT(func(v ExecutionConfigResponse) string { return v.NetworkUri }).(pulumi.StringOutput)
}

// Optional. Service account that used to execute workload.
func (o ExecutionConfigResponseOutput) ServiceAccount() pulumi.StringOutput {
	return o.ApplyT(func(v ExecutionConfigResponse) string { return v.ServiceAccount }).(pulumi.StringOutput)
}

// Optional. A Cloud Storage bucket used to stage workload dependencies, config files, and store workload output and other ephemeral data, such as Spark history files. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location according to the region where your workload is running, and then create and manage project-level, per-location staging and temporary buckets. This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ExecutionConfigResponseOutput) StagingBucket() pulumi.StringOutput {
	return o.ApplyT(func(v ExecutionConfigResponse) string { return v.StagingBucket }).(pulumi.StringOutput)
}

// Optional. Subnetwork URI to connect workload to.
func (o ExecutionConfigResponseOutput) SubnetworkUri() pulumi.StringOutput {
	return o.ApplyT(func(v ExecutionConfigResponse) string { return v.SubnetworkUri }).(pulumi.StringOutput)
}

// Optional. The duration after which the workload will be terminated. When the workload exceeds this duration, it will be unconditionally terminated without waiting for ongoing work to finish. If ttl is not specified for a batch workload, the workload will be allowed to run until it exits naturally (or runs forever without exiting). If ttl is not specified for an interactive session, it defaults to 24h. If ttl is not specified for a batch that uses 2.1+ runtime version, it defaults to 4h. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)). If both ttl and idle_ttl are specified (for an interactive session), the conditions are treated as OR conditions: the workload will be terminated when it has been idle for idle_ttl or when ttl has been exceeded, whichever occurs first.
func (o ExecutionConfigResponseOutput) Ttl() pulumi.StringOutput {
	return o.ApplyT(func(v ExecutionConfigResponse) string { return v.Ttl }).(pulumi.StringOutput)
}

// Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec.Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
type Expr struct {
	// Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
	Description *string `pulumi:"description"`
	// Textual representation of an expression in Common Expression Language syntax.
	Expression *string `pulumi:"expression"`
	// Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
	Location *string `pulumi:"location"`
	// Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
	Title *string `pulumi:"title"`
}

// ExprInput is an input type that accepts ExprArgs and ExprOutput values.
// You can construct a concrete instance of `ExprInput` via:
//
//	ExprArgs{...}
type ExprInput interface {
	pulumi.Input

	ToExprOutput() ExprOutput
	ToExprOutputWithContext(context.Context) ExprOutput
}

// Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec.Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
type ExprArgs struct {
	// Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
	Description pulumi.StringPtrInput `pulumi:"description"`
	// Textual representation of an expression in Common Expression Language syntax.
	Expression pulumi.StringPtrInput `pulumi:"expression"`
	// Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
	Location pulumi.StringPtrInput `pulumi:"location"`
	// Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
	Title pulumi.StringPtrInput `pulumi:"title"`
}

func (ExprArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*Expr)(nil)).Elem()
}

func (i ExprArgs) ToExprOutput() ExprOutput {
	return i.ToExprOutputWithContext(context.Background())
}

func (i ExprArgs) ToExprOutputWithContext(ctx context.Context) ExprOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ExprOutput)
}

func (i ExprArgs) ToExprPtrOutput() ExprPtrOutput {
	return i.ToExprPtrOutputWithContext(context.Background())
}

func (i ExprArgs) ToExprPtrOutputWithContext(ctx context.Context) ExprPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ExprOutput).ToExprPtrOutputWithContext(ctx)
}

// ExprPtrInput is an input type that accepts ExprArgs, ExprPtr and ExprPtrOutput values.
// You can construct a concrete instance of `ExprPtrInput` via:
//
//	        ExprArgs{...}
//
//	or:
//
//	        nil
type ExprPtrInput interface {
	pulumi.Input

	ToExprPtrOutput() ExprPtrOutput
	ToExprPtrOutputWithContext(context.Context) ExprPtrOutput
}

type exprPtrType ExprArgs

func ExprPtr(v *ExprArgs) ExprPtrInput {
	return (*exprPtrType)(v)
}

func (*exprPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**Expr)(nil)).Elem()
}

func (i *exprPtrType) ToExprPtrOutput() ExprPtrOutput {
	return i.ToExprPtrOutputWithContext(context.Background())
}

func (i *exprPtrType) ToExprPtrOutputWithContext(ctx context.Context) ExprPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ExprPtrOutput)
}

// Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec.Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
type ExprOutput struct{ *pulumi.OutputState }

func (ExprOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*Expr)(nil)).Elem()
}

func (o ExprOutput) ToExprOutput() ExprOutput {
	return o
}

func (o ExprOutput) ToExprOutputWithContext(ctx context.Context) ExprOutput {
	return o
}

func (o ExprOutput) ToExprPtrOutput() ExprPtrOutput {
	return o.ToExprPtrOutputWithContext(context.Background())
}

func (o ExprOutput) ToExprPtrOutputWithContext(ctx context.Context) ExprPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v Expr) *Expr {
		return &v
	}).(ExprPtrOutput)
}

// Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
func (o ExprOutput) Description() pulumi.StringPtrOutput {
	return o.ApplyT(func(v Expr) *string { return v.Description }).(pulumi.StringPtrOutput)
}

// Textual representation of an expression in Common Expression Language syntax.
func (o ExprOutput) Expression() pulumi.StringPtrOutput {
	return o.ApplyT(func(v Expr) *string { return v.Expression }).(pulumi.StringPtrOutput)
}

// Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
func (o ExprOutput) Location() pulumi.StringPtrOutput {
	return o.ApplyT(func(v Expr) *string { return v.Location }).(pulumi.StringPtrOutput)
}

// Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
func (o ExprOutput) Title() pulumi.StringPtrOutput {
	return o.ApplyT(func(v Expr) *string { return v.Title }).(pulumi.StringPtrOutput)
}

type ExprPtrOutput struct{ *pulumi.OutputState }

func (ExprPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**Expr)(nil)).Elem()
}

func (o ExprPtrOutput) ToExprPtrOutput() ExprPtrOutput {
	return o
}

func (o ExprPtrOutput) ToExprPtrOutputWithContext(ctx context.Context) ExprPtrOutput {
	return o
}

func (o ExprPtrOutput) Elem() ExprOutput {
	return o.ApplyT(func(v *Expr) Expr {
		if v != nil {
			return *v
		}
		var ret Expr
		return ret
	}).(ExprOutput)
}

// Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
func (o ExprPtrOutput) Description() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Expr) *string {
		if v == nil {
			return nil
		}
		return v.Description
	}).(pulumi.StringPtrOutput)
}

// Textual representation of an expression in Common Expression Language syntax.
func (o ExprPtrOutput) Expression() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Expr) *string {
		if v == nil {
			return nil
		}
		return v.Expression
	}).(pulumi.StringPtrOutput)
}

// Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
func (o ExprPtrOutput) Location() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Expr) *string {
		if v == nil {
			return nil
		}
		return v.Location
	}).(pulumi.StringPtrOutput)
}

// Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
func (o ExprPtrOutput) Title() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Expr) *string {
		if v == nil {
			return nil
		}
		return v.Title
	}).(pulumi.StringPtrOutput)
}

// Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec.Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
type ExprResponse struct {
	// Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
	Description string `pulumi:"description"`
	// Textual representation of an expression in Common Expression Language syntax.
	Expression string `pulumi:"expression"`
	// Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
	Location string `pulumi:"location"`
	// Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
	Title string `pulumi:"title"`
}

// Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec.Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
type ExprResponseOutput struct{ *pulumi.OutputState }

func (ExprResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ExprResponse)(nil)).Elem()
}

func (o ExprResponseOutput) ToExprResponseOutput() ExprResponseOutput {
	return o
}

func (o ExprResponseOutput) ToExprResponseOutputWithContext(ctx context.Context) ExprResponseOutput {
	return o
}

// Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
func (o ExprResponseOutput) Description() pulumi.StringOutput {
	return o.ApplyT(func(v ExprResponse) string { return v.Description }).(pulumi.StringOutput)
}

// Textual representation of an expression in Common Expression Language syntax.
func (o ExprResponseOutput) Expression() pulumi.StringOutput {
	return o.ApplyT(func(v ExprResponse) string { return v.Expression }).(pulumi.StringOutput)
}

// Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
func (o ExprResponseOutput) Location() pulumi.StringOutput {
	return o.ApplyT(func(v ExprResponse) string { return v.Location }).(pulumi.StringOutput)
}

// Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
func (o ExprResponseOutput) Title() pulumi.StringOutput {
	return o.ApplyT(func(v ExprResponse) string { return v.Title }).(pulumi.StringOutput)
}

// Common config settings for resources of Compute Engine cluster instances, applicable to all instances in the cluster.
type GceClusterConfig struct {
	// Optional. Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs).
	ConfidentialInstanceConfig *ConfidentialInstanceConfig `pulumi:"confidentialInstanceConfig"`
	// Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
	InternalIpOnly *bool `pulumi:"internalIpOnly"`
	// The Compute Engine metadata entries to add to all instances (see Project and instance metadata (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
	Metadata map[string]string `pulumi:"metadata"`
	// Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see Using Subnetworks (https://cloud.google.com/compute/docs/subnetworks) for more information).A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/global/networks/default projects/[project_id]/global/networks/default default
	NetworkUri *string `pulumi:"networkUri"`
	// Optional. Node Group Affinity for sole-tenant clusters.
	NodeGroupAffinity *NodeGroupAffinity `pulumi:"nodeGroupAffinity"`
	// Optional. The type of IPv6 access for a cluster.
	PrivateIpv6GoogleAccess *GceClusterConfigPrivateIpv6GoogleAccess `pulumi:"privateIpv6GoogleAccess"`
	// Optional. Reservation Affinity for consuming Zonal reservation.
	ReservationAffinity *ReservationAffinity `pulumi:"reservationAffinity"`
	// Optional. The Dataproc service account (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) (also see VM Data Plane identity (https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity)) used by Dataproc cluster VM instances to access Google Cloud Platform services.If not specified, the Compute Engine default service account (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
	ServiceAccount *string `pulumi:"serviceAccount"`
	// Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: https://www.googleapis.com/auth/cloud.useraccounts.readonly https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/logging.writeIf no scopes are specified, the following defaults are also provided: https://www.googleapis.com/auth/bigquery https://www.googleapis.com/auth/bigtable.admin.table https://www.googleapis.com/auth/bigtable.data https://www.googleapis.com/auth/devstorage.full_control
	ServiceAccountScopes []string `pulumi:"serviceAccountScopes"`
	// Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
	ShieldedInstanceConfig *ShieldedInstanceConfig `pulumi:"shieldedInstanceConfig"`
	// Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/[region]/subnetworks/sub0 projects/[project_id]/regions/[region]/subnetworks/sub0 sub0
	SubnetworkUri *string `pulumi:"subnetworkUri"`
	// The Compute Engine tags to add to all instances (see Tagging instances (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
	Tags []string `pulumi:"tags"`
	// Optional. The Compute Engine zone where the Dataproc cluster will be located. If omitted, the service will pick a zone in the cluster's Compute Engine region. On a get request, zone will always be present.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone] projects/[project_id]/zones/[zone] [zone]
	ZoneUri *string `pulumi:"zoneUri"`
}

// GceClusterConfigInput is an input type that accepts GceClusterConfigArgs and GceClusterConfigOutput values.
// You can construct a concrete instance of `GceClusterConfigInput` via:
//
//	GceClusterConfigArgs{...}
type GceClusterConfigInput interface {
	pulumi.Input

	ToGceClusterConfigOutput() GceClusterConfigOutput
	ToGceClusterConfigOutputWithContext(context.Context) GceClusterConfigOutput
}

// Common config settings for resources of Compute Engine cluster instances, applicable to all instances in the cluster.
type GceClusterConfigArgs struct {
	// Optional. Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs).
	ConfidentialInstanceConfig ConfidentialInstanceConfigPtrInput `pulumi:"confidentialInstanceConfig"`
	// Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
	InternalIpOnly pulumi.BoolPtrInput `pulumi:"internalIpOnly"`
	// The Compute Engine metadata entries to add to all instances (see Project and instance metadata (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
	Metadata pulumi.StringMapInput `pulumi:"metadata"`
	// Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see Using Subnetworks (https://cloud.google.com/compute/docs/subnetworks) for more information).A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/global/networks/default projects/[project_id]/global/networks/default default
	NetworkUri pulumi.StringPtrInput `pulumi:"networkUri"`
	// Optional. Node Group Affinity for sole-tenant clusters.
	NodeGroupAffinity NodeGroupAffinityPtrInput `pulumi:"nodeGroupAffinity"`
	// Optional. The type of IPv6 access for a cluster.
	PrivateIpv6GoogleAccess GceClusterConfigPrivateIpv6GoogleAccessPtrInput `pulumi:"privateIpv6GoogleAccess"`
	// Optional. Reservation Affinity for consuming Zonal reservation.
	ReservationAffinity ReservationAffinityPtrInput `pulumi:"reservationAffinity"`
	// Optional. The Dataproc service account (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) (also see VM Data Plane identity (https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity)) used by Dataproc cluster VM instances to access Google Cloud Platform services.If not specified, the Compute Engine default service account (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
	ServiceAccount pulumi.StringPtrInput `pulumi:"serviceAccount"`
	// Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: https://www.googleapis.com/auth/cloud.useraccounts.readonly https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/logging.writeIf no scopes are specified, the following defaults are also provided: https://www.googleapis.com/auth/bigquery https://www.googleapis.com/auth/bigtable.admin.table https://www.googleapis.com/auth/bigtable.data https://www.googleapis.com/auth/devstorage.full_control
	ServiceAccountScopes pulumi.StringArrayInput `pulumi:"serviceAccountScopes"`
	// Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
	ShieldedInstanceConfig ShieldedInstanceConfigPtrInput `pulumi:"shieldedInstanceConfig"`
	// Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/[region]/subnetworks/sub0 projects/[project_id]/regions/[region]/subnetworks/sub0 sub0
	SubnetworkUri pulumi.StringPtrInput `pulumi:"subnetworkUri"`
	// The Compute Engine tags to add to all instances (see Tagging instances (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
	Tags pulumi.StringArrayInput `pulumi:"tags"`
	// Optional. The Compute Engine zone where the Dataproc cluster will be located. If omitted, the service will pick a zone in the cluster's Compute Engine region. On a get request, zone will always be present.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone] projects/[project_id]/zones/[zone] [zone]
	ZoneUri pulumi.StringPtrInput `pulumi:"zoneUri"`
}

func (GceClusterConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*GceClusterConfig)(nil)).Elem()
}

func (i GceClusterConfigArgs) ToGceClusterConfigOutput() GceClusterConfigOutput {
	return i.ToGceClusterConfigOutputWithContext(context.Background())
}

func (i GceClusterConfigArgs) ToGceClusterConfigOutputWithContext(ctx context.Context) GceClusterConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GceClusterConfigOutput)
}

func (i GceClusterConfigArgs) ToGceClusterConfigPtrOutput() GceClusterConfigPtrOutput {
	return i.ToGceClusterConfigPtrOutputWithContext(context.Background())
}

func (i GceClusterConfigArgs) ToGceClusterConfigPtrOutputWithContext(ctx context.Context) GceClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GceClusterConfigOutput).ToGceClusterConfigPtrOutputWithContext(ctx)
}

// GceClusterConfigPtrInput is an input type that accepts GceClusterConfigArgs, GceClusterConfigPtr and GceClusterConfigPtrOutput values.
// You can construct a concrete instance of `GceClusterConfigPtrInput` via:
//
//	        GceClusterConfigArgs{...}
//
//	or:
//
//	        nil
type GceClusterConfigPtrInput interface {
	pulumi.Input

	ToGceClusterConfigPtrOutput() GceClusterConfigPtrOutput
	ToGceClusterConfigPtrOutputWithContext(context.Context) GceClusterConfigPtrOutput
}

type gceClusterConfigPtrType GceClusterConfigArgs

func GceClusterConfigPtr(v *GceClusterConfigArgs) GceClusterConfigPtrInput {
	return (*gceClusterConfigPtrType)(v)
}

func (*gceClusterConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**GceClusterConfig)(nil)).Elem()
}

func (i *gceClusterConfigPtrType) ToGceClusterConfigPtrOutput() GceClusterConfigPtrOutput {
	return i.ToGceClusterConfigPtrOutputWithContext(context.Background())
}

func (i *gceClusterConfigPtrType) ToGceClusterConfigPtrOutputWithContext(ctx context.Context) GceClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GceClusterConfigPtrOutput)
}

// Common config settings for resources of Compute Engine cluster instances, applicable to all instances in the cluster.
type GceClusterConfigOutput struct{ *pulumi.OutputState }

func (GceClusterConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GceClusterConfig)(nil)).Elem()
}

func (o GceClusterConfigOutput) ToGceClusterConfigOutput() GceClusterConfigOutput {
	return o
}

func (o GceClusterConfigOutput) ToGceClusterConfigOutputWithContext(ctx context.Context) GceClusterConfigOutput {
	return o
}

func (o GceClusterConfigOutput) ToGceClusterConfigPtrOutput() GceClusterConfigPtrOutput {
	return o.ToGceClusterConfigPtrOutputWithContext(context.Background())
}

func (o GceClusterConfigOutput) ToGceClusterConfigPtrOutputWithContext(ctx context.Context) GceClusterConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v GceClusterConfig) *GceClusterConfig {
		return &v
	}).(GceClusterConfigPtrOutput)
}

// Optional. Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs).
func (o GceClusterConfigOutput) ConfidentialInstanceConfig() ConfidentialInstanceConfigPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *ConfidentialInstanceConfig { return v.ConfidentialInstanceConfig }).(ConfidentialInstanceConfigPtrOutput)
}

// Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
func (o GceClusterConfigOutput) InternalIpOnly() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *bool { return v.InternalIpOnly }).(pulumi.BoolPtrOutput)
}

// The Compute Engine metadata entries to add to all instances (see Project and instance metadata (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
func (o GceClusterConfigOutput) Metadata() pulumi.StringMapOutput {
	return o.ApplyT(func(v GceClusterConfig) map[string]string { return v.Metadata }).(pulumi.StringMapOutput)
}

// Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see Using Subnetworks (https://cloud.google.com/compute/docs/subnetworks) for more information).A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/global/networks/default projects/[project_id]/global/networks/default default
func (o GceClusterConfigOutput) NetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *string { return v.NetworkUri }).(pulumi.StringPtrOutput)
}

// Optional. Node Group Affinity for sole-tenant clusters.
func (o GceClusterConfigOutput) NodeGroupAffinity() NodeGroupAffinityPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *NodeGroupAffinity { return v.NodeGroupAffinity }).(NodeGroupAffinityPtrOutput)
}

// Optional. The type of IPv6 access for a cluster.
func (o GceClusterConfigOutput) PrivateIpv6GoogleAccess() GceClusterConfigPrivateIpv6GoogleAccessPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *GceClusterConfigPrivateIpv6GoogleAccess { return v.PrivateIpv6GoogleAccess }).(GceClusterConfigPrivateIpv6GoogleAccessPtrOutput)
}

// Optional. Reservation Affinity for consuming Zonal reservation.
func (o GceClusterConfigOutput) ReservationAffinity() ReservationAffinityPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *ReservationAffinity { return v.ReservationAffinity }).(ReservationAffinityPtrOutput)
}

// Optional. The Dataproc service account (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) (also see VM Data Plane identity (https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity)) used by Dataproc cluster VM instances to access Google Cloud Platform services.If not specified, the Compute Engine default service account (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
func (o GceClusterConfigOutput) ServiceAccount() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *string { return v.ServiceAccount }).(pulumi.StringPtrOutput)
}

// Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: https://www.googleapis.com/auth/cloud.useraccounts.readonly https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/logging.writeIf no scopes are specified, the following defaults are also provided: https://www.googleapis.com/auth/bigquery https://www.googleapis.com/auth/bigtable.admin.table https://www.googleapis.com/auth/bigtable.data https://www.googleapis.com/auth/devstorage.full_control
func (o GceClusterConfigOutput) ServiceAccountScopes() pulumi.StringArrayOutput {
	return o.ApplyT(func(v GceClusterConfig) []string { return v.ServiceAccountScopes }).(pulumi.StringArrayOutput)
}

// Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
func (o GceClusterConfigOutput) ShieldedInstanceConfig() ShieldedInstanceConfigPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *ShieldedInstanceConfig { return v.ShieldedInstanceConfig }).(ShieldedInstanceConfigPtrOutput)
}

// Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/[region]/subnetworks/sub0 projects/[project_id]/regions/[region]/subnetworks/sub0 sub0
func (o GceClusterConfigOutput) SubnetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *string { return v.SubnetworkUri }).(pulumi.StringPtrOutput)
}

// The Compute Engine tags to add to all instances (see Tagging instances (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
func (o GceClusterConfigOutput) Tags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v GceClusterConfig) []string { return v.Tags }).(pulumi.StringArrayOutput)
}

// Optional. The Compute Engine zone where the Dataproc cluster will be located. If omitted, the service will pick a zone in the cluster's Compute Engine region. On a get request, zone will always be present.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone] projects/[project_id]/zones/[zone] [zone]
func (o GceClusterConfigOutput) ZoneUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *string { return v.ZoneUri }).(pulumi.StringPtrOutput)
}

type GceClusterConfigPtrOutput struct{ *pulumi.OutputState }

func (GceClusterConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**GceClusterConfig)(nil)).Elem()
}

func (o GceClusterConfigPtrOutput) ToGceClusterConfigPtrOutput() GceClusterConfigPtrOutput {
	return o
}

func (o GceClusterConfigPtrOutput) ToGceClusterConfigPtrOutputWithContext(ctx context.Context) GceClusterConfigPtrOutput {
	return o
}

func (o GceClusterConfigPtrOutput) Elem() GceClusterConfigOutput {
	return o.ApplyT(func(v *GceClusterConfig) GceClusterConfig {
		if v != nil {
			return *v
		}
		var ret GceClusterConfig
		return ret
	}).(GceClusterConfigOutput)
}

// Optional. Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs).
func (o GceClusterConfigPtrOutput) ConfidentialInstanceConfig() ConfidentialInstanceConfigPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *ConfidentialInstanceConfig {
		if v == nil {
			return nil
		}
		return v.ConfidentialInstanceConfig
	}).(ConfidentialInstanceConfigPtrOutput)
}

// Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
func (o GceClusterConfigPtrOutput) InternalIpOnly() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *bool {
		if v == nil {
			return nil
		}
		return v.InternalIpOnly
	}).(pulumi.BoolPtrOutput)
}

// The Compute Engine metadata entries to add to all instances (see Project and instance metadata (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
func (o GceClusterConfigPtrOutput) Metadata() pulumi.StringMapOutput {
	return o.ApplyT(func(v *GceClusterConfig) map[string]string {
		if v == nil {
			return nil
		}
		return v.Metadata
	}).(pulumi.StringMapOutput)
}

// Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see Using Subnetworks (https://cloud.google.com/compute/docs/subnetworks) for more information).A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/global/networks/default projects/[project_id]/global/networks/default default
func (o GceClusterConfigPtrOutput) NetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.NetworkUri
	}).(pulumi.StringPtrOutput)
}

// Optional. Node Group Affinity for sole-tenant clusters.
func (o GceClusterConfigPtrOutput) NodeGroupAffinity() NodeGroupAffinityPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *NodeGroupAffinity {
		if v == nil {
			return nil
		}
		return v.NodeGroupAffinity
	}).(NodeGroupAffinityPtrOutput)
}

// Optional. The type of IPv6 access for a cluster.
func (o GceClusterConfigPtrOutput) PrivateIpv6GoogleAccess() GceClusterConfigPrivateIpv6GoogleAccessPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *GceClusterConfigPrivateIpv6GoogleAccess {
		if v == nil {
			return nil
		}
		return v.PrivateIpv6GoogleAccess
	}).(GceClusterConfigPrivateIpv6GoogleAccessPtrOutput)
}

// Optional. Reservation Affinity for consuming Zonal reservation.
func (o GceClusterConfigPtrOutput) ReservationAffinity() ReservationAffinityPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *ReservationAffinity {
		if v == nil {
			return nil
		}
		return v.ReservationAffinity
	}).(ReservationAffinityPtrOutput)
}

// Optional. The Dataproc service account (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) (also see VM Data Plane identity (https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity)) used by Dataproc cluster VM instances to access Google Cloud Platform services.If not specified, the Compute Engine default service account (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
func (o GceClusterConfigPtrOutput) ServiceAccount() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.ServiceAccount
	}).(pulumi.StringPtrOutput)
}

// Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: https://www.googleapis.com/auth/cloud.useraccounts.readonly https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/logging.writeIf no scopes are specified, the following defaults are also provided: https://www.googleapis.com/auth/bigquery https://www.googleapis.com/auth/bigtable.admin.table https://www.googleapis.com/auth/bigtable.data https://www.googleapis.com/auth/devstorage.full_control
func (o GceClusterConfigPtrOutput) ServiceAccountScopes() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *GceClusterConfig) []string {
		if v == nil {
			return nil
		}
		return v.ServiceAccountScopes
	}).(pulumi.StringArrayOutput)
}

// Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
func (o GceClusterConfigPtrOutput) ShieldedInstanceConfig() ShieldedInstanceConfigPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *ShieldedInstanceConfig {
		if v == nil {
			return nil
		}
		return v.ShieldedInstanceConfig
	}).(ShieldedInstanceConfigPtrOutput)
}

// Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/[region]/subnetworks/sub0 projects/[project_id]/regions/[region]/subnetworks/sub0 sub0
func (o GceClusterConfigPtrOutput) SubnetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.SubnetworkUri
	}).(pulumi.StringPtrOutput)
}

// The Compute Engine tags to add to all instances (see Tagging instances (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
func (o GceClusterConfigPtrOutput) Tags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *GceClusterConfig) []string {
		if v == nil {
			return nil
		}
		return v.Tags
	}).(pulumi.StringArrayOutput)
}

// Optional. The Compute Engine zone where the Dataproc cluster will be located. If omitted, the service will pick a zone in the cluster's Compute Engine region. On a get request, zone will always be present.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone] projects/[project_id]/zones/[zone] [zone]
func (o GceClusterConfigPtrOutput) ZoneUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.ZoneUri
	}).(pulumi.StringPtrOutput)
}

// Common config settings for resources of Compute Engine cluster instances, applicable to all instances in the cluster.
type GceClusterConfigResponse struct {
	// Optional. Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs).
	ConfidentialInstanceConfig ConfidentialInstanceConfigResponse `pulumi:"confidentialInstanceConfig"`
	// Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
	InternalIpOnly bool `pulumi:"internalIpOnly"`
	// The Compute Engine metadata entries to add to all instances (see Project and instance metadata (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
	Metadata map[string]string `pulumi:"metadata"`
	// Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see Using Subnetworks (https://cloud.google.com/compute/docs/subnetworks) for more information).A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/global/networks/default projects/[project_id]/global/networks/default default
	NetworkUri string `pulumi:"networkUri"`
	// Optional. Node Group Affinity for sole-tenant clusters.
	NodeGroupAffinity NodeGroupAffinityResponse `pulumi:"nodeGroupAffinity"`
	// Optional. The type of IPv6 access for a cluster.
	PrivateIpv6GoogleAccess string `pulumi:"privateIpv6GoogleAccess"`
	// Optional. Reservation Affinity for consuming Zonal reservation.
	ReservationAffinity ReservationAffinityResponse `pulumi:"reservationAffinity"`
	// Optional. The Dataproc service account (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) (also see VM Data Plane identity (https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity)) used by Dataproc cluster VM instances to access Google Cloud Platform services.If not specified, the Compute Engine default service account (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
	ServiceAccount string `pulumi:"serviceAccount"`
	// Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: https://www.googleapis.com/auth/cloud.useraccounts.readonly https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/logging.writeIf no scopes are specified, the following defaults are also provided: https://www.googleapis.com/auth/bigquery https://www.googleapis.com/auth/bigtable.admin.table https://www.googleapis.com/auth/bigtable.data https://www.googleapis.com/auth/devstorage.full_control
	ServiceAccountScopes []string `pulumi:"serviceAccountScopes"`
	// Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
	ShieldedInstanceConfig ShieldedInstanceConfigResponse `pulumi:"shieldedInstanceConfig"`
	// Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/[region]/subnetworks/sub0 projects/[project_id]/regions/[region]/subnetworks/sub0 sub0
	SubnetworkUri string `pulumi:"subnetworkUri"`
	// The Compute Engine tags to add to all instances (see Tagging instances (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
	Tags []string `pulumi:"tags"`
	// Optional. The Compute Engine zone where the Dataproc cluster will be located. If omitted, the service will pick a zone in the cluster's Compute Engine region. On a get request, zone will always be present.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone] projects/[project_id]/zones/[zone] [zone]
	ZoneUri string `pulumi:"zoneUri"`
}

// Common config settings for resources of Compute Engine cluster instances, applicable to all instances in the cluster.
type GceClusterConfigResponseOutput struct{ *pulumi.OutputState }

func (GceClusterConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GceClusterConfigResponse)(nil)).Elem()
}

func (o GceClusterConfigResponseOutput) ToGceClusterConfigResponseOutput() GceClusterConfigResponseOutput {
	return o
}

func (o GceClusterConfigResponseOutput) ToGceClusterConfigResponseOutputWithContext(ctx context.Context) GceClusterConfigResponseOutput {
	return o
}

// Optional. Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs).
func (o GceClusterConfigResponseOutput) ConfidentialInstanceConfig() ConfidentialInstanceConfigResponseOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) ConfidentialInstanceConfigResponse {
		return v.ConfidentialInstanceConfig
	}).(ConfidentialInstanceConfigResponseOutput)
}

// Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
func (o GceClusterConfigResponseOutput) InternalIpOnly() pulumi.BoolOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) bool { return v.InternalIpOnly }).(pulumi.BoolOutput)
}

// The Compute Engine metadata entries to add to all instances (see Project and instance metadata (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
func (o GceClusterConfigResponseOutput) Metadata() pulumi.StringMapOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) map[string]string { return v.Metadata }).(pulumi.StringMapOutput)
}

// Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see Using Subnetworks (https://cloud.google.com/compute/docs/subnetworks) for more information).A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/global/networks/default projects/[project_id]/global/networks/default default
func (o GceClusterConfigResponseOutput) NetworkUri() pulumi.StringOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) string { return v.NetworkUri }).(pulumi.StringOutput)
}

// Optional. Node Group Affinity for sole-tenant clusters.
func (o GceClusterConfigResponseOutput) NodeGroupAffinity() NodeGroupAffinityResponseOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) NodeGroupAffinityResponse { return v.NodeGroupAffinity }).(NodeGroupAffinityResponseOutput)
}

// Optional. The type of IPv6 access for a cluster.
func (o GceClusterConfigResponseOutput) PrivateIpv6GoogleAccess() pulumi.StringOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) string { return v.PrivateIpv6GoogleAccess }).(pulumi.StringOutput)
}

// Optional. Reservation Affinity for consuming Zonal reservation.
func (o GceClusterConfigResponseOutput) ReservationAffinity() ReservationAffinityResponseOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) ReservationAffinityResponse { return v.ReservationAffinity }).(ReservationAffinityResponseOutput)
}

// Optional. The Dataproc service account (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) (also see VM Data Plane identity (https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity)) used by Dataproc cluster VM instances to access Google Cloud Platform services.If not specified, the Compute Engine default service account (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
func (o GceClusterConfigResponseOutput) ServiceAccount() pulumi.StringOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) string { return v.ServiceAccount }).(pulumi.StringOutput)
}

// Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: https://www.googleapis.com/auth/cloud.useraccounts.readonly https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/logging.writeIf no scopes are specified, the following defaults are also provided: https://www.googleapis.com/auth/bigquery https://www.googleapis.com/auth/bigtable.admin.table https://www.googleapis.com/auth/bigtable.data https://www.googleapis.com/auth/devstorage.full_control
func (o GceClusterConfigResponseOutput) ServiceAccountScopes() pulumi.StringArrayOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) []string { return v.ServiceAccountScopes }).(pulumi.StringArrayOutput)
}

// Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
func (o GceClusterConfigResponseOutput) ShieldedInstanceConfig() ShieldedInstanceConfigResponseOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) ShieldedInstanceConfigResponse { return v.ShieldedInstanceConfig }).(ShieldedInstanceConfigResponseOutput)
}

// Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/[region]/subnetworks/sub0 projects/[project_id]/regions/[region]/subnetworks/sub0 sub0
func (o GceClusterConfigResponseOutput) SubnetworkUri() pulumi.StringOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) string { return v.SubnetworkUri }).(pulumi.StringOutput)
}

// The Compute Engine tags to add to all instances (see Tagging instances (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
func (o GceClusterConfigResponseOutput) Tags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) []string { return v.Tags }).(pulumi.StringArrayOutput)
}

// Optional. The Compute Engine zone where the Dataproc cluster will be located. If omitted, the service will pick a zone in the cluster's Compute Engine region. On a get request, zone will always be present.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone] projects/[project_id]/zones/[zone] [zone]
func (o GceClusterConfigResponseOutput) ZoneUri() pulumi.StringOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) string { return v.ZoneUri }).(pulumi.StringOutput)
}

// The cluster's GKE config.
type GkeClusterConfig struct {
	// Optional. A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster (the GKE cluster can be zonal or regional). Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
	GkeClusterTarget *string `pulumi:"gkeClusterTarget"`
	// Optional. Deprecated. Use gkeClusterTarget. Used only for the deprecated beta. A target for the deployment.
	//
	// Deprecated: Optional. Deprecated. Use gkeClusterTarget. Used only for the deprecated beta. A target for the deployment.
	NamespacedGkeDeploymentTarget *NamespacedGkeDeploymentTarget `pulumi:"namespacedGkeDeploymentTarget"`
	// Optional. GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget. Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
	NodePoolTarget []GkeNodePoolTarget `pulumi:"nodePoolTarget"`
}

// GkeClusterConfigInput is an input type that accepts GkeClusterConfigArgs and GkeClusterConfigOutput values.
// You can construct a concrete instance of `GkeClusterConfigInput` via:
//
//	GkeClusterConfigArgs{...}
type GkeClusterConfigInput interface {
	pulumi.Input

	ToGkeClusterConfigOutput() GkeClusterConfigOutput
	ToGkeClusterConfigOutputWithContext(context.Context) GkeClusterConfigOutput
}

// The cluster's GKE config.
type GkeClusterConfigArgs struct {
	// Optional. A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster (the GKE cluster can be zonal or regional). Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
	GkeClusterTarget pulumi.StringPtrInput `pulumi:"gkeClusterTarget"`
	// Optional. Deprecated. Use gkeClusterTarget. Used only for the deprecated beta. A target for the deployment.
	//
	// Deprecated: Optional. Deprecated. Use gkeClusterTarget. Used only for the deprecated beta. A target for the deployment.
	NamespacedGkeDeploymentTarget NamespacedGkeDeploymentTargetPtrInput `pulumi:"namespacedGkeDeploymentTarget"`
	// Optional. GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget. Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
	NodePoolTarget GkeNodePoolTargetArrayInput `pulumi:"nodePoolTarget"`
}

func (GkeClusterConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeClusterConfig)(nil)).Elem()
}

func (i GkeClusterConfigArgs) ToGkeClusterConfigOutput() GkeClusterConfigOutput {
	return i.ToGkeClusterConfigOutputWithContext(context.Background())
}

func (i GkeClusterConfigArgs) ToGkeClusterConfigOutputWithContext(ctx context.Context) GkeClusterConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeClusterConfigOutput)
}

func (i GkeClusterConfigArgs) ToGkeClusterConfigPtrOutput() GkeClusterConfigPtrOutput {
	return i.ToGkeClusterConfigPtrOutputWithContext(context.Background())
}

func (i GkeClusterConfigArgs) ToGkeClusterConfigPtrOutputWithContext(ctx context.Context) GkeClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeClusterConfigOutput).ToGkeClusterConfigPtrOutputWithContext(ctx)
}

// GkeClusterConfigPtrInput is an input type that accepts GkeClusterConfigArgs, GkeClusterConfigPtr and GkeClusterConfigPtrOutput values.
// You can construct a concrete instance of `GkeClusterConfigPtrInput` via:
//
//	        GkeClusterConfigArgs{...}
//
//	or:
//
//	        nil
type GkeClusterConfigPtrInput interface {
	pulumi.Input

	ToGkeClusterConfigPtrOutput() GkeClusterConfigPtrOutput
	ToGkeClusterConfigPtrOutputWithContext(context.Context) GkeClusterConfigPtrOutput
}

type gkeClusterConfigPtrType GkeClusterConfigArgs

func GkeClusterConfigPtr(v *GkeClusterConfigArgs) GkeClusterConfigPtrInput {
	return (*gkeClusterConfigPtrType)(v)
}

func (*gkeClusterConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**GkeClusterConfig)(nil)).Elem()
}

func (i *gkeClusterConfigPtrType) ToGkeClusterConfigPtrOutput() GkeClusterConfigPtrOutput {
	return i.ToGkeClusterConfigPtrOutputWithContext(context.Background())
}

func (i *gkeClusterConfigPtrType) ToGkeClusterConfigPtrOutputWithContext(ctx context.Context) GkeClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeClusterConfigPtrOutput)
}

// The cluster's GKE config.
type GkeClusterConfigOutput struct{ *pulumi.OutputState }

func (GkeClusterConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeClusterConfig)(nil)).Elem()
}

func (o GkeClusterConfigOutput) ToGkeClusterConfigOutput() GkeClusterConfigOutput {
	return o
}

func (o GkeClusterConfigOutput) ToGkeClusterConfigOutputWithContext(ctx context.Context) GkeClusterConfigOutput {
	return o
}

func (o GkeClusterConfigOutput) ToGkeClusterConfigPtrOutput() GkeClusterConfigPtrOutput {
	return o.ToGkeClusterConfigPtrOutputWithContext(context.Background())
}

func (o GkeClusterConfigOutput) ToGkeClusterConfigPtrOutputWithContext(ctx context.Context) GkeClusterConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v GkeClusterConfig) *GkeClusterConfig {
		return &v
	}).(GkeClusterConfigPtrOutput)
}

// Optional. A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster (the GKE cluster can be zonal or regional). Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
func (o GkeClusterConfigOutput) GkeClusterTarget() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GkeClusterConfig) *string { return v.GkeClusterTarget }).(pulumi.StringPtrOutput)
}

// Optional. Deprecated. Use gkeClusterTarget. Used only for the deprecated beta. A target for the deployment.
//
// Deprecated: Optional. Deprecated. Use gkeClusterTarget. Used only for the deprecated beta. A target for the deployment.
func (o GkeClusterConfigOutput) NamespacedGkeDeploymentTarget() NamespacedGkeDeploymentTargetPtrOutput {
	return o.ApplyT(func(v GkeClusterConfig) *NamespacedGkeDeploymentTarget { return v.NamespacedGkeDeploymentTarget }).(NamespacedGkeDeploymentTargetPtrOutput)
}

// Optional. GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget. Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
func (o GkeClusterConfigOutput) NodePoolTarget() GkeNodePoolTargetArrayOutput {
	return o.ApplyT(func(v GkeClusterConfig) []GkeNodePoolTarget { return v.NodePoolTarget }).(GkeNodePoolTargetArrayOutput)
}

type GkeClusterConfigPtrOutput struct{ *pulumi.OutputState }

func (GkeClusterConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**GkeClusterConfig)(nil)).Elem()
}

func (o GkeClusterConfigPtrOutput) ToGkeClusterConfigPtrOutput() GkeClusterConfigPtrOutput {
	return o
}

func (o GkeClusterConfigPtrOutput) ToGkeClusterConfigPtrOutputWithContext(ctx context.Context) GkeClusterConfigPtrOutput {
	return o
}

func (o GkeClusterConfigPtrOutput) Elem() GkeClusterConfigOutput {
	return o.ApplyT(func(v *GkeClusterConfig) GkeClusterConfig {
		if v != nil {
			return *v
		}
		var ret GkeClusterConfig
		return ret
	}).(GkeClusterConfigOutput)
}

// Optional. A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster (the GKE cluster can be zonal or regional). Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
func (o GkeClusterConfigPtrOutput) GkeClusterTarget() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *GkeClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.GkeClusterTarget
	}).(pulumi.StringPtrOutput)
}

// Optional. Deprecated. Use gkeClusterTarget. Used only for the deprecated beta. A target for the deployment.
//
// Deprecated: Optional. Deprecated. Use gkeClusterTarget. Used only for the deprecated beta. A target for the deployment.
func (o GkeClusterConfigPtrOutput) NamespacedGkeDeploymentTarget() NamespacedGkeDeploymentTargetPtrOutput {
	return o.ApplyT(func(v *GkeClusterConfig) *NamespacedGkeDeploymentTarget {
		if v == nil {
			return nil
		}
		return v.NamespacedGkeDeploymentTarget
	}).(NamespacedGkeDeploymentTargetPtrOutput)
}

// Optional. GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget. Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
func (o GkeClusterConfigPtrOutput) NodePoolTarget() GkeNodePoolTargetArrayOutput {
	return o.ApplyT(func(v *GkeClusterConfig) []GkeNodePoolTarget {
		if v == nil {
			return nil
		}
		return v.NodePoolTarget
	}).(GkeNodePoolTargetArrayOutput)
}

// The cluster's GKE config.
type GkeClusterConfigResponse struct {
	// Optional. A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster (the GKE cluster can be zonal or regional). Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
	GkeClusterTarget string `pulumi:"gkeClusterTarget"`
	// Optional. Deprecated. Use gkeClusterTarget. Used only for the deprecated beta. A target for the deployment.
	//
	// Deprecated: Optional. Deprecated. Use gkeClusterTarget. Used only for the deprecated beta. A target for the deployment.
	NamespacedGkeDeploymentTarget NamespacedGkeDeploymentTargetResponse `pulumi:"namespacedGkeDeploymentTarget"`
	// Optional. GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget. Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
	NodePoolTarget []GkeNodePoolTargetResponse `pulumi:"nodePoolTarget"`
}

// The cluster's GKE config.
type GkeClusterConfigResponseOutput struct{ *pulumi.OutputState }

func (GkeClusterConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeClusterConfigResponse)(nil)).Elem()
}

func (o GkeClusterConfigResponseOutput) ToGkeClusterConfigResponseOutput() GkeClusterConfigResponseOutput {
	return o
}

func (o GkeClusterConfigResponseOutput) ToGkeClusterConfigResponseOutputWithContext(ctx context.Context) GkeClusterConfigResponseOutput {
	return o
}

// Optional. A target GKE cluster to deploy to. It must be in the same project and region as the Dataproc cluster (the GKE cluster can be zonal or regional). Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
func (o GkeClusterConfigResponseOutput) GkeClusterTarget() pulumi.StringOutput {
	return o.ApplyT(func(v GkeClusterConfigResponse) string { return v.GkeClusterTarget }).(pulumi.StringOutput)
}

// Optional. Deprecated. Use gkeClusterTarget. Used only for the deprecated beta. A target for the deployment.
//
// Deprecated: Optional. Deprecated. Use gkeClusterTarget. Used only for the deprecated beta. A target for the deployment.
func (o GkeClusterConfigResponseOutput) NamespacedGkeDeploymentTarget() NamespacedGkeDeploymentTargetResponseOutput {
	return o.ApplyT(func(v GkeClusterConfigResponse) NamespacedGkeDeploymentTargetResponse {
		return v.NamespacedGkeDeploymentTarget
	}).(NamespacedGkeDeploymentTargetResponseOutput)
}

// Optional. GKE node pools where workloads will be scheduled. At least one node pool must be assigned the DEFAULT GkeNodePoolTarget.Role. If a GkeNodePoolTarget is not specified, Dataproc constructs a DEFAULT GkeNodePoolTarget. Each role can be given to only one GkeNodePoolTarget. All node pools must have the same location settings.
func (o GkeClusterConfigResponseOutput) NodePoolTarget() GkeNodePoolTargetResponseArrayOutput {
	return o.ApplyT(func(v GkeClusterConfigResponse) []GkeNodePoolTargetResponse { return v.NodePoolTarget }).(GkeNodePoolTargetResponseArrayOutput)
}

// Parameters that describe cluster nodes.
type GkeNodeConfig struct {
	// Optional. A list of hardware accelerators (https://cloud.google.com/compute/docs/gpus) to attach to each node.
	Accelerators []GkeNodePoolAcceleratorConfig `pulumi:"accelerators"`
	// Optional. The Customer Managed Encryption Key (CMEK) (https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek) used to encrypt the boot disk attached to each node in the node pool. Specify the key using the following format: projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}
	BootDiskKmsKey *string `pulumi:"bootDiskKmsKey"`
	// Optional. The number of local SSD disks to attach to the node, which is limited by the maximum number of disks allowable per zone (see Adding Local SSDs (https://cloud.google.com/compute/docs/disks/local-ssd)).
	LocalSsdCount *int `pulumi:"localSsdCount"`
	// Optional. The name of a Compute Engine machine type (https://cloud.google.com/compute/docs/machine-types).
	MachineType *string `pulumi:"machineType"`
	// Optional. Minimum CPU platform (https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform) to be used by this instance. The instance may be scheduled on the specified or a newer CPU platform. Specify the friendly names of CPU platforms, such as "Intel Haswell"` or Intel Sandy Bridge".
	MinCpuPlatform *string `pulumi:"minCpuPlatform"`
	// Optional. Whether the nodes are created as legacy preemptible VM instances (https://cloud.google.com/compute/docs/instances/preemptible). Also see Spot VMs, preemptible VM instances without a maximum lifetime. Legacy and Spot preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
	Preemptible *bool `pulumi:"preemptible"`
	// Optional. Whether the nodes are created as Spot VM instances (https://cloud.google.com/compute/docs/instances/spot). Spot VMs are the latest update to legacy preemptible VMs. Spot VMs do not have a maximum lifetime. Legacy and Spot preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
	Spot *bool `pulumi:"spot"`
}

// GkeNodeConfigInput is an input type that accepts GkeNodeConfigArgs and GkeNodeConfigOutput values.
// You can construct a concrete instance of `GkeNodeConfigInput` via:
//
//	GkeNodeConfigArgs{...}
type GkeNodeConfigInput interface {
	pulumi.Input

	ToGkeNodeConfigOutput() GkeNodeConfigOutput
	ToGkeNodeConfigOutputWithContext(context.Context) GkeNodeConfigOutput
}

// Parameters that describe cluster nodes.
type GkeNodeConfigArgs struct {
	// Optional. A list of hardware accelerators (https://cloud.google.com/compute/docs/gpus) to attach to each node.
	Accelerators GkeNodePoolAcceleratorConfigArrayInput `pulumi:"accelerators"`
	// Optional. The Customer Managed Encryption Key (CMEK) (https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek) used to encrypt the boot disk attached to each node in the node pool. Specify the key using the following format: projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}
	BootDiskKmsKey pulumi.StringPtrInput `pulumi:"bootDiskKmsKey"`
	// Optional. The number of local SSD disks to attach to the node, which is limited by the maximum number of disks allowable per zone (see Adding Local SSDs (https://cloud.google.com/compute/docs/disks/local-ssd)).
	LocalSsdCount pulumi.IntPtrInput `pulumi:"localSsdCount"`
	// Optional. The name of a Compute Engine machine type (https://cloud.google.com/compute/docs/machine-types).
	MachineType pulumi.StringPtrInput `pulumi:"machineType"`
	// Optional. Minimum CPU platform (https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform) to be used by this instance. The instance may be scheduled on the specified or a newer CPU platform. Specify the friendly names of CPU platforms, such as "Intel Haswell"` or Intel Sandy Bridge".
	MinCpuPlatform pulumi.StringPtrInput `pulumi:"minCpuPlatform"`
	// Optional. Whether the nodes are created as legacy preemptible VM instances (https://cloud.google.com/compute/docs/instances/preemptible). Also see Spot VMs, preemptible VM instances without a maximum lifetime. Legacy and Spot preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
	Preemptible pulumi.BoolPtrInput `pulumi:"preemptible"`
	// Optional. Whether the nodes are created as Spot VM instances (https://cloud.google.com/compute/docs/instances/spot). Spot VMs are the latest update to legacy preemptible VMs. Spot VMs do not have a maximum lifetime. Legacy and Spot preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
	Spot pulumi.BoolPtrInput `pulumi:"spot"`
}

func (GkeNodeConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodeConfig)(nil)).Elem()
}

func (i GkeNodeConfigArgs) ToGkeNodeConfigOutput() GkeNodeConfigOutput {
	return i.ToGkeNodeConfigOutputWithContext(context.Background())
}

func (i GkeNodeConfigArgs) ToGkeNodeConfigOutputWithContext(ctx context.Context) GkeNodeConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeNodeConfigOutput)
}

func (i GkeNodeConfigArgs) ToGkeNodeConfigPtrOutput() GkeNodeConfigPtrOutput {
	return i.ToGkeNodeConfigPtrOutputWithContext(context.Background())
}

func (i GkeNodeConfigArgs) ToGkeNodeConfigPtrOutputWithContext(ctx context.Context) GkeNodeConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeNodeConfigOutput).ToGkeNodeConfigPtrOutputWithContext(ctx)
}

// GkeNodeConfigPtrInput is an input type that accepts GkeNodeConfigArgs, GkeNodeConfigPtr and GkeNodeConfigPtrOutput values.
// You can construct a concrete instance of `GkeNodeConfigPtrInput` via:
//
//	        GkeNodeConfigArgs{...}
//
//	or:
//
//	        nil
type GkeNodeConfigPtrInput interface {
	pulumi.Input

	ToGkeNodeConfigPtrOutput() GkeNodeConfigPtrOutput
	ToGkeNodeConfigPtrOutputWithContext(context.Context) GkeNodeConfigPtrOutput
}

type gkeNodeConfigPtrType GkeNodeConfigArgs

func GkeNodeConfigPtr(v *GkeNodeConfigArgs) GkeNodeConfigPtrInput {
	return (*gkeNodeConfigPtrType)(v)
}

func (*gkeNodeConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**GkeNodeConfig)(nil)).Elem()
}

func (i *gkeNodeConfigPtrType) ToGkeNodeConfigPtrOutput() GkeNodeConfigPtrOutput {
	return i.ToGkeNodeConfigPtrOutputWithContext(context.Background())
}

func (i *gkeNodeConfigPtrType) ToGkeNodeConfigPtrOutputWithContext(ctx context.Context) GkeNodeConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeNodeConfigPtrOutput)
}

// Parameters that describe cluster nodes.
type GkeNodeConfigOutput struct{ *pulumi.OutputState }

func (GkeNodeConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodeConfig)(nil)).Elem()
}

func (o GkeNodeConfigOutput) ToGkeNodeConfigOutput() GkeNodeConfigOutput {
	return o
}

func (o GkeNodeConfigOutput) ToGkeNodeConfigOutputWithContext(ctx context.Context) GkeNodeConfigOutput {
	return o
}

func (o GkeNodeConfigOutput) ToGkeNodeConfigPtrOutput() GkeNodeConfigPtrOutput {
	return o.ToGkeNodeConfigPtrOutputWithContext(context.Background())
}

func (o GkeNodeConfigOutput) ToGkeNodeConfigPtrOutputWithContext(ctx context.Context) GkeNodeConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v GkeNodeConfig) *GkeNodeConfig {
		return &v
	}).(GkeNodeConfigPtrOutput)
}

// Optional. A list of hardware accelerators (https://cloud.google.com/compute/docs/gpus) to attach to each node.
func (o GkeNodeConfigOutput) Accelerators() GkeNodePoolAcceleratorConfigArrayOutput {
	return o.ApplyT(func(v GkeNodeConfig) []GkeNodePoolAcceleratorConfig { return v.Accelerators }).(GkeNodePoolAcceleratorConfigArrayOutput)
}

// Optional. The Customer Managed Encryption Key (CMEK) (https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek) used to encrypt the boot disk attached to each node in the node pool. Specify the key using the following format: projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}
func (o GkeNodeConfigOutput) BootDiskKmsKey() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GkeNodeConfig) *string { return v.BootDiskKmsKey }).(pulumi.StringPtrOutput)
}

// Optional. The number of local SSD disks to attach to the node, which is limited by the maximum number of disks allowable per zone (see Adding Local SSDs (https://cloud.google.com/compute/docs/disks/local-ssd)).
func (o GkeNodeConfigOutput) LocalSsdCount() pulumi.IntPtrOutput {
	return o.ApplyT(func(v GkeNodeConfig) *int { return v.LocalSsdCount }).(pulumi.IntPtrOutput)
}

// Optional. The name of a Compute Engine machine type (https://cloud.google.com/compute/docs/machine-types).
func (o GkeNodeConfigOutput) MachineType() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GkeNodeConfig) *string { return v.MachineType }).(pulumi.StringPtrOutput)
}

// Optional. Minimum CPU platform (https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform) to be used by this instance. The instance may be scheduled on the specified or a newer CPU platform. Specify the friendly names of CPU platforms, such as "Intel Haswell"` or Intel Sandy Bridge".
func (o GkeNodeConfigOutput) MinCpuPlatform() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GkeNodeConfig) *string { return v.MinCpuPlatform }).(pulumi.StringPtrOutput)
}

// Optional. Whether the nodes are created as legacy preemptible VM instances (https://cloud.google.com/compute/docs/instances/preemptible). Also see Spot VMs, preemptible VM instances without a maximum lifetime. Legacy and Spot preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
func (o GkeNodeConfigOutput) Preemptible() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v GkeNodeConfig) *bool { return v.Preemptible }).(pulumi.BoolPtrOutput)
}

// Optional. Whether the nodes are created as Spot VM instances (https://cloud.google.com/compute/docs/instances/spot). Spot VMs are the latest update to legacy preemptible VMs. Spot VMs do not have a maximum lifetime. Legacy and Spot preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
func (o GkeNodeConfigOutput) Spot() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v GkeNodeConfig) *bool { return v.Spot }).(pulumi.BoolPtrOutput)
}

type GkeNodeConfigPtrOutput struct{ *pulumi.OutputState }

func (GkeNodeConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**GkeNodeConfig)(nil)).Elem()
}

func (o GkeNodeConfigPtrOutput) ToGkeNodeConfigPtrOutput() GkeNodeConfigPtrOutput {
	return o
}

func (o GkeNodeConfigPtrOutput) ToGkeNodeConfigPtrOutputWithContext(ctx context.Context) GkeNodeConfigPtrOutput {
	return o
}

func (o GkeNodeConfigPtrOutput) Elem() GkeNodeConfigOutput {
	return o.ApplyT(func(v *GkeNodeConfig) GkeNodeConfig {
		if v != nil {
			return *v
		}
		var ret GkeNodeConfig
		return ret
	}).(GkeNodeConfigOutput)
}

// Optional. A list of hardware accelerators (https://cloud.google.com/compute/docs/gpus) to attach to each node.
func (o GkeNodeConfigPtrOutput) Accelerators() GkeNodePoolAcceleratorConfigArrayOutput {
	return o.ApplyT(func(v *GkeNodeConfig) []GkeNodePoolAcceleratorConfig {
		if v == nil {
			return nil
		}
		return v.Accelerators
	}).(GkeNodePoolAcceleratorConfigArrayOutput)
}

// Optional. The Customer Managed Encryption Key (CMEK) (https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek) used to encrypt the boot disk attached to each node in the node pool. Specify the key using the following format: projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}
func (o GkeNodeConfigPtrOutput) BootDiskKmsKey() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *GkeNodeConfig) *string {
		if v == nil {
			return nil
		}
		return v.BootDiskKmsKey
	}).(pulumi.StringPtrOutput)
}

// Optional. The number of local SSD disks to attach to the node, which is limited by the maximum number of disks allowable per zone (see Adding Local SSDs (https://cloud.google.com/compute/docs/disks/local-ssd)).
func (o GkeNodeConfigPtrOutput) LocalSsdCount() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *GkeNodeConfig) *int {
		if v == nil {
			return nil
		}
		return v.LocalSsdCount
	}).(pulumi.IntPtrOutput)
}

// Optional. The name of a Compute Engine machine type (https://cloud.google.com/compute/docs/machine-types).
func (o GkeNodeConfigPtrOutput) MachineType() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *GkeNodeConfig) *string {
		if v == nil {
			return nil
		}
		return v.MachineType
	}).(pulumi.StringPtrOutput)
}

// Optional. Minimum CPU platform (https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform) to be used by this instance. The instance may be scheduled on the specified or a newer CPU platform. Specify the friendly names of CPU platforms, such as "Intel Haswell"` or Intel Sandy Bridge".
func (o GkeNodeConfigPtrOutput) MinCpuPlatform() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *GkeNodeConfig) *string {
		if v == nil {
			return nil
		}
		return v.MinCpuPlatform
	}).(pulumi.StringPtrOutput)
}

// Optional. Whether the nodes are created as legacy preemptible VM instances (https://cloud.google.com/compute/docs/instances/preemptible). Also see Spot VMs, preemptible VM instances without a maximum lifetime. Legacy and Spot preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
func (o GkeNodeConfigPtrOutput) Preemptible() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *GkeNodeConfig) *bool {
		if v == nil {
			return nil
		}
		return v.Preemptible
	}).(pulumi.BoolPtrOutput)
}

// Optional. Whether the nodes are created as Spot VM instances (https://cloud.google.com/compute/docs/instances/spot). Spot VMs are the latest update to legacy preemptible VMs. Spot VMs do not have a maximum lifetime. Legacy and Spot preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
func (o GkeNodeConfigPtrOutput) Spot() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *GkeNodeConfig) *bool {
		if v == nil {
			return nil
		}
		return v.Spot
	}).(pulumi.BoolPtrOutput)
}

// Parameters that describe cluster nodes.
type GkeNodeConfigResponse struct {
	// Optional. A list of hardware accelerators (https://cloud.google.com/compute/docs/gpus) to attach to each node.
	Accelerators []GkeNodePoolAcceleratorConfigResponse `pulumi:"accelerators"`
	// Optional. The Customer Managed Encryption Key (CMEK) (https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek) used to encrypt the boot disk attached to each node in the node pool. Specify the key using the following format: projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}
	BootDiskKmsKey string `pulumi:"bootDiskKmsKey"`
	// Optional. The number of local SSD disks to attach to the node, which is limited by the maximum number of disks allowable per zone (see Adding Local SSDs (https://cloud.google.com/compute/docs/disks/local-ssd)).
	LocalSsdCount int `pulumi:"localSsdCount"`
	// Optional. The name of a Compute Engine machine type (https://cloud.google.com/compute/docs/machine-types).
	MachineType string `pulumi:"machineType"`
	// Optional. Minimum CPU platform (https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform) to be used by this instance. The instance may be scheduled on the specified or a newer CPU platform. Specify the friendly names of CPU platforms, such as "Intel Haswell"` or Intel Sandy Bridge".
	MinCpuPlatform string `pulumi:"minCpuPlatform"`
	// Optional. Whether the nodes are created as legacy preemptible VM instances (https://cloud.google.com/compute/docs/instances/preemptible). Also see Spot VMs, preemptible VM instances without a maximum lifetime. Legacy and Spot preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
	Preemptible bool `pulumi:"preemptible"`
	// Optional. Whether the nodes are created as Spot VM instances (https://cloud.google.com/compute/docs/instances/spot). Spot VMs are the latest update to legacy preemptible VMs. Spot VMs do not have a maximum lifetime. Legacy and Spot preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
	Spot bool `pulumi:"spot"`
}

// Parameters that describe cluster nodes.
type GkeNodeConfigResponseOutput struct{ *pulumi.OutputState }

func (GkeNodeConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodeConfigResponse)(nil)).Elem()
}

func (o GkeNodeConfigResponseOutput) ToGkeNodeConfigResponseOutput() GkeNodeConfigResponseOutput {
	return o
}

func (o GkeNodeConfigResponseOutput) ToGkeNodeConfigResponseOutputWithContext(ctx context.Context) GkeNodeConfigResponseOutput {
	return o
}

// Optional. A list of hardware accelerators (https://cloud.google.com/compute/docs/gpus) to attach to each node.
func (o GkeNodeConfigResponseOutput) Accelerators() GkeNodePoolAcceleratorConfigResponseArrayOutput {
	return o.ApplyT(func(v GkeNodeConfigResponse) []GkeNodePoolAcceleratorConfigResponse { return v.Accelerators }).(GkeNodePoolAcceleratorConfigResponseArrayOutput)
}

// Optional. The Customer Managed Encryption Key (CMEK) (https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek) used to encrypt the boot disk attached to each node in the node pool. Specify the key using the following format: projects/{project}/locations/{location}/keyRings/{key_ring}/cryptoKeys/{crypto_key}
func (o GkeNodeConfigResponseOutput) BootDiskKmsKey() pulumi.StringOutput {
	return o.ApplyT(func(v GkeNodeConfigResponse) string { return v.BootDiskKmsKey }).(pulumi.StringOutput)
}

// Optional. The number of local SSD disks to attach to the node, which is limited by the maximum number of disks allowable per zone (see Adding Local SSDs (https://cloud.google.com/compute/docs/disks/local-ssd)).
func (o GkeNodeConfigResponseOutput) LocalSsdCount() pulumi.IntOutput {
	return o.ApplyT(func(v GkeNodeConfigResponse) int { return v.LocalSsdCount }).(pulumi.IntOutput)
}

// Optional. The name of a Compute Engine machine type (https://cloud.google.com/compute/docs/machine-types).
func (o GkeNodeConfigResponseOutput) MachineType() pulumi.StringOutput {
	return o.ApplyT(func(v GkeNodeConfigResponse) string { return v.MachineType }).(pulumi.StringOutput)
}

// Optional. Minimum CPU platform (https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform) to be used by this instance. The instance may be scheduled on the specified or a newer CPU platform. Specify the friendly names of CPU platforms, such as "Intel Haswell"` or Intel Sandy Bridge".
func (o GkeNodeConfigResponseOutput) MinCpuPlatform() pulumi.StringOutput {
	return o.ApplyT(func(v GkeNodeConfigResponse) string { return v.MinCpuPlatform }).(pulumi.StringOutput)
}

// Optional. Whether the nodes are created as legacy preemptible VM instances (https://cloud.google.com/compute/docs/instances/preemptible). Also see Spot VMs, preemptible VM instances without a maximum lifetime. Legacy and Spot preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
func (o GkeNodeConfigResponseOutput) Preemptible() pulumi.BoolOutput {
	return o.ApplyT(func(v GkeNodeConfigResponse) bool { return v.Preemptible }).(pulumi.BoolOutput)
}

// Optional. Whether the nodes are created as Spot VM instances (https://cloud.google.com/compute/docs/instances/spot). Spot VMs are the latest update to legacy preemptible VMs. Spot VMs do not have a maximum lifetime. Legacy and Spot preemptible nodes cannot be used in a node pool with the CONTROLLER role or in the DEFAULT node pool if the CONTROLLER role is not assigned (the DEFAULT node pool will assume the CONTROLLER role).
func (o GkeNodeConfigResponseOutput) Spot() pulumi.BoolOutput {
	return o.ApplyT(func(v GkeNodeConfigResponse) bool { return v.Spot }).(pulumi.BoolOutput)
}

// A GkeNodeConfigAcceleratorConfig represents a Hardware Accelerator request for a node pool.
type GkeNodePoolAcceleratorConfig struct {
	// The number of accelerator cards exposed to an instance.
	AcceleratorCount *string `pulumi:"acceleratorCount"`
	// The accelerator type resource namename (see GPUs on Compute Engine).
	AcceleratorType *string `pulumi:"acceleratorType"`
	// Size of partitions to create on the GPU. Valid values are described in the NVIDIA mig user guide (https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#partitioning).
	GpuPartitionSize *string `pulumi:"gpuPartitionSize"`
}

// GkeNodePoolAcceleratorConfigInput is an input type that accepts GkeNodePoolAcceleratorConfigArgs and GkeNodePoolAcceleratorConfigOutput values.
// You can construct a concrete instance of `GkeNodePoolAcceleratorConfigInput` via:
//
//	GkeNodePoolAcceleratorConfigArgs{...}
type GkeNodePoolAcceleratorConfigInput interface {
	pulumi.Input

	ToGkeNodePoolAcceleratorConfigOutput() GkeNodePoolAcceleratorConfigOutput
	ToGkeNodePoolAcceleratorConfigOutputWithContext(context.Context) GkeNodePoolAcceleratorConfigOutput
}

// A GkeNodeConfigAcceleratorConfig represents a Hardware Accelerator request for a node pool.
type GkeNodePoolAcceleratorConfigArgs struct {
	// The number of accelerator cards exposed to an instance.
	AcceleratorCount pulumi.StringPtrInput `pulumi:"acceleratorCount"`
	// The accelerator type resource namename (see GPUs on Compute Engine).
	AcceleratorType pulumi.StringPtrInput `pulumi:"acceleratorType"`
	// Size of partitions to create on the GPU. Valid values are described in the NVIDIA mig user guide (https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#partitioning).
	GpuPartitionSize pulumi.StringPtrInput `pulumi:"gpuPartitionSize"`
}

func (GkeNodePoolAcceleratorConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodePoolAcceleratorConfig)(nil)).Elem()
}

func (i GkeNodePoolAcceleratorConfigArgs) ToGkeNodePoolAcceleratorConfigOutput() GkeNodePoolAcceleratorConfigOutput {
	return i.ToGkeNodePoolAcceleratorConfigOutputWithContext(context.Background())
}

func (i GkeNodePoolAcceleratorConfigArgs) ToGkeNodePoolAcceleratorConfigOutputWithContext(ctx context.Context) GkeNodePoolAcceleratorConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeNodePoolAcceleratorConfigOutput)
}

// GkeNodePoolAcceleratorConfigArrayInput is an input type that accepts GkeNodePoolAcceleratorConfigArray and GkeNodePoolAcceleratorConfigArrayOutput values.
// You can construct a concrete instance of `GkeNodePoolAcceleratorConfigArrayInput` via:
//
//	GkeNodePoolAcceleratorConfigArray{ GkeNodePoolAcceleratorConfigArgs{...} }
type GkeNodePoolAcceleratorConfigArrayInput interface {
	pulumi.Input

	ToGkeNodePoolAcceleratorConfigArrayOutput() GkeNodePoolAcceleratorConfigArrayOutput
	ToGkeNodePoolAcceleratorConfigArrayOutputWithContext(context.Context) GkeNodePoolAcceleratorConfigArrayOutput
}

type GkeNodePoolAcceleratorConfigArray []GkeNodePoolAcceleratorConfigInput

func (GkeNodePoolAcceleratorConfigArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]GkeNodePoolAcceleratorConfig)(nil)).Elem()
}

func (i GkeNodePoolAcceleratorConfigArray) ToGkeNodePoolAcceleratorConfigArrayOutput() GkeNodePoolAcceleratorConfigArrayOutput {
	return i.ToGkeNodePoolAcceleratorConfigArrayOutputWithContext(context.Background())
}

func (i GkeNodePoolAcceleratorConfigArray) ToGkeNodePoolAcceleratorConfigArrayOutputWithContext(ctx context.Context) GkeNodePoolAcceleratorConfigArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeNodePoolAcceleratorConfigArrayOutput)
}

// A GkeNodeConfigAcceleratorConfig represents a Hardware Accelerator request for a node pool.
type GkeNodePoolAcceleratorConfigOutput struct{ *pulumi.OutputState }

func (GkeNodePoolAcceleratorConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodePoolAcceleratorConfig)(nil)).Elem()
}

func (o GkeNodePoolAcceleratorConfigOutput) ToGkeNodePoolAcceleratorConfigOutput() GkeNodePoolAcceleratorConfigOutput {
	return o
}

func (o GkeNodePoolAcceleratorConfigOutput) ToGkeNodePoolAcceleratorConfigOutputWithContext(ctx context.Context) GkeNodePoolAcceleratorConfigOutput {
	return o
}

// The number of accelerator cards exposed to an instance.
func (o GkeNodePoolAcceleratorConfigOutput) AcceleratorCount() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GkeNodePoolAcceleratorConfig) *string { return v.AcceleratorCount }).(pulumi.StringPtrOutput)
}

// The accelerator type resource namename (see GPUs on Compute Engine).
func (o GkeNodePoolAcceleratorConfigOutput) AcceleratorType() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GkeNodePoolAcceleratorConfig) *string { return v.AcceleratorType }).(pulumi.StringPtrOutput)
}

// Size of partitions to create on the GPU. Valid values are described in the NVIDIA mig user guide (https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#partitioning).
func (o GkeNodePoolAcceleratorConfigOutput) GpuPartitionSize() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GkeNodePoolAcceleratorConfig) *string { return v.GpuPartitionSize }).(pulumi.StringPtrOutput)
}

type GkeNodePoolAcceleratorConfigArrayOutput struct{ *pulumi.OutputState }

func (GkeNodePoolAcceleratorConfigArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]GkeNodePoolAcceleratorConfig)(nil)).Elem()
}

func (o GkeNodePoolAcceleratorConfigArrayOutput) ToGkeNodePoolAcceleratorConfigArrayOutput() GkeNodePoolAcceleratorConfigArrayOutput {
	return o
}

func (o GkeNodePoolAcceleratorConfigArrayOutput) ToGkeNodePoolAcceleratorConfigArrayOutputWithContext(ctx context.Context) GkeNodePoolAcceleratorConfigArrayOutput {
	return o
}

func (o GkeNodePoolAcceleratorConfigArrayOutput) Index(i pulumi.IntInput) GkeNodePoolAcceleratorConfigOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) GkeNodePoolAcceleratorConfig {
		return vs[0].([]GkeNodePoolAcceleratorConfig)[vs[1].(int)]
	}).(GkeNodePoolAcceleratorConfigOutput)
}

// A GkeNodeConfigAcceleratorConfig represents a Hardware Accelerator request for a node pool.
type GkeNodePoolAcceleratorConfigResponse struct {
	// The number of accelerator cards exposed to an instance.
	AcceleratorCount string `pulumi:"acceleratorCount"`
	// The accelerator type resource namename (see GPUs on Compute Engine).
	AcceleratorType string `pulumi:"acceleratorType"`
	// Size of partitions to create on the GPU. Valid values are described in the NVIDIA mig user guide (https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#partitioning).
	GpuPartitionSize string `pulumi:"gpuPartitionSize"`
}

// A GkeNodeConfigAcceleratorConfig represents a Hardware Accelerator request for a node pool.
type GkeNodePoolAcceleratorConfigResponseOutput struct{ *pulumi.OutputState }

func (GkeNodePoolAcceleratorConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodePoolAcceleratorConfigResponse)(nil)).Elem()
}

func (o GkeNodePoolAcceleratorConfigResponseOutput) ToGkeNodePoolAcceleratorConfigResponseOutput() GkeNodePoolAcceleratorConfigResponseOutput {
	return o
}

func (o GkeNodePoolAcceleratorConfigResponseOutput) ToGkeNodePoolAcceleratorConfigResponseOutputWithContext(ctx context.Context) GkeNodePoolAcceleratorConfigResponseOutput {
	return o
}

// The number of accelerator cards exposed to an instance.
func (o GkeNodePoolAcceleratorConfigResponseOutput) AcceleratorCount() pulumi.StringOutput {
	return o.ApplyT(func(v GkeNodePoolAcceleratorConfigResponse) string { return v.AcceleratorCount }).(pulumi.StringOutput)
}

// The accelerator type resource namename (see GPUs on Compute Engine).
func (o GkeNodePoolAcceleratorConfigResponseOutput) AcceleratorType() pulumi.StringOutput {
	return o.ApplyT(func(v GkeNodePoolAcceleratorConfigResponse) string { return v.AcceleratorType }).(pulumi.StringOutput)
}

// Size of partitions to create on the GPU. Valid values are described in the NVIDIA mig user guide (https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#partitioning).
func (o GkeNodePoolAcceleratorConfigResponseOutput) GpuPartitionSize() pulumi.StringOutput {
	return o.ApplyT(func(v GkeNodePoolAcceleratorConfigResponse) string { return v.GpuPartitionSize }).(pulumi.StringOutput)
}

type GkeNodePoolAcceleratorConfigResponseArrayOutput struct{ *pulumi.OutputState }

func (GkeNodePoolAcceleratorConfigResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]GkeNodePoolAcceleratorConfigResponse)(nil)).Elem()
}

func (o GkeNodePoolAcceleratorConfigResponseArrayOutput) ToGkeNodePoolAcceleratorConfigResponseArrayOutput() GkeNodePoolAcceleratorConfigResponseArrayOutput {
	return o
}

func (o GkeNodePoolAcceleratorConfigResponseArrayOutput) ToGkeNodePoolAcceleratorConfigResponseArrayOutputWithContext(ctx context.Context) GkeNodePoolAcceleratorConfigResponseArrayOutput {
	return o
}

func (o GkeNodePoolAcceleratorConfigResponseArrayOutput) Index(i pulumi.IntInput) GkeNodePoolAcceleratorConfigResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) GkeNodePoolAcceleratorConfigResponse {
		return vs[0].([]GkeNodePoolAcceleratorConfigResponse)[vs[1].(int)]
	}).(GkeNodePoolAcceleratorConfigResponseOutput)
}

// GkeNodePoolAutoscaling contains information the cluster autoscaler needs to adjust the size of the node pool to the current cluster usage.
type GkeNodePoolAutoscalingConfig struct {
	// The maximum number of nodes in the node pool. Must be >= min_node_count, and must be > 0. Note: Quota must be sufficient to scale up the cluster.
	MaxNodeCount *int `pulumi:"maxNodeCount"`
	// The minimum number of nodes in the node pool. Must be >= 0 and <= max_node_count.
	MinNodeCount *int `pulumi:"minNodeCount"`
}

// GkeNodePoolAutoscalingConfigInput is an input type that accepts GkeNodePoolAutoscalingConfigArgs and GkeNodePoolAutoscalingConfigOutput values.
// You can construct a concrete instance of `GkeNodePoolAutoscalingConfigInput` via:
//
//	GkeNodePoolAutoscalingConfigArgs{...}
type GkeNodePoolAutoscalingConfigInput interface {
	pulumi.Input

	ToGkeNodePoolAutoscalingConfigOutput() GkeNodePoolAutoscalingConfigOutput
	ToGkeNodePoolAutoscalingConfigOutputWithContext(context.Context) GkeNodePoolAutoscalingConfigOutput
}

// GkeNodePoolAutoscaling contains information the cluster autoscaler needs to adjust the size of the node pool to the current cluster usage.
type GkeNodePoolAutoscalingConfigArgs struct {
	// The maximum number of nodes in the node pool. Must be >= min_node_count, and must be > 0. Note: Quota must be sufficient to scale up the cluster.
	MaxNodeCount pulumi.IntPtrInput `pulumi:"maxNodeCount"`
	// The minimum number of nodes in the node pool. Must be >= 0 and <= max_node_count.
	MinNodeCount pulumi.IntPtrInput `pulumi:"minNodeCount"`
}

func (GkeNodePoolAutoscalingConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodePoolAutoscalingConfig)(nil)).Elem()
}

func (i GkeNodePoolAutoscalingConfigArgs) ToGkeNodePoolAutoscalingConfigOutput() GkeNodePoolAutoscalingConfigOutput {
	return i.ToGkeNodePoolAutoscalingConfigOutputWithContext(context.Background())
}

func (i GkeNodePoolAutoscalingConfigArgs) ToGkeNodePoolAutoscalingConfigOutputWithContext(ctx context.Context) GkeNodePoolAutoscalingConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeNodePoolAutoscalingConfigOutput)
}

func (i GkeNodePoolAutoscalingConfigArgs) ToGkeNodePoolAutoscalingConfigPtrOutput() GkeNodePoolAutoscalingConfigPtrOutput {
	return i.ToGkeNodePoolAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i GkeNodePoolAutoscalingConfigArgs) ToGkeNodePoolAutoscalingConfigPtrOutputWithContext(ctx context.Context) GkeNodePoolAutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeNodePoolAutoscalingConfigOutput).ToGkeNodePoolAutoscalingConfigPtrOutputWithContext(ctx)
}

// GkeNodePoolAutoscalingConfigPtrInput is an input type that accepts GkeNodePoolAutoscalingConfigArgs, GkeNodePoolAutoscalingConfigPtr and GkeNodePoolAutoscalingConfigPtrOutput values.
// You can construct a concrete instance of `GkeNodePoolAutoscalingConfigPtrInput` via:
//
//	        GkeNodePoolAutoscalingConfigArgs{...}
//
//	or:
//
//	        nil
type GkeNodePoolAutoscalingConfigPtrInput interface {
	pulumi.Input

	ToGkeNodePoolAutoscalingConfigPtrOutput() GkeNodePoolAutoscalingConfigPtrOutput
	ToGkeNodePoolAutoscalingConfigPtrOutputWithContext(context.Context) GkeNodePoolAutoscalingConfigPtrOutput
}

type gkeNodePoolAutoscalingConfigPtrType GkeNodePoolAutoscalingConfigArgs

func GkeNodePoolAutoscalingConfigPtr(v *GkeNodePoolAutoscalingConfigArgs) GkeNodePoolAutoscalingConfigPtrInput {
	return (*gkeNodePoolAutoscalingConfigPtrType)(v)
}

func (*gkeNodePoolAutoscalingConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**GkeNodePoolAutoscalingConfig)(nil)).Elem()
}

func (i *gkeNodePoolAutoscalingConfigPtrType) ToGkeNodePoolAutoscalingConfigPtrOutput() GkeNodePoolAutoscalingConfigPtrOutput {
	return i.ToGkeNodePoolAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i *gkeNodePoolAutoscalingConfigPtrType) ToGkeNodePoolAutoscalingConfigPtrOutputWithContext(ctx context.Context) GkeNodePoolAutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeNodePoolAutoscalingConfigPtrOutput)
}

// GkeNodePoolAutoscaling contains information the cluster autoscaler needs to adjust the size of the node pool to the current cluster usage.
type GkeNodePoolAutoscalingConfigOutput struct{ *pulumi.OutputState }

func (GkeNodePoolAutoscalingConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodePoolAutoscalingConfig)(nil)).Elem()
}

func (o GkeNodePoolAutoscalingConfigOutput) ToGkeNodePoolAutoscalingConfigOutput() GkeNodePoolAutoscalingConfigOutput {
	return o
}

func (o GkeNodePoolAutoscalingConfigOutput) ToGkeNodePoolAutoscalingConfigOutputWithContext(ctx context.Context) GkeNodePoolAutoscalingConfigOutput {
	return o
}

func (o GkeNodePoolAutoscalingConfigOutput) ToGkeNodePoolAutoscalingConfigPtrOutput() GkeNodePoolAutoscalingConfigPtrOutput {
	return o.ToGkeNodePoolAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (o GkeNodePoolAutoscalingConfigOutput) ToGkeNodePoolAutoscalingConfigPtrOutputWithContext(ctx context.Context) GkeNodePoolAutoscalingConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v GkeNodePoolAutoscalingConfig) *GkeNodePoolAutoscalingConfig {
		return &v
	}).(GkeNodePoolAutoscalingConfigPtrOutput)
}

// The maximum number of nodes in the node pool. Must be >= min_node_count, and must be > 0. Note: Quota must be sufficient to scale up the cluster.
func (o GkeNodePoolAutoscalingConfigOutput) MaxNodeCount() pulumi.IntPtrOutput {
	return o.ApplyT(func(v GkeNodePoolAutoscalingConfig) *int { return v.MaxNodeCount }).(pulumi.IntPtrOutput)
}

// The minimum number of nodes in the node pool. Must be >= 0 and <= max_node_count.
func (o GkeNodePoolAutoscalingConfigOutput) MinNodeCount() pulumi.IntPtrOutput {
	return o.ApplyT(func(v GkeNodePoolAutoscalingConfig) *int { return v.MinNodeCount }).(pulumi.IntPtrOutput)
}

type GkeNodePoolAutoscalingConfigPtrOutput struct{ *pulumi.OutputState }

func (GkeNodePoolAutoscalingConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**GkeNodePoolAutoscalingConfig)(nil)).Elem()
}

func (o GkeNodePoolAutoscalingConfigPtrOutput) ToGkeNodePoolAutoscalingConfigPtrOutput() GkeNodePoolAutoscalingConfigPtrOutput {
	return o
}

func (o GkeNodePoolAutoscalingConfigPtrOutput) ToGkeNodePoolAutoscalingConfigPtrOutputWithContext(ctx context.Context) GkeNodePoolAutoscalingConfigPtrOutput {
	return o
}

func (o GkeNodePoolAutoscalingConfigPtrOutput) Elem() GkeNodePoolAutoscalingConfigOutput {
	return o.ApplyT(func(v *GkeNodePoolAutoscalingConfig) GkeNodePoolAutoscalingConfig {
		if v != nil {
			return *v
		}
		var ret GkeNodePoolAutoscalingConfig
		return ret
	}).(GkeNodePoolAutoscalingConfigOutput)
}

// The maximum number of nodes in the node pool. Must be >= min_node_count, and must be > 0. Note: Quota must be sufficient to scale up the cluster.
func (o GkeNodePoolAutoscalingConfigPtrOutput) MaxNodeCount() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *GkeNodePoolAutoscalingConfig) *int {
		if v == nil {
			return nil
		}
		return v.MaxNodeCount
	}).(pulumi.IntPtrOutput)
}

// The minimum number of nodes in the node pool. Must be >= 0 and <= max_node_count.
func (o GkeNodePoolAutoscalingConfigPtrOutput) MinNodeCount() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *GkeNodePoolAutoscalingConfig) *int {
		if v == nil {
			return nil
		}
		return v.MinNodeCount
	}).(pulumi.IntPtrOutput)
}

// GkeNodePoolAutoscaling contains information the cluster autoscaler needs to adjust the size of the node pool to the current cluster usage.
type GkeNodePoolAutoscalingConfigResponse struct {
	// The maximum number of nodes in the node pool. Must be >= min_node_count, and must be > 0. Note: Quota must be sufficient to scale up the cluster.
	MaxNodeCount int `pulumi:"maxNodeCount"`
	// The minimum number of nodes in the node pool. Must be >= 0 and <= max_node_count.
	MinNodeCount int `pulumi:"minNodeCount"`
}

// GkeNodePoolAutoscaling contains information the cluster autoscaler needs to adjust the size of the node pool to the current cluster usage.
type GkeNodePoolAutoscalingConfigResponseOutput struct{ *pulumi.OutputState }

func (GkeNodePoolAutoscalingConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodePoolAutoscalingConfigResponse)(nil)).Elem()
}

func (o GkeNodePoolAutoscalingConfigResponseOutput) ToGkeNodePoolAutoscalingConfigResponseOutput() GkeNodePoolAutoscalingConfigResponseOutput {
	return o
}

func (o GkeNodePoolAutoscalingConfigResponseOutput) ToGkeNodePoolAutoscalingConfigResponseOutputWithContext(ctx context.Context) GkeNodePoolAutoscalingConfigResponseOutput {
	return o
}

// The maximum number of nodes in the node pool. Must be >= min_node_count, and must be > 0. Note: Quota must be sufficient to scale up the cluster.
func (o GkeNodePoolAutoscalingConfigResponseOutput) MaxNodeCount() pulumi.IntOutput {
	return o.ApplyT(func(v GkeNodePoolAutoscalingConfigResponse) int { return v.MaxNodeCount }).(pulumi.IntOutput)
}

// The minimum number of nodes in the node pool. Must be >= 0 and <= max_node_count.
func (o GkeNodePoolAutoscalingConfigResponseOutput) MinNodeCount() pulumi.IntOutput {
	return o.ApplyT(func(v GkeNodePoolAutoscalingConfigResponse) int { return v.MinNodeCount }).(pulumi.IntOutput)
}

// The configuration of a GKE node pool used by a Dataproc-on-GKE cluster (https://cloud.google.com/dataproc/docs/concepts/jobs/dataproc-gke#create-a-dataproc-on-gke-cluster).
type GkeNodePoolConfig struct {
	// Optional. The autoscaler configuration for this node pool. The autoscaler is enabled only when a valid configuration is present.
	Autoscaling *GkeNodePoolAutoscalingConfig `pulumi:"autoscaling"`
	// Optional. The node pool configuration.
	Config *GkeNodeConfig `pulumi:"config"`
	// Optional. The list of Compute Engine zones (https://cloud.google.com/compute/docs/zones#available) where node pool nodes associated with a Dataproc on GKE virtual cluster will be located.Note: All node pools associated with a virtual cluster must be located in the same region as the virtual cluster, and they must be located in the same zone within that region.If a location is not specified during node pool creation, Dataproc on GKE will choose the zone.
	Locations []string `pulumi:"locations"`
}

// GkeNodePoolConfigInput is an input type that accepts GkeNodePoolConfigArgs and GkeNodePoolConfigOutput values.
// You can construct a concrete instance of `GkeNodePoolConfigInput` via:
//
//	GkeNodePoolConfigArgs{...}
type GkeNodePoolConfigInput interface {
	pulumi.Input

	ToGkeNodePoolConfigOutput() GkeNodePoolConfigOutput
	ToGkeNodePoolConfigOutputWithContext(context.Context) GkeNodePoolConfigOutput
}

// The configuration of a GKE node pool used by a Dataproc-on-GKE cluster (https://cloud.google.com/dataproc/docs/concepts/jobs/dataproc-gke#create-a-dataproc-on-gke-cluster).
type GkeNodePoolConfigArgs struct {
	// Optional. The autoscaler configuration for this node pool. The autoscaler is enabled only when a valid configuration is present.
	Autoscaling GkeNodePoolAutoscalingConfigPtrInput `pulumi:"autoscaling"`
	// Optional. The node pool configuration.
	Config GkeNodeConfigPtrInput `pulumi:"config"`
	// Optional. The list of Compute Engine zones (https://cloud.google.com/compute/docs/zones#available) where node pool nodes associated with a Dataproc on GKE virtual cluster will be located.Note: All node pools associated with a virtual cluster must be located in the same region as the virtual cluster, and they must be located in the same zone within that region.If a location is not specified during node pool creation, Dataproc on GKE will choose the zone.
	Locations pulumi.StringArrayInput `pulumi:"locations"`
}

func (GkeNodePoolConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodePoolConfig)(nil)).Elem()
}

func (i GkeNodePoolConfigArgs) ToGkeNodePoolConfigOutput() GkeNodePoolConfigOutput {
	return i.ToGkeNodePoolConfigOutputWithContext(context.Background())
}

func (i GkeNodePoolConfigArgs) ToGkeNodePoolConfigOutputWithContext(ctx context.Context) GkeNodePoolConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeNodePoolConfigOutput)
}

func (i GkeNodePoolConfigArgs) ToGkeNodePoolConfigPtrOutput() GkeNodePoolConfigPtrOutput {
	return i.ToGkeNodePoolConfigPtrOutputWithContext(context.Background())
}

func (i GkeNodePoolConfigArgs) ToGkeNodePoolConfigPtrOutputWithContext(ctx context.Context) GkeNodePoolConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeNodePoolConfigOutput).ToGkeNodePoolConfigPtrOutputWithContext(ctx)
}

// GkeNodePoolConfigPtrInput is an input type that accepts GkeNodePoolConfigArgs, GkeNodePoolConfigPtr and GkeNodePoolConfigPtrOutput values.
// You can construct a concrete instance of `GkeNodePoolConfigPtrInput` via:
//
//	        GkeNodePoolConfigArgs{...}
//
//	or:
//
//	        nil
type GkeNodePoolConfigPtrInput interface {
	pulumi.Input

	ToGkeNodePoolConfigPtrOutput() GkeNodePoolConfigPtrOutput
	ToGkeNodePoolConfigPtrOutputWithContext(context.Context) GkeNodePoolConfigPtrOutput
}

type gkeNodePoolConfigPtrType GkeNodePoolConfigArgs

func GkeNodePoolConfigPtr(v *GkeNodePoolConfigArgs) GkeNodePoolConfigPtrInput {
	return (*gkeNodePoolConfigPtrType)(v)
}

func (*gkeNodePoolConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**GkeNodePoolConfig)(nil)).Elem()
}

func (i *gkeNodePoolConfigPtrType) ToGkeNodePoolConfigPtrOutput() GkeNodePoolConfigPtrOutput {
	return i.ToGkeNodePoolConfigPtrOutputWithContext(context.Background())
}

func (i *gkeNodePoolConfigPtrType) ToGkeNodePoolConfigPtrOutputWithContext(ctx context.Context) GkeNodePoolConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeNodePoolConfigPtrOutput)
}

// The configuration of a GKE node pool used by a Dataproc-on-GKE cluster (https://cloud.google.com/dataproc/docs/concepts/jobs/dataproc-gke#create-a-dataproc-on-gke-cluster).
type GkeNodePoolConfigOutput struct{ *pulumi.OutputState }

func (GkeNodePoolConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodePoolConfig)(nil)).Elem()
}

func (o GkeNodePoolConfigOutput) ToGkeNodePoolConfigOutput() GkeNodePoolConfigOutput {
	return o
}

func (o GkeNodePoolConfigOutput) ToGkeNodePoolConfigOutputWithContext(ctx context.Context) GkeNodePoolConfigOutput {
	return o
}

func (o GkeNodePoolConfigOutput) ToGkeNodePoolConfigPtrOutput() GkeNodePoolConfigPtrOutput {
	return o.ToGkeNodePoolConfigPtrOutputWithContext(context.Background())
}

func (o GkeNodePoolConfigOutput) ToGkeNodePoolConfigPtrOutputWithContext(ctx context.Context) GkeNodePoolConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v GkeNodePoolConfig) *GkeNodePoolConfig {
		return &v
	}).(GkeNodePoolConfigPtrOutput)
}

// Optional. The autoscaler configuration for this node pool. The autoscaler is enabled only when a valid configuration is present.
func (o GkeNodePoolConfigOutput) Autoscaling() GkeNodePoolAutoscalingConfigPtrOutput {
	return o.ApplyT(func(v GkeNodePoolConfig) *GkeNodePoolAutoscalingConfig { return v.Autoscaling }).(GkeNodePoolAutoscalingConfigPtrOutput)
}

// Optional. The node pool configuration.
func (o GkeNodePoolConfigOutput) Config() GkeNodeConfigPtrOutput {
	return o.ApplyT(func(v GkeNodePoolConfig) *GkeNodeConfig { return v.Config }).(GkeNodeConfigPtrOutput)
}

// Optional. The list of Compute Engine zones (https://cloud.google.com/compute/docs/zones#available) where node pool nodes associated with a Dataproc on GKE virtual cluster will be located.Note: All node pools associated with a virtual cluster must be located in the same region as the virtual cluster, and they must be located in the same zone within that region.If a location is not specified during node pool creation, Dataproc on GKE will choose the zone.
func (o GkeNodePoolConfigOutput) Locations() pulumi.StringArrayOutput {
	return o.ApplyT(func(v GkeNodePoolConfig) []string { return v.Locations }).(pulumi.StringArrayOutput)
}

type GkeNodePoolConfigPtrOutput struct{ *pulumi.OutputState }

func (GkeNodePoolConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**GkeNodePoolConfig)(nil)).Elem()
}

func (o GkeNodePoolConfigPtrOutput) ToGkeNodePoolConfigPtrOutput() GkeNodePoolConfigPtrOutput {
	return o
}

func (o GkeNodePoolConfigPtrOutput) ToGkeNodePoolConfigPtrOutputWithContext(ctx context.Context) GkeNodePoolConfigPtrOutput {
	return o
}

func (o GkeNodePoolConfigPtrOutput) Elem() GkeNodePoolConfigOutput {
	return o.ApplyT(func(v *GkeNodePoolConfig) GkeNodePoolConfig {
		if v != nil {
			return *v
		}
		var ret GkeNodePoolConfig
		return ret
	}).(GkeNodePoolConfigOutput)
}

// Optional. The autoscaler configuration for this node pool. The autoscaler is enabled only when a valid configuration is present.
func (o GkeNodePoolConfigPtrOutput) Autoscaling() GkeNodePoolAutoscalingConfigPtrOutput {
	return o.ApplyT(func(v *GkeNodePoolConfig) *GkeNodePoolAutoscalingConfig {
		if v == nil {
			return nil
		}
		return v.Autoscaling
	}).(GkeNodePoolAutoscalingConfigPtrOutput)
}

// Optional. The node pool configuration.
func (o GkeNodePoolConfigPtrOutput) Config() GkeNodeConfigPtrOutput {
	return o.ApplyT(func(v *GkeNodePoolConfig) *GkeNodeConfig {
		if v == nil {
			return nil
		}
		return v.Config
	}).(GkeNodeConfigPtrOutput)
}

// Optional. The list of Compute Engine zones (https://cloud.google.com/compute/docs/zones#available) where node pool nodes associated with a Dataproc on GKE virtual cluster will be located.Note: All node pools associated with a virtual cluster must be located in the same region as the virtual cluster, and they must be located in the same zone within that region.If a location is not specified during node pool creation, Dataproc on GKE will choose the zone.
func (o GkeNodePoolConfigPtrOutput) Locations() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *GkeNodePoolConfig) []string {
		if v == nil {
			return nil
		}
		return v.Locations
	}).(pulumi.StringArrayOutput)
}

// The configuration of a GKE node pool used by a Dataproc-on-GKE cluster (https://cloud.google.com/dataproc/docs/concepts/jobs/dataproc-gke#create-a-dataproc-on-gke-cluster).
type GkeNodePoolConfigResponse struct {
	// Optional. The autoscaler configuration for this node pool. The autoscaler is enabled only when a valid configuration is present.
	Autoscaling GkeNodePoolAutoscalingConfigResponse `pulumi:"autoscaling"`
	// Optional. The node pool configuration.
	Config GkeNodeConfigResponse `pulumi:"config"`
	// Optional. The list of Compute Engine zones (https://cloud.google.com/compute/docs/zones#available) where node pool nodes associated with a Dataproc on GKE virtual cluster will be located.Note: All node pools associated with a virtual cluster must be located in the same region as the virtual cluster, and they must be located in the same zone within that region.If a location is not specified during node pool creation, Dataproc on GKE will choose the zone.
	Locations []string `pulumi:"locations"`
}

// The configuration of a GKE node pool used by a Dataproc-on-GKE cluster (https://cloud.google.com/dataproc/docs/concepts/jobs/dataproc-gke#create-a-dataproc-on-gke-cluster).
type GkeNodePoolConfigResponseOutput struct{ *pulumi.OutputState }

func (GkeNodePoolConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodePoolConfigResponse)(nil)).Elem()
}

func (o GkeNodePoolConfigResponseOutput) ToGkeNodePoolConfigResponseOutput() GkeNodePoolConfigResponseOutput {
	return o
}

func (o GkeNodePoolConfigResponseOutput) ToGkeNodePoolConfigResponseOutputWithContext(ctx context.Context) GkeNodePoolConfigResponseOutput {
	return o
}

// Optional. The autoscaler configuration for this node pool. The autoscaler is enabled only when a valid configuration is present.
func (o GkeNodePoolConfigResponseOutput) Autoscaling() GkeNodePoolAutoscalingConfigResponseOutput {
	return o.ApplyT(func(v GkeNodePoolConfigResponse) GkeNodePoolAutoscalingConfigResponse { return v.Autoscaling }).(GkeNodePoolAutoscalingConfigResponseOutput)
}

// Optional. The node pool configuration.
func (o GkeNodePoolConfigResponseOutput) Config() GkeNodeConfigResponseOutput {
	return o.ApplyT(func(v GkeNodePoolConfigResponse) GkeNodeConfigResponse { return v.Config }).(GkeNodeConfigResponseOutput)
}

// Optional. The list of Compute Engine zones (https://cloud.google.com/compute/docs/zones#available) where node pool nodes associated with a Dataproc on GKE virtual cluster will be located.Note: All node pools associated with a virtual cluster must be located in the same region as the virtual cluster, and they must be located in the same zone within that region.If a location is not specified during node pool creation, Dataproc on GKE will choose the zone.
func (o GkeNodePoolConfigResponseOutput) Locations() pulumi.StringArrayOutput {
	return o.ApplyT(func(v GkeNodePoolConfigResponse) []string { return v.Locations }).(pulumi.StringArrayOutput)
}

// GKE node pools that Dataproc workloads run on.
type GkeNodePoolTarget struct {
	// The target GKE node pool. Format: 'projects/{project}/locations/{location}/clusters/{cluster}/nodePools/{node_pool}'
	NodePool string `pulumi:"nodePool"`
	// Input only. The configuration for the GKE node pool.If specified, Dataproc attempts to create a node pool with the specified shape. If one with the same name already exists, it is verified against all specified fields. If a field differs, the virtual cluster creation will fail.If omitted, any node pool with the specified name is used. If a node pool with the specified name does not exist, Dataproc create a node pool with default values.This is an input only field. It will not be returned by the API.
	NodePoolConfig *GkeNodePoolConfig `pulumi:"nodePoolConfig"`
	// The roles associated with the GKE node pool.
	Roles []GkeNodePoolTargetRolesItem `pulumi:"roles"`
}

// GkeNodePoolTargetInput is an input type that accepts GkeNodePoolTargetArgs and GkeNodePoolTargetOutput values.
// You can construct a concrete instance of `GkeNodePoolTargetInput` via:
//
//	GkeNodePoolTargetArgs{...}
type GkeNodePoolTargetInput interface {
	pulumi.Input

	ToGkeNodePoolTargetOutput() GkeNodePoolTargetOutput
	ToGkeNodePoolTargetOutputWithContext(context.Context) GkeNodePoolTargetOutput
}

// GKE node pools that Dataproc workloads run on.
type GkeNodePoolTargetArgs struct {
	// The target GKE node pool. Format: 'projects/{project}/locations/{location}/clusters/{cluster}/nodePools/{node_pool}'
	NodePool pulumi.StringInput `pulumi:"nodePool"`
	// Input only. The configuration for the GKE node pool.If specified, Dataproc attempts to create a node pool with the specified shape. If one with the same name already exists, it is verified against all specified fields. If a field differs, the virtual cluster creation will fail.If omitted, any node pool with the specified name is used. If a node pool with the specified name does not exist, Dataproc create a node pool with default values.This is an input only field. It will not be returned by the API.
	NodePoolConfig GkeNodePoolConfigPtrInput `pulumi:"nodePoolConfig"`
	// The roles associated with the GKE node pool.
	Roles GkeNodePoolTargetRolesItemArrayInput `pulumi:"roles"`
}

func (GkeNodePoolTargetArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodePoolTarget)(nil)).Elem()
}

func (i GkeNodePoolTargetArgs) ToGkeNodePoolTargetOutput() GkeNodePoolTargetOutput {
	return i.ToGkeNodePoolTargetOutputWithContext(context.Background())
}

func (i GkeNodePoolTargetArgs) ToGkeNodePoolTargetOutputWithContext(ctx context.Context) GkeNodePoolTargetOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeNodePoolTargetOutput)
}

// GkeNodePoolTargetArrayInput is an input type that accepts GkeNodePoolTargetArray and GkeNodePoolTargetArrayOutput values.
// You can construct a concrete instance of `GkeNodePoolTargetArrayInput` via:
//
//	GkeNodePoolTargetArray{ GkeNodePoolTargetArgs{...} }
type GkeNodePoolTargetArrayInput interface {
	pulumi.Input

	ToGkeNodePoolTargetArrayOutput() GkeNodePoolTargetArrayOutput
	ToGkeNodePoolTargetArrayOutputWithContext(context.Context) GkeNodePoolTargetArrayOutput
}

type GkeNodePoolTargetArray []GkeNodePoolTargetInput

func (GkeNodePoolTargetArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]GkeNodePoolTarget)(nil)).Elem()
}

func (i GkeNodePoolTargetArray) ToGkeNodePoolTargetArrayOutput() GkeNodePoolTargetArrayOutput {
	return i.ToGkeNodePoolTargetArrayOutputWithContext(context.Background())
}

func (i GkeNodePoolTargetArray) ToGkeNodePoolTargetArrayOutputWithContext(ctx context.Context) GkeNodePoolTargetArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeNodePoolTargetArrayOutput)
}

// GKE node pools that Dataproc workloads run on.
type GkeNodePoolTargetOutput struct{ *pulumi.OutputState }

func (GkeNodePoolTargetOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodePoolTarget)(nil)).Elem()
}

func (o GkeNodePoolTargetOutput) ToGkeNodePoolTargetOutput() GkeNodePoolTargetOutput {
	return o
}

func (o GkeNodePoolTargetOutput) ToGkeNodePoolTargetOutputWithContext(ctx context.Context) GkeNodePoolTargetOutput {
	return o
}

// The target GKE node pool. Format: 'projects/{project}/locations/{location}/clusters/{cluster}/nodePools/{node_pool}'
func (o GkeNodePoolTargetOutput) NodePool() pulumi.StringOutput {
	return o.ApplyT(func(v GkeNodePoolTarget) string { return v.NodePool }).(pulumi.StringOutput)
}

// Input only. The configuration for the GKE node pool.If specified, Dataproc attempts to create a node pool with the specified shape. If one with the same name already exists, it is verified against all specified fields. If a field differs, the virtual cluster creation will fail.If omitted, any node pool with the specified name is used. If a node pool with the specified name does not exist, Dataproc create a node pool with default values.This is an input only field. It will not be returned by the API.
func (o GkeNodePoolTargetOutput) NodePoolConfig() GkeNodePoolConfigPtrOutput {
	return o.ApplyT(func(v GkeNodePoolTarget) *GkeNodePoolConfig { return v.NodePoolConfig }).(GkeNodePoolConfigPtrOutput)
}

// The roles associated with the GKE node pool.
func (o GkeNodePoolTargetOutput) Roles() GkeNodePoolTargetRolesItemArrayOutput {
	return o.ApplyT(func(v GkeNodePoolTarget) []GkeNodePoolTargetRolesItem { return v.Roles }).(GkeNodePoolTargetRolesItemArrayOutput)
}

type GkeNodePoolTargetArrayOutput struct{ *pulumi.OutputState }

func (GkeNodePoolTargetArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]GkeNodePoolTarget)(nil)).Elem()
}

func (o GkeNodePoolTargetArrayOutput) ToGkeNodePoolTargetArrayOutput() GkeNodePoolTargetArrayOutput {
	return o
}

func (o GkeNodePoolTargetArrayOutput) ToGkeNodePoolTargetArrayOutputWithContext(ctx context.Context) GkeNodePoolTargetArrayOutput {
	return o
}

func (o GkeNodePoolTargetArrayOutput) Index(i pulumi.IntInput) GkeNodePoolTargetOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) GkeNodePoolTarget {
		return vs[0].([]GkeNodePoolTarget)[vs[1].(int)]
	}).(GkeNodePoolTargetOutput)
}

// GKE node pools that Dataproc workloads run on.
type GkeNodePoolTargetResponse struct {
	// The target GKE node pool. Format: 'projects/{project}/locations/{location}/clusters/{cluster}/nodePools/{node_pool}'
	NodePool string `pulumi:"nodePool"`
	// Input only. The configuration for the GKE node pool.If specified, Dataproc attempts to create a node pool with the specified shape. If one with the same name already exists, it is verified against all specified fields. If a field differs, the virtual cluster creation will fail.If omitted, any node pool with the specified name is used. If a node pool with the specified name does not exist, Dataproc create a node pool with default values.This is an input only field. It will not be returned by the API.
	NodePoolConfig GkeNodePoolConfigResponse `pulumi:"nodePoolConfig"`
	// The roles associated with the GKE node pool.
	Roles []string `pulumi:"roles"`
}

// GKE node pools that Dataproc workloads run on.
type GkeNodePoolTargetResponseOutput struct{ *pulumi.OutputState }

func (GkeNodePoolTargetResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeNodePoolTargetResponse)(nil)).Elem()
}

func (o GkeNodePoolTargetResponseOutput) ToGkeNodePoolTargetResponseOutput() GkeNodePoolTargetResponseOutput {
	return o
}

func (o GkeNodePoolTargetResponseOutput) ToGkeNodePoolTargetResponseOutputWithContext(ctx context.Context) GkeNodePoolTargetResponseOutput {
	return o
}

// The target GKE node pool. Format: 'projects/{project}/locations/{location}/clusters/{cluster}/nodePools/{node_pool}'
func (o GkeNodePoolTargetResponseOutput) NodePool() pulumi.StringOutput {
	return o.ApplyT(func(v GkeNodePoolTargetResponse) string { return v.NodePool }).(pulumi.StringOutput)
}

// Input only. The configuration for the GKE node pool.If specified, Dataproc attempts to create a node pool with the specified shape. If one with the same name already exists, it is verified against all specified fields. If a field differs, the virtual cluster creation will fail.If omitted, any node pool with the specified name is used. If a node pool with the specified name does not exist, Dataproc create a node pool with default values.This is an input only field. It will not be returned by the API.
func (o GkeNodePoolTargetResponseOutput) NodePoolConfig() GkeNodePoolConfigResponseOutput {
	return o.ApplyT(func(v GkeNodePoolTargetResponse) GkeNodePoolConfigResponse { return v.NodePoolConfig }).(GkeNodePoolConfigResponseOutput)
}

// The roles associated with the GKE node pool.
func (o GkeNodePoolTargetResponseOutput) Roles() pulumi.StringArrayOutput {
	return o.ApplyT(func(v GkeNodePoolTargetResponse) []string { return v.Roles }).(pulumi.StringArrayOutput)
}

type GkeNodePoolTargetResponseArrayOutput struct{ *pulumi.OutputState }

func (GkeNodePoolTargetResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]GkeNodePoolTargetResponse)(nil)).Elem()
}

func (o GkeNodePoolTargetResponseArrayOutput) ToGkeNodePoolTargetResponseArrayOutput() GkeNodePoolTargetResponseArrayOutput {
	return o
}

func (o GkeNodePoolTargetResponseArrayOutput) ToGkeNodePoolTargetResponseArrayOutputWithContext(ctx context.Context) GkeNodePoolTargetResponseArrayOutput {
	return o
}

func (o GkeNodePoolTargetResponseArrayOutput) Index(i pulumi.IntInput) GkeNodePoolTargetResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) GkeNodePoolTargetResponse {
		return vs[0].([]GkeNodePoolTargetResponse)[vs[1].(int)]
	}).(GkeNodePoolTargetResponseOutput)
}

// A Dataproc job for running Apache Hadoop MapReduce (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) jobs on Apache Hadoop YARN (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
type HadoopJob struct {
	// Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris.
	MainClass *string `pulumi:"mainClass"`
	// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
	MainJarFileUri *string `pulumi:"mainJarFileUri"`
	// Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
	Properties map[string]string `pulumi:"properties"`
}

// HadoopJobInput is an input type that accepts HadoopJobArgs and HadoopJobOutput values.
// You can construct a concrete instance of `HadoopJobInput` via:
//
//	HadoopJobArgs{...}
type HadoopJobInput interface {
	pulumi.Input

	ToHadoopJobOutput() HadoopJobOutput
	ToHadoopJobOutputWithContext(context.Context) HadoopJobOutput
}

// A Dataproc job for running Apache Hadoop MapReduce (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) jobs on Apache Hadoop YARN (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
type HadoopJobArgs struct {
	// Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris.
	MainClass pulumi.StringPtrInput `pulumi:"mainClass"`
	// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
	MainJarFileUri pulumi.StringPtrInput `pulumi:"mainJarFileUri"`
	// Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
	Properties pulumi.StringMapInput `pulumi:"properties"`
}

func (HadoopJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*HadoopJob)(nil)).Elem()
}

func (i HadoopJobArgs) ToHadoopJobOutput() HadoopJobOutput {
	return i.ToHadoopJobOutputWithContext(context.Background())
}

func (i HadoopJobArgs) ToHadoopJobOutputWithContext(ctx context.Context) HadoopJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(HadoopJobOutput)
}

func (i HadoopJobArgs) ToHadoopJobPtrOutput() HadoopJobPtrOutput {
	return i.ToHadoopJobPtrOutputWithContext(context.Background())
}

func (i HadoopJobArgs) ToHadoopJobPtrOutputWithContext(ctx context.Context) HadoopJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(HadoopJobOutput).ToHadoopJobPtrOutputWithContext(ctx)
}

// HadoopJobPtrInput is an input type that accepts HadoopJobArgs, HadoopJobPtr and HadoopJobPtrOutput values.
// You can construct a concrete instance of `HadoopJobPtrInput` via:
//
//	        HadoopJobArgs{...}
//
//	or:
//
//	        nil
type HadoopJobPtrInput interface {
	pulumi.Input

	ToHadoopJobPtrOutput() HadoopJobPtrOutput
	ToHadoopJobPtrOutputWithContext(context.Context) HadoopJobPtrOutput
}

type hadoopJobPtrType HadoopJobArgs

func HadoopJobPtr(v *HadoopJobArgs) HadoopJobPtrInput {
	return (*hadoopJobPtrType)(v)
}

func (*hadoopJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**HadoopJob)(nil)).Elem()
}

func (i *hadoopJobPtrType) ToHadoopJobPtrOutput() HadoopJobPtrOutput {
	return i.ToHadoopJobPtrOutputWithContext(context.Background())
}

func (i *hadoopJobPtrType) ToHadoopJobPtrOutputWithContext(ctx context.Context) HadoopJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(HadoopJobPtrOutput)
}

// A Dataproc job for running Apache Hadoop MapReduce (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) jobs on Apache Hadoop YARN (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
type HadoopJobOutput struct{ *pulumi.OutputState }

func (HadoopJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*HadoopJob)(nil)).Elem()
}

func (o HadoopJobOutput) ToHadoopJobOutput() HadoopJobOutput {
	return o
}

func (o HadoopJobOutput) ToHadoopJobOutputWithContext(ctx context.Context) HadoopJobOutput {
	return o
}

func (o HadoopJobOutput) ToHadoopJobPtrOutput() HadoopJobPtrOutput {
	return o.ToHadoopJobPtrOutputWithContext(context.Background())
}

func (o HadoopJobOutput) ToHadoopJobPtrOutputWithContext(ctx context.Context) HadoopJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v HadoopJob) *HadoopJob {
		return &v
	}).(HadoopJobPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
func (o HadoopJobOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJob) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o HadoopJobOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJob) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
func (o HadoopJobOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJob) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
func (o HadoopJobOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJob) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o HadoopJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v HadoopJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris.
func (o HadoopJobOutput) MainClass() pulumi.StringPtrOutput {
	return o.ApplyT(func(v HadoopJob) *string { return v.MainClass }).(pulumi.StringPtrOutput)
}

// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
func (o HadoopJobOutput) MainJarFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v HadoopJob) *string { return v.MainJarFileUri }).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
func (o HadoopJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v HadoopJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

type HadoopJobPtrOutput struct{ *pulumi.OutputState }

func (HadoopJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**HadoopJob)(nil)).Elem()
}

func (o HadoopJobPtrOutput) ToHadoopJobPtrOutput() HadoopJobPtrOutput {
	return o
}

func (o HadoopJobPtrOutput) ToHadoopJobPtrOutputWithContext(ctx context.Context) HadoopJobPtrOutput {
	return o
}

func (o HadoopJobPtrOutput) Elem() HadoopJobOutput {
	return o.ApplyT(func(v *HadoopJob) HadoopJob {
		if v != nil {
			return *v
		}
		var ret HadoopJob
		return ret
	}).(HadoopJobOutput)
}

// Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
func (o HadoopJobPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *HadoopJob) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o HadoopJobPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *HadoopJob) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
func (o HadoopJobPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *HadoopJob) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
func (o HadoopJobPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *HadoopJob) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o HadoopJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *HadoopJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris.
func (o HadoopJobPtrOutput) MainClass() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *HadoopJob) *string {
		if v == nil {
			return nil
		}
		return v.MainClass
	}).(pulumi.StringPtrOutput)
}

// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
func (o HadoopJobPtrOutput) MainJarFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *HadoopJob) *string {
		if v == nil {
			return nil
		}
		return v.MainJarFileUri
	}).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
func (o HadoopJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *HadoopJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Hadoop MapReduce (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) jobs on Apache Hadoop YARN (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
type HadoopJobResponse struct {
	// Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris.
	MainClass string `pulumi:"mainClass"`
	// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
	MainJarFileUri string `pulumi:"mainJarFileUri"`
	// Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
	Properties map[string]string `pulumi:"properties"`
}

// A Dataproc job for running Apache Hadoop MapReduce (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) jobs on Apache Hadoop YARN (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
type HadoopJobResponseOutput struct{ *pulumi.OutputState }

func (HadoopJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*HadoopJobResponse)(nil)).Elem()
}

func (o HadoopJobResponseOutput) ToHadoopJobResponseOutput() HadoopJobResponseOutput {
	return o
}

func (o HadoopJobResponseOutput) ToHadoopJobResponseOutputWithContext(ctx context.Context) HadoopJobResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
func (o HadoopJobResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJobResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o HadoopJobResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJobResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
func (o HadoopJobResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJobResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
func (o HadoopJobResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJobResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o HadoopJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v HadoopJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris.
func (o HadoopJobResponseOutput) MainClass() pulumi.StringOutput {
	return o.ApplyT(func(v HadoopJobResponse) string { return v.MainClass }).(pulumi.StringOutput)
}

// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
func (o HadoopJobResponseOutput) MainJarFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v HadoopJobResponse) string { return v.MainJarFileUri }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
func (o HadoopJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v HadoopJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Hive (https://hive.apache.org/) queries on YARN.
type HiveJob struct {
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure *bool `pulumi:"continueOnFailure"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains Hive queries.
	QueryFileUri *string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList *QueryList `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
	ScriptVariables map[string]string `pulumi:"scriptVariables"`
}

// HiveJobInput is an input type that accepts HiveJobArgs and HiveJobOutput values.
// You can construct a concrete instance of `HiveJobInput` via:
//
//	HiveJobArgs{...}
type HiveJobInput interface {
	pulumi.Input

	ToHiveJobOutput() HiveJobOutput
	ToHiveJobOutputWithContext(context.Context) HiveJobOutput
}

// A Dataproc job for running Apache Hive (https://hive.apache.org/) queries on YARN.
type HiveJobArgs struct {
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure pulumi.BoolPtrInput `pulumi:"continueOnFailure"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
	Properties pulumi.StringMapInput `pulumi:"properties"`
	// The HCFS URI of the script that contains Hive queries.
	QueryFileUri pulumi.StringPtrInput `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListPtrInput `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
	ScriptVariables pulumi.StringMapInput `pulumi:"scriptVariables"`
}

func (HiveJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*HiveJob)(nil)).Elem()
}

func (i HiveJobArgs) ToHiveJobOutput() HiveJobOutput {
	return i.ToHiveJobOutputWithContext(context.Background())
}

func (i HiveJobArgs) ToHiveJobOutputWithContext(ctx context.Context) HiveJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(HiveJobOutput)
}

func (i HiveJobArgs) ToHiveJobPtrOutput() HiveJobPtrOutput {
	return i.ToHiveJobPtrOutputWithContext(context.Background())
}

func (i HiveJobArgs) ToHiveJobPtrOutputWithContext(ctx context.Context) HiveJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(HiveJobOutput).ToHiveJobPtrOutputWithContext(ctx)
}

// HiveJobPtrInput is an input type that accepts HiveJobArgs, HiveJobPtr and HiveJobPtrOutput values.
// You can construct a concrete instance of `HiveJobPtrInput` via:
//
//	        HiveJobArgs{...}
//
//	or:
//
//	        nil
type HiveJobPtrInput interface {
	pulumi.Input

	ToHiveJobPtrOutput() HiveJobPtrOutput
	ToHiveJobPtrOutputWithContext(context.Context) HiveJobPtrOutput
}

type hiveJobPtrType HiveJobArgs

func HiveJobPtr(v *HiveJobArgs) HiveJobPtrInput {
	return (*hiveJobPtrType)(v)
}

func (*hiveJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**HiveJob)(nil)).Elem()
}

func (i *hiveJobPtrType) ToHiveJobPtrOutput() HiveJobPtrOutput {
	return i.ToHiveJobPtrOutputWithContext(context.Background())
}

func (i *hiveJobPtrType) ToHiveJobPtrOutputWithContext(ctx context.Context) HiveJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(HiveJobPtrOutput)
}

// A Dataproc job for running Apache Hive (https://hive.apache.org/) queries on YARN.
type HiveJobOutput struct{ *pulumi.OutputState }

func (HiveJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*HiveJob)(nil)).Elem()
}

func (o HiveJobOutput) ToHiveJobOutput() HiveJobOutput {
	return o
}

func (o HiveJobOutput) ToHiveJobOutputWithContext(ctx context.Context) HiveJobOutput {
	return o
}

func (o HiveJobOutput) ToHiveJobPtrOutput() HiveJobPtrOutput {
	return o.ToHiveJobPtrOutputWithContext(context.Background())
}

func (o HiveJobOutput) ToHiveJobPtrOutputWithContext(ctx context.Context) HiveJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v HiveJob) *HiveJob {
		return &v
	}).(HiveJobPtrOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o HiveJobOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v HiveJob) *bool { return v.ContinueOnFailure }).(pulumi.BoolPtrOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
func (o HiveJobOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HiveJob) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
func (o HiveJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v HiveJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains Hive queries.
func (o HiveJobOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v HiveJob) *string { return v.QueryFileUri }).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o HiveJobOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v HiveJob) *QueryList { return v.QueryList }).(QueryListPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
func (o HiveJobOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v HiveJob) map[string]string { return v.ScriptVariables }).(pulumi.StringMapOutput)
}

type HiveJobPtrOutput struct{ *pulumi.OutputState }

func (HiveJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**HiveJob)(nil)).Elem()
}

func (o HiveJobPtrOutput) ToHiveJobPtrOutput() HiveJobPtrOutput {
	return o
}

func (o HiveJobPtrOutput) ToHiveJobPtrOutputWithContext(ctx context.Context) HiveJobPtrOutput {
	return o
}

func (o HiveJobPtrOutput) Elem() HiveJobOutput {
	return o.ApplyT(func(v *HiveJob) HiveJob {
		if v != nil {
			return *v
		}
		var ret HiveJob
		return ret
	}).(HiveJobOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o HiveJobPtrOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *HiveJob) *bool {
		if v == nil {
			return nil
		}
		return v.ContinueOnFailure
	}).(pulumi.BoolPtrOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
func (o HiveJobPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *HiveJob) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
func (o HiveJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *HiveJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains Hive queries.
func (o HiveJobPtrOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *HiveJob) *string {
		if v == nil {
			return nil
		}
		return v.QueryFileUri
	}).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o HiveJobPtrOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v *HiveJob) *QueryList {
		if v == nil {
			return nil
		}
		return v.QueryList
	}).(QueryListPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
func (o HiveJobPtrOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v *HiveJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.ScriptVariables
	}).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Hive (https://hive.apache.org/) queries on YARN.
type HiveJobResponse struct {
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure bool `pulumi:"continueOnFailure"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains Hive queries.
	QueryFileUri string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListResponse `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
	ScriptVariables map[string]string `pulumi:"scriptVariables"`
}

// A Dataproc job for running Apache Hive (https://hive.apache.org/) queries on YARN.
type HiveJobResponseOutput struct{ *pulumi.OutputState }

func (HiveJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*HiveJobResponse)(nil)).Elem()
}

func (o HiveJobResponseOutput) ToHiveJobResponseOutput() HiveJobResponseOutput {
	return o
}

func (o HiveJobResponseOutput) ToHiveJobResponseOutputWithContext(ctx context.Context) HiveJobResponseOutput {
	return o
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o HiveJobResponseOutput) ContinueOnFailure() pulumi.BoolOutput {
	return o.ApplyT(func(v HiveJobResponse) bool { return v.ContinueOnFailure }).(pulumi.BoolOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
func (o HiveJobResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HiveJobResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
func (o HiveJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v HiveJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains Hive queries.
func (o HiveJobResponseOutput) QueryFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v HiveJobResponse) string { return v.QueryFileUri }).(pulumi.StringOutput)
}

// A list of queries.
func (o HiveJobResponseOutput) QueryList() QueryListResponseOutput {
	return o.ApplyT(func(v HiveJobResponse) QueryListResponse { return v.QueryList }).(QueryListResponseOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
func (o HiveJobResponseOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v HiveJobResponse) map[string]string { return v.ScriptVariables }).(pulumi.StringMapOutput)
}

// Identity related configuration, including service account based secure multi-tenancy user mappings.
type IdentityConfig struct {
	// Map of user to service account.
	UserServiceAccountMapping map[string]string `pulumi:"userServiceAccountMapping"`
}

// IdentityConfigInput is an input type that accepts IdentityConfigArgs and IdentityConfigOutput values.
// You can construct a concrete instance of `IdentityConfigInput` via:
//
//	IdentityConfigArgs{...}
type IdentityConfigInput interface {
	pulumi.Input

	ToIdentityConfigOutput() IdentityConfigOutput
	ToIdentityConfigOutputWithContext(context.Context) IdentityConfigOutput
}

// Identity related configuration, including service account based secure multi-tenancy user mappings.
type IdentityConfigArgs struct {
	// Map of user to service account.
	UserServiceAccountMapping pulumi.StringMapInput `pulumi:"userServiceAccountMapping"`
}

func (IdentityConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*IdentityConfig)(nil)).Elem()
}

func (i IdentityConfigArgs) ToIdentityConfigOutput() IdentityConfigOutput {
	return i.ToIdentityConfigOutputWithContext(context.Background())
}

func (i IdentityConfigArgs) ToIdentityConfigOutputWithContext(ctx context.Context) IdentityConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(IdentityConfigOutput)
}

func (i IdentityConfigArgs) ToIdentityConfigPtrOutput() IdentityConfigPtrOutput {
	return i.ToIdentityConfigPtrOutputWithContext(context.Background())
}

func (i IdentityConfigArgs) ToIdentityConfigPtrOutputWithContext(ctx context.Context) IdentityConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(IdentityConfigOutput).ToIdentityConfigPtrOutputWithContext(ctx)
}

// IdentityConfigPtrInput is an input type that accepts IdentityConfigArgs, IdentityConfigPtr and IdentityConfigPtrOutput values.
// You can construct a concrete instance of `IdentityConfigPtrInput` via:
//
//	        IdentityConfigArgs{...}
//
//	or:
//
//	        nil
type IdentityConfigPtrInput interface {
	pulumi.Input

	ToIdentityConfigPtrOutput() IdentityConfigPtrOutput
	ToIdentityConfigPtrOutputWithContext(context.Context) IdentityConfigPtrOutput
}

type identityConfigPtrType IdentityConfigArgs

func IdentityConfigPtr(v *IdentityConfigArgs) IdentityConfigPtrInput {
	return (*identityConfigPtrType)(v)
}

func (*identityConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**IdentityConfig)(nil)).Elem()
}

func (i *identityConfigPtrType) ToIdentityConfigPtrOutput() IdentityConfigPtrOutput {
	return i.ToIdentityConfigPtrOutputWithContext(context.Background())
}

func (i *identityConfigPtrType) ToIdentityConfigPtrOutputWithContext(ctx context.Context) IdentityConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(IdentityConfigPtrOutput)
}

// Identity related configuration, including service account based secure multi-tenancy user mappings.
type IdentityConfigOutput struct{ *pulumi.OutputState }

func (IdentityConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*IdentityConfig)(nil)).Elem()
}

func (o IdentityConfigOutput) ToIdentityConfigOutput() IdentityConfigOutput {
	return o
}

func (o IdentityConfigOutput) ToIdentityConfigOutputWithContext(ctx context.Context) IdentityConfigOutput {
	return o
}

func (o IdentityConfigOutput) ToIdentityConfigPtrOutput() IdentityConfigPtrOutput {
	return o.ToIdentityConfigPtrOutputWithContext(context.Background())
}

func (o IdentityConfigOutput) ToIdentityConfigPtrOutputWithContext(ctx context.Context) IdentityConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v IdentityConfig) *IdentityConfig {
		return &v
	}).(IdentityConfigPtrOutput)
}

// Map of user to service account.
func (o IdentityConfigOutput) UserServiceAccountMapping() pulumi.StringMapOutput {
	return o.ApplyT(func(v IdentityConfig) map[string]string { return v.UserServiceAccountMapping }).(pulumi.StringMapOutput)
}

type IdentityConfigPtrOutput struct{ *pulumi.OutputState }

func (IdentityConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**IdentityConfig)(nil)).Elem()
}

func (o IdentityConfigPtrOutput) ToIdentityConfigPtrOutput() IdentityConfigPtrOutput {
	return o
}

func (o IdentityConfigPtrOutput) ToIdentityConfigPtrOutputWithContext(ctx context.Context) IdentityConfigPtrOutput {
	return o
}

func (o IdentityConfigPtrOutput) Elem() IdentityConfigOutput {
	return o.ApplyT(func(v *IdentityConfig) IdentityConfig {
		if v != nil {
			return *v
		}
		var ret IdentityConfig
		return ret
	}).(IdentityConfigOutput)
}

// Map of user to service account.
func (o IdentityConfigPtrOutput) UserServiceAccountMapping() pulumi.StringMapOutput {
	return o.ApplyT(func(v *IdentityConfig) map[string]string {
		if v == nil {
			return nil
		}
		return v.UserServiceAccountMapping
	}).(pulumi.StringMapOutput)
}

// Identity related configuration, including service account based secure multi-tenancy user mappings.
type IdentityConfigResponse struct {
	// Map of user to service account.
	UserServiceAccountMapping map[string]string `pulumi:"userServiceAccountMapping"`
}

// Identity related configuration, including service account based secure multi-tenancy user mappings.
type IdentityConfigResponseOutput struct{ *pulumi.OutputState }

func (IdentityConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*IdentityConfigResponse)(nil)).Elem()
}

func (o IdentityConfigResponseOutput) ToIdentityConfigResponseOutput() IdentityConfigResponseOutput {
	return o
}

func (o IdentityConfigResponseOutput) ToIdentityConfigResponseOutputWithContext(ctx context.Context) IdentityConfigResponseOutput {
	return o
}

// Map of user to service account.
func (o IdentityConfigResponseOutput) UserServiceAccountMapping() pulumi.StringMapOutput {
	return o.ApplyT(func(v IdentityConfigResponse) map[string]string { return v.UserServiceAccountMapping }).(pulumi.StringMapOutput)
}

// Configuration for the size bounds of an instance group, including its proportional size to other groups.
type InstanceGroupAutoscalingPolicyConfig struct {
	// Maximum number of instances for this group. Required for primary workers. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set.Primary workers - Bounds: [min_instances, ). Secondary workers - Bounds: [min_instances, ). Default: 0.
	MaxInstances int `pulumi:"maxInstances"`
	// Optional. Minimum number of instances for this group.Primary workers - Bounds: 2, max_instances. Default: 2. Secondary workers - Bounds: 0, max_instances. Default: 0.
	MinInstances *int `pulumi:"minInstances"`
	// Optional. Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker.The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if max_instances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created.If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
	Weight *int `pulumi:"weight"`
}

// InstanceGroupAutoscalingPolicyConfigInput is an input type that accepts InstanceGroupAutoscalingPolicyConfigArgs and InstanceGroupAutoscalingPolicyConfigOutput values.
// You can construct a concrete instance of `InstanceGroupAutoscalingPolicyConfigInput` via:
//
//	InstanceGroupAutoscalingPolicyConfigArgs{...}
type InstanceGroupAutoscalingPolicyConfigInput interface {
	pulumi.Input

	ToInstanceGroupAutoscalingPolicyConfigOutput() InstanceGroupAutoscalingPolicyConfigOutput
	ToInstanceGroupAutoscalingPolicyConfigOutputWithContext(context.Context) InstanceGroupAutoscalingPolicyConfigOutput
}

// Configuration for the size bounds of an instance group, including its proportional size to other groups.
type InstanceGroupAutoscalingPolicyConfigArgs struct {
	// Maximum number of instances for this group. Required for primary workers. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set.Primary workers - Bounds: [min_instances, ). Secondary workers - Bounds: [min_instances, ). Default: 0.
	MaxInstances pulumi.IntInput `pulumi:"maxInstances"`
	// Optional. Minimum number of instances for this group.Primary workers - Bounds: 2, max_instances. Default: 2. Secondary workers - Bounds: 0, max_instances. Default: 0.
	MinInstances pulumi.IntPtrInput `pulumi:"minInstances"`
	// Optional. Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker.The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if max_instances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created.If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
	Weight pulumi.IntPtrInput `pulumi:"weight"`
}

func (InstanceGroupAutoscalingPolicyConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceGroupAutoscalingPolicyConfig)(nil)).Elem()
}

func (i InstanceGroupAutoscalingPolicyConfigArgs) ToInstanceGroupAutoscalingPolicyConfigOutput() InstanceGroupAutoscalingPolicyConfigOutput {
	return i.ToInstanceGroupAutoscalingPolicyConfigOutputWithContext(context.Background())
}

func (i InstanceGroupAutoscalingPolicyConfigArgs) ToInstanceGroupAutoscalingPolicyConfigOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InstanceGroupAutoscalingPolicyConfigOutput)
}

func (i InstanceGroupAutoscalingPolicyConfigArgs) ToInstanceGroupAutoscalingPolicyConfigPtrOutput() InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return i.ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(context.Background())
}

func (i InstanceGroupAutoscalingPolicyConfigArgs) ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InstanceGroupAutoscalingPolicyConfigOutput).ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(ctx)
}

// InstanceGroupAutoscalingPolicyConfigPtrInput is an input type that accepts InstanceGroupAutoscalingPolicyConfigArgs, InstanceGroupAutoscalingPolicyConfigPtr and InstanceGroupAutoscalingPolicyConfigPtrOutput values.
// You can construct a concrete instance of `InstanceGroupAutoscalingPolicyConfigPtrInput` via:
//
//	        InstanceGroupAutoscalingPolicyConfigArgs{...}
//
//	or:
//
//	        nil
type InstanceGroupAutoscalingPolicyConfigPtrInput interface {
	pulumi.Input

	ToInstanceGroupAutoscalingPolicyConfigPtrOutput() InstanceGroupAutoscalingPolicyConfigPtrOutput
	ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(context.Context) InstanceGroupAutoscalingPolicyConfigPtrOutput
}

type instanceGroupAutoscalingPolicyConfigPtrType InstanceGroupAutoscalingPolicyConfigArgs

func InstanceGroupAutoscalingPolicyConfigPtr(v *InstanceGroupAutoscalingPolicyConfigArgs) InstanceGroupAutoscalingPolicyConfigPtrInput {
	return (*instanceGroupAutoscalingPolicyConfigPtrType)(v)
}

func (*instanceGroupAutoscalingPolicyConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**InstanceGroupAutoscalingPolicyConfig)(nil)).Elem()
}

func (i *instanceGroupAutoscalingPolicyConfigPtrType) ToInstanceGroupAutoscalingPolicyConfigPtrOutput() InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return i.ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(context.Background())
}

func (i *instanceGroupAutoscalingPolicyConfigPtrType) ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InstanceGroupAutoscalingPolicyConfigPtrOutput)
}

// Configuration for the size bounds of an instance group, including its proportional size to other groups.
type InstanceGroupAutoscalingPolicyConfigOutput struct{ *pulumi.OutputState }

func (InstanceGroupAutoscalingPolicyConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceGroupAutoscalingPolicyConfig)(nil)).Elem()
}

func (o InstanceGroupAutoscalingPolicyConfigOutput) ToInstanceGroupAutoscalingPolicyConfigOutput() InstanceGroupAutoscalingPolicyConfigOutput {
	return o
}

func (o InstanceGroupAutoscalingPolicyConfigOutput) ToInstanceGroupAutoscalingPolicyConfigOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigOutput {
	return o
}

func (o InstanceGroupAutoscalingPolicyConfigOutput) ToInstanceGroupAutoscalingPolicyConfigPtrOutput() InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return o.ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(context.Background())
}

func (o InstanceGroupAutoscalingPolicyConfigOutput) ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v InstanceGroupAutoscalingPolicyConfig) *InstanceGroupAutoscalingPolicyConfig {
		return &v
	}).(InstanceGroupAutoscalingPolicyConfigPtrOutput)
}

// Maximum number of instances for this group. Required for primary workers. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set.Primary workers - Bounds: [min_instances, ). Secondary workers - Bounds: [min_instances, ). Default: 0.
func (o InstanceGroupAutoscalingPolicyConfigOutput) MaxInstances() pulumi.IntOutput {
	return o.ApplyT(func(v InstanceGroupAutoscalingPolicyConfig) int { return v.MaxInstances }).(pulumi.IntOutput)
}

// Optional. Minimum number of instances for this group.Primary workers - Bounds: 2, max_instances. Default: 2. Secondary workers - Bounds: 0, max_instances. Default: 0.
func (o InstanceGroupAutoscalingPolicyConfigOutput) MinInstances() pulumi.IntPtrOutput {
	return o.ApplyT(func(v InstanceGroupAutoscalingPolicyConfig) *int { return v.MinInstances }).(pulumi.IntPtrOutput)
}

// Optional. Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker.The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if max_instances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created.If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
func (o InstanceGroupAutoscalingPolicyConfigOutput) Weight() pulumi.IntPtrOutput {
	return o.ApplyT(func(v InstanceGroupAutoscalingPolicyConfig) *int { return v.Weight }).(pulumi.IntPtrOutput)
}

type InstanceGroupAutoscalingPolicyConfigPtrOutput struct{ *pulumi.OutputState }

func (InstanceGroupAutoscalingPolicyConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**InstanceGroupAutoscalingPolicyConfig)(nil)).Elem()
}

func (o InstanceGroupAutoscalingPolicyConfigPtrOutput) ToInstanceGroupAutoscalingPolicyConfigPtrOutput() InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return o
}

func (o InstanceGroupAutoscalingPolicyConfigPtrOutput) ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return o
}

func (o InstanceGroupAutoscalingPolicyConfigPtrOutput) Elem() InstanceGroupAutoscalingPolicyConfigOutput {
	return o.ApplyT(func(v *InstanceGroupAutoscalingPolicyConfig) InstanceGroupAutoscalingPolicyConfig {
		if v != nil {
			return *v
		}
		var ret InstanceGroupAutoscalingPolicyConfig
		return ret
	}).(InstanceGroupAutoscalingPolicyConfigOutput)
}

// Maximum number of instances for this group. Required for primary workers. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set.Primary workers - Bounds: [min_instances, ). Secondary workers - Bounds: [min_instances, ). Default: 0.
func (o InstanceGroupAutoscalingPolicyConfigPtrOutput) MaxInstances() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *InstanceGroupAutoscalingPolicyConfig) *int {
		if v == nil {
			return nil
		}
		return &v.MaxInstances
	}).(pulumi.IntPtrOutput)
}

// Optional. Minimum number of instances for this group.Primary workers - Bounds: 2, max_instances. Default: 2. Secondary workers - Bounds: 0, max_instances. Default: 0.
func (o InstanceGroupAutoscalingPolicyConfigPtrOutput) MinInstances() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *InstanceGroupAutoscalingPolicyConfig) *int {
		if v == nil {
			return nil
		}
		return v.MinInstances
	}).(pulumi.IntPtrOutput)
}

// Optional. Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker.The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if max_instances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created.If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
func (o InstanceGroupAutoscalingPolicyConfigPtrOutput) Weight() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *InstanceGroupAutoscalingPolicyConfig) *int {
		if v == nil {
			return nil
		}
		return v.Weight
	}).(pulumi.IntPtrOutput)
}

// Configuration for the size bounds of an instance group, including its proportional size to other groups.
type InstanceGroupAutoscalingPolicyConfigResponse struct {
	// Maximum number of instances for this group. Required for primary workers. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set.Primary workers - Bounds: [min_instances, ). Secondary workers - Bounds: [min_instances, ). Default: 0.
	MaxInstances int `pulumi:"maxInstances"`
	// Optional. Minimum number of instances for this group.Primary workers - Bounds: 2, max_instances. Default: 2. Secondary workers - Bounds: 0, max_instances. Default: 0.
	MinInstances int `pulumi:"minInstances"`
	// Optional. Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker.The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if max_instances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created.If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
	Weight int `pulumi:"weight"`
}

// Configuration for the size bounds of an instance group, including its proportional size to other groups.
type InstanceGroupAutoscalingPolicyConfigResponseOutput struct{ *pulumi.OutputState }

func (InstanceGroupAutoscalingPolicyConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceGroupAutoscalingPolicyConfigResponse)(nil)).Elem()
}

func (o InstanceGroupAutoscalingPolicyConfigResponseOutput) ToInstanceGroupAutoscalingPolicyConfigResponseOutput() InstanceGroupAutoscalingPolicyConfigResponseOutput {
	return o
}

func (o InstanceGroupAutoscalingPolicyConfigResponseOutput) ToInstanceGroupAutoscalingPolicyConfigResponseOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigResponseOutput {
	return o
}

// Maximum number of instances for this group. Required for primary workers. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set.Primary workers - Bounds: [min_instances, ). Secondary workers - Bounds: [min_instances, ). Default: 0.
func (o InstanceGroupAutoscalingPolicyConfigResponseOutput) MaxInstances() pulumi.IntOutput {
	return o.ApplyT(func(v InstanceGroupAutoscalingPolicyConfigResponse) int { return v.MaxInstances }).(pulumi.IntOutput)
}

// Optional. Minimum number of instances for this group.Primary workers - Bounds: 2, max_instances. Default: 2. Secondary workers - Bounds: 0, max_instances. Default: 0.
func (o InstanceGroupAutoscalingPolicyConfigResponseOutput) MinInstances() pulumi.IntOutput {
	return o.ApplyT(func(v InstanceGroupAutoscalingPolicyConfigResponse) int { return v.MinInstances }).(pulumi.IntOutput)
}

// Optional. Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker.The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if max_instances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created.If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
func (o InstanceGroupAutoscalingPolicyConfigResponseOutput) Weight() pulumi.IntOutput {
	return o.ApplyT(func(v InstanceGroupAutoscalingPolicyConfigResponse) int { return v.Weight }).(pulumi.IntOutput)
}

// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
type InstanceGroupConfig struct {
	// Optional. The Compute Engine accelerator configuration for these instances.
	Accelerators []AcceleratorConfig `pulumi:"accelerators"`
	// Optional. Disk option config settings.
	DiskConfig *DiskConfig `pulumi:"diskConfig"`
	// Optional. The Compute Engine image resource used for cluster instances.The URI can represent an image or image family.Image examples: https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/[image-id] projects/[project_id]/global/images/[image-id] image-idImage family examples. Dataproc will use the most recent image from the family: https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/family/[custom-image-family-name] projects/[project_id]/global/images/family/[custom-image-family-name]If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
	ImageUri *string `pulumi:"imageUri"`
	// Optional. The Compute Engine machine type used for cluster instances.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2 projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2 n1-standard-2Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2.
	MachineTypeUri *string `pulumi:"machineTypeUri"`
	// Optional. Specifies the minimum cpu platform for the Instance Group. See Dataproc -> Minimum CPU Platform (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
	MinCpuPlatform *string `pulumi:"minCpuPlatform"`
	// Optional. The number of VM instances in the instance group. For HA cluster master_config groups, must be set to 3. For standard cluster master_config groups, must be set to 1.
	NumInstances *int `pulumi:"numInstances"`
	// Optional. Specifies the preemptibility of the instance group.The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed.The default value for secondary instances is PREEMPTIBLE.
	Preemptibility *InstanceGroupConfigPreemptibility `pulumi:"preemptibility"`
}

// InstanceGroupConfigInput is an input type that accepts InstanceGroupConfigArgs and InstanceGroupConfigOutput values.
// You can construct a concrete instance of `InstanceGroupConfigInput` via:
//
//	InstanceGroupConfigArgs{...}
type InstanceGroupConfigInput interface {
	pulumi.Input

	ToInstanceGroupConfigOutput() InstanceGroupConfigOutput
	ToInstanceGroupConfigOutputWithContext(context.Context) InstanceGroupConfigOutput
}

// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
type InstanceGroupConfigArgs struct {
	// Optional. The Compute Engine accelerator configuration for these instances.
	Accelerators AcceleratorConfigArrayInput `pulumi:"accelerators"`
	// Optional. Disk option config settings.
	DiskConfig DiskConfigPtrInput `pulumi:"diskConfig"`
	// Optional. The Compute Engine image resource used for cluster instances.The URI can represent an image or image family.Image examples: https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/[image-id] projects/[project_id]/global/images/[image-id] image-idImage family examples. Dataproc will use the most recent image from the family: https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/family/[custom-image-family-name] projects/[project_id]/global/images/family/[custom-image-family-name]If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
	ImageUri pulumi.StringPtrInput `pulumi:"imageUri"`
	// Optional. The Compute Engine machine type used for cluster instances.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2 projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2 n1-standard-2Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2.
	MachineTypeUri pulumi.StringPtrInput `pulumi:"machineTypeUri"`
	// Optional. Specifies the minimum cpu platform for the Instance Group. See Dataproc -> Minimum CPU Platform (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
	MinCpuPlatform pulumi.StringPtrInput `pulumi:"minCpuPlatform"`
	// Optional. The number of VM instances in the instance group. For HA cluster master_config groups, must be set to 3. For standard cluster master_config groups, must be set to 1.
	NumInstances pulumi.IntPtrInput `pulumi:"numInstances"`
	// Optional. Specifies the preemptibility of the instance group.The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed.The default value for secondary instances is PREEMPTIBLE.
	Preemptibility InstanceGroupConfigPreemptibilityPtrInput `pulumi:"preemptibility"`
}

func (InstanceGroupConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceGroupConfig)(nil)).Elem()
}

func (i InstanceGroupConfigArgs) ToInstanceGroupConfigOutput() InstanceGroupConfigOutput {
	return i.ToInstanceGroupConfigOutputWithContext(context.Background())
}

func (i InstanceGroupConfigArgs) ToInstanceGroupConfigOutputWithContext(ctx context.Context) InstanceGroupConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InstanceGroupConfigOutput)
}

func (i InstanceGroupConfigArgs) ToInstanceGroupConfigPtrOutput() InstanceGroupConfigPtrOutput {
	return i.ToInstanceGroupConfigPtrOutputWithContext(context.Background())
}

func (i InstanceGroupConfigArgs) ToInstanceGroupConfigPtrOutputWithContext(ctx context.Context) InstanceGroupConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InstanceGroupConfigOutput).ToInstanceGroupConfigPtrOutputWithContext(ctx)
}

// InstanceGroupConfigPtrInput is an input type that accepts InstanceGroupConfigArgs, InstanceGroupConfigPtr and InstanceGroupConfigPtrOutput values.
// You can construct a concrete instance of `InstanceGroupConfigPtrInput` via:
//
//	        InstanceGroupConfigArgs{...}
//
//	or:
//
//	        nil
type InstanceGroupConfigPtrInput interface {
	pulumi.Input

	ToInstanceGroupConfigPtrOutput() InstanceGroupConfigPtrOutput
	ToInstanceGroupConfigPtrOutputWithContext(context.Context) InstanceGroupConfigPtrOutput
}

type instanceGroupConfigPtrType InstanceGroupConfigArgs

func InstanceGroupConfigPtr(v *InstanceGroupConfigArgs) InstanceGroupConfigPtrInput {
	return (*instanceGroupConfigPtrType)(v)
}

func (*instanceGroupConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**InstanceGroupConfig)(nil)).Elem()
}

func (i *instanceGroupConfigPtrType) ToInstanceGroupConfigPtrOutput() InstanceGroupConfigPtrOutput {
	return i.ToInstanceGroupConfigPtrOutputWithContext(context.Background())
}

func (i *instanceGroupConfigPtrType) ToInstanceGroupConfigPtrOutputWithContext(ctx context.Context) InstanceGroupConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InstanceGroupConfigPtrOutput)
}

// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
type InstanceGroupConfigOutput struct{ *pulumi.OutputState }

func (InstanceGroupConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceGroupConfig)(nil)).Elem()
}

func (o InstanceGroupConfigOutput) ToInstanceGroupConfigOutput() InstanceGroupConfigOutput {
	return o
}

func (o InstanceGroupConfigOutput) ToInstanceGroupConfigOutputWithContext(ctx context.Context) InstanceGroupConfigOutput {
	return o
}

func (o InstanceGroupConfigOutput) ToInstanceGroupConfigPtrOutput() InstanceGroupConfigPtrOutput {
	return o.ToInstanceGroupConfigPtrOutputWithContext(context.Background())
}

func (o InstanceGroupConfigOutput) ToInstanceGroupConfigPtrOutputWithContext(ctx context.Context) InstanceGroupConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v InstanceGroupConfig) *InstanceGroupConfig {
		return &v
	}).(InstanceGroupConfigPtrOutput)
}

// Optional. The Compute Engine accelerator configuration for these instances.
func (o InstanceGroupConfigOutput) Accelerators() AcceleratorConfigArrayOutput {
	return o.ApplyT(func(v InstanceGroupConfig) []AcceleratorConfig { return v.Accelerators }).(AcceleratorConfigArrayOutput)
}

// Optional. Disk option config settings.
func (o InstanceGroupConfigOutput) DiskConfig() DiskConfigPtrOutput {
	return o.ApplyT(func(v InstanceGroupConfig) *DiskConfig { return v.DiskConfig }).(DiskConfigPtrOutput)
}

// Optional. The Compute Engine image resource used for cluster instances.The URI can represent an image or image family.Image examples: https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/[image-id] projects/[project_id]/global/images/[image-id] image-idImage family examples. Dataproc will use the most recent image from the family: https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/family/[custom-image-family-name] projects/[project_id]/global/images/family/[custom-image-family-name]If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
func (o InstanceGroupConfigOutput) ImageUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v InstanceGroupConfig) *string { return v.ImageUri }).(pulumi.StringPtrOutput)
}

// Optional. The Compute Engine machine type used for cluster instances.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2 projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2 n1-standard-2Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2.
func (o InstanceGroupConfigOutput) MachineTypeUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v InstanceGroupConfig) *string { return v.MachineTypeUri }).(pulumi.StringPtrOutput)
}

// Optional. Specifies the minimum cpu platform for the Instance Group. See Dataproc -> Minimum CPU Platform (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
func (o InstanceGroupConfigOutput) MinCpuPlatform() pulumi.StringPtrOutput {
	return o.ApplyT(func(v InstanceGroupConfig) *string { return v.MinCpuPlatform }).(pulumi.StringPtrOutput)
}

// Optional. The number of VM instances in the instance group. For HA cluster master_config groups, must be set to 3. For standard cluster master_config groups, must be set to 1.
func (o InstanceGroupConfigOutput) NumInstances() pulumi.IntPtrOutput {
	return o.ApplyT(func(v InstanceGroupConfig) *int { return v.NumInstances }).(pulumi.IntPtrOutput)
}

// Optional. Specifies the preemptibility of the instance group.The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed.The default value for secondary instances is PREEMPTIBLE.
func (o InstanceGroupConfigOutput) Preemptibility() InstanceGroupConfigPreemptibilityPtrOutput {
	return o.ApplyT(func(v InstanceGroupConfig) *InstanceGroupConfigPreemptibility { return v.Preemptibility }).(InstanceGroupConfigPreemptibilityPtrOutput)
}

type InstanceGroupConfigPtrOutput struct{ *pulumi.OutputState }

func (InstanceGroupConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**InstanceGroupConfig)(nil)).Elem()
}

func (o InstanceGroupConfigPtrOutput) ToInstanceGroupConfigPtrOutput() InstanceGroupConfigPtrOutput {
	return o
}

func (o InstanceGroupConfigPtrOutput) ToInstanceGroupConfigPtrOutputWithContext(ctx context.Context) InstanceGroupConfigPtrOutput {
	return o
}

func (o InstanceGroupConfigPtrOutput) Elem() InstanceGroupConfigOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) InstanceGroupConfig {
		if v != nil {
			return *v
		}
		var ret InstanceGroupConfig
		return ret
	}).(InstanceGroupConfigOutput)
}

// Optional. The Compute Engine accelerator configuration for these instances.
func (o InstanceGroupConfigPtrOutput) Accelerators() AcceleratorConfigArrayOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) []AcceleratorConfig {
		if v == nil {
			return nil
		}
		return v.Accelerators
	}).(AcceleratorConfigArrayOutput)
}

// Optional. Disk option config settings.
func (o InstanceGroupConfigPtrOutput) DiskConfig() DiskConfigPtrOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) *DiskConfig {
		if v == nil {
			return nil
		}
		return v.DiskConfig
	}).(DiskConfigPtrOutput)
}

// Optional. The Compute Engine image resource used for cluster instances.The URI can represent an image or image family.Image examples: https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/[image-id] projects/[project_id]/global/images/[image-id] image-idImage family examples. Dataproc will use the most recent image from the family: https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/family/[custom-image-family-name] projects/[project_id]/global/images/family/[custom-image-family-name]If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
func (o InstanceGroupConfigPtrOutput) ImageUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) *string {
		if v == nil {
			return nil
		}
		return v.ImageUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The Compute Engine machine type used for cluster instances.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2 projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2 n1-standard-2Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2.
func (o InstanceGroupConfigPtrOutput) MachineTypeUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) *string {
		if v == nil {
			return nil
		}
		return v.MachineTypeUri
	}).(pulumi.StringPtrOutput)
}

// Optional. Specifies the minimum cpu platform for the Instance Group. See Dataproc -> Minimum CPU Platform (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
func (o InstanceGroupConfigPtrOutput) MinCpuPlatform() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) *string {
		if v == nil {
			return nil
		}
		return v.MinCpuPlatform
	}).(pulumi.StringPtrOutput)
}

// Optional. The number of VM instances in the instance group. For HA cluster master_config groups, must be set to 3. For standard cluster master_config groups, must be set to 1.
func (o InstanceGroupConfigPtrOutput) NumInstances() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) *int {
		if v == nil {
			return nil
		}
		return v.NumInstances
	}).(pulumi.IntPtrOutput)
}

// Optional. Specifies the preemptibility of the instance group.The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed.The default value for secondary instances is PREEMPTIBLE.
func (o InstanceGroupConfigPtrOutput) Preemptibility() InstanceGroupConfigPreemptibilityPtrOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) *InstanceGroupConfigPreemptibility {
		if v == nil {
			return nil
		}
		return v.Preemptibility
	}).(InstanceGroupConfigPreemptibilityPtrOutput)
}

// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
type InstanceGroupConfigResponse struct {
	// Optional. The Compute Engine accelerator configuration for these instances.
	Accelerators []AcceleratorConfigResponse `pulumi:"accelerators"`
	// Optional. Disk option config settings.
	DiskConfig DiskConfigResponse `pulumi:"diskConfig"`
	// Optional. The Compute Engine image resource used for cluster instances.The URI can represent an image or image family.Image examples: https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/[image-id] projects/[project_id]/global/images/[image-id] image-idImage family examples. Dataproc will use the most recent image from the family: https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/family/[custom-image-family-name] projects/[project_id]/global/images/family/[custom-image-family-name]If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
	ImageUri string `pulumi:"imageUri"`
	// The list of instance names. Dataproc derives the names from cluster_name, num_instances, and the instance group.
	InstanceNames []string `pulumi:"instanceNames"`
	// List of references to Compute Engine instances.
	InstanceReferences []InstanceReferenceResponse `pulumi:"instanceReferences"`
	// Specifies that this instance group contains preemptible instances.
	IsPreemptible bool `pulumi:"isPreemptible"`
	// Optional. The Compute Engine machine type used for cluster instances.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2 projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2 n1-standard-2Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2.
	MachineTypeUri string `pulumi:"machineTypeUri"`
	// The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
	ManagedGroupConfig ManagedGroupConfigResponse `pulumi:"managedGroupConfig"`
	// Optional. Specifies the minimum cpu platform for the Instance Group. See Dataproc -> Minimum CPU Platform (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
	MinCpuPlatform string `pulumi:"minCpuPlatform"`
	// Optional. The number of VM instances in the instance group. For HA cluster master_config groups, must be set to 3. For standard cluster master_config groups, must be set to 1.
	NumInstances int `pulumi:"numInstances"`
	// Optional. Specifies the preemptibility of the instance group.The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed.The default value for secondary instances is PREEMPTIBLE.
	Preemptibility string `pulumi:"preemptibility"`
}

// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
type InstanceGroupConfigResponseOutput struct{ *pulumi.OutputState }

func (InstanceGroupConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceGroupConfigResponse)(nil)).Elem()
}

func (o InstanceGroupConfigResponseOutput) ToInstanceGroupConfigResponseOutput() InstanceGroupConfigResponseOutput {
	return o
}

func (o InstanceGroupConfigResponseOutput) ToInstanceGroupConfigResponseOutputWithContext(ctx context.Context) InstanceGroupConfigResponseOutput {
	return o
}

// Optional. The Compute Engine accelerator configuration for these instances.
func (o InstanceGroupConfigResponseOutput) Accelerators() AcceleratorConfigResponseArrayOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) []AcceleratorConfigResponse { return v.Accelerators }).(AcceleratorConfigResponseArrayOutput)
}

// Optional. Disk option config settings.
func (o InstanceGroupConfigResponseOutput) DiskConfig() DiskConfigResponseOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) DiskConfigResponse { return v.DiskConfig }).(DiskConfigResponseOutput)
}

// Optional. The Compute Engine image resource used for cluster instances.The URI can represent an image or image family.Image examples: https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/[image-id] projects/[project_id]/global/images/[image-id] image-idImage family examples. Dataproc will use the most recent image from the family: https://www.googleapis.com/compute/v1/projects/[project_id]/global/images/family/[custom-image-family-name] projects/[project_id]/global/images/family/[custom-image-family-name]If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
func (o InstanceGroupConfigResponseOutput) ImageUri() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) string { return v.ImageUri }).(pulumi.StringOutput)
}

// The list of instance names. Dataproc derives the names from cluster_name, num_instances, and the instance group.
func (o InstanceGroupConfigResponseOutput) InstanceNames() pulumi.StringArrayOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) []string { return v.InstanceNames }).(pulumi.StringArrayOutput)
}

// List of references to Compute Engine instances.
func (o InstanceGroupConfigResponseOutput) InstanceReferences() InstanceReferenceResponseArrayOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) []InstanceReferenceResponse { return v.InstanceReferences }).(InstanceReferenceResponseArrayOutput)
}

// Specifies that this instance group contains preemptible instances.
func (o InstanceGroupConfigResponseOutput) IsPreemptible() pulumi.BoolOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) bool { return v.IsPreemptible }).(pulumi.BoolOutput)
}

// Optional. The Compute Engine machine type used for cluster instances.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2 projects/[project_id]/zones/[zone]/machineTypes/n1-standard-2 n1-standard-2Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2.
func (o InstanceGroupConfigResponseOutput) MachineTypeUri() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) string { return v.MachineTypeUri }).(pulumi.StringOutput)
}

// The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
func (o InstanceGroupConfigResponseOutput) ManagedGroupConfig() ManagedGroupConfigResponseOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) ManagedGroupConfigResponse { return v.ManagedGroupConfig }).(ManagedGroupConfigResponseOutput)
}

// Optional. Specifies the minimum cpu platform for the Instance Group. See Dataproc -> Minimum CPU Platform (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
func (o InstanceGroupConfigResponseOutput) MinCpuPlatform() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) string { return v.MinCpuPlatform }).(pulumi.StringOutput)
}

// Optional. The number of VM instances in the instance group. For HA cluster master_config groups, must be set to 3. For standard cluster master_config groups, must be set to 1.
func (o InstanceGroupConfigResponseOutput) NumInstances() pulumi.IntOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) int { return v.NumInstances }).(pulumi.IntOutput)
}

// Optional. Specifies the preemptibility of the instance group.The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed.The default value for secondary instances is PREEMPTIBLE.
func (o InstanceGroupConfigResponseOutput) Preemptibility() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) string { return v.Preemptibility }).(pulumi.StringOutput)
}

// A reference to a Compute Engine instance.
type InstanceReferenceResponse struct {
	// The unique identifier of the Compute Engine instance.
	InstanceId string `pulumi:"instanceId"`
	// The user-friendly name of the Compute Engine instance.
	InstanceName string `pulumi:"instanceName"`
	// The public ECIES key used for sharing data with this instance.
	PublicEciesKey string `pulumi:"publicEciesKey"`
	// The public RSA key used for sharing data with this instance.
	PublicKey string `pulumi:"publicKey"`
}

// A reference to a Compute Engine instance.
type InstanceReferenceResponseOutput struct{ *pulumi.OutputState }

func (InstanceReferenceResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceReferenceResponse)(nil)).Elem()
}

func (o InstanceReferenceResponseOutput) ToInstanceReferenceResponseOutput() InstanceReferenceResponseOutput {
	return o
}

func (o InstanceReferenceResponseOutput) ToInstanceReferenceResponseOutputWithContext(ctx context.Context) InstanceReferenceResponseOutput {
	return o
}

// The unique identifier of the Compute Engine instance.
func (o InstanceReferenceResponseOutput) InstanceId() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceReferenceResponse) string { return v.InstanceId }).(pulumi.StringOutput)
}

// The user-friendly name of the Compute Engine instance.
func (o InstanceReferenceResponseOutput) InstanceName() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceReferenceResponse) string { return v.InstanceName }).(pulumi.StringOutput)
}

// The public ECIES key used for sharing data with this instance.
func (o InstanceReferenceResponseOutput) PublicEciesKey() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceReferenceResponse) string { return v.PublicEciesKey }).(pulumi.StringOutput)
}

// The public RSA key used for sharing data with this instance.
func (o InstanceReferenceResponseOutput) PublicKey() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceReferenceResponse) string { return v.PublicKey }).(pulumi.StringOutput)
}

type InstanceReferenceResponseArrayOutput struct{ *pulumi.OutputState }

func (InstanceReferenceResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]InstanceReferenceResponse)(nil)).Elem()
}

func (o InstanceReferenceResponseArrayOutput) ToInstanceReferenceResponseArrayOutput() InstanceReferenceResponseArrayOutput {
	return o
}

func (o InstanceReferenceResponseArrayOutput) ToInstanceReferenceResponseArrayOutputWithContext(ctx context.Context) InstanceReferenceResponseArrayOutput {
	return o
}

func (o InstanceReferenceResponseArrayOutput) Index(i pulumi.IntInput) InstanceReferenceResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) InstanceReferenceResponse {
		return vs[0].([]InstanceReferenceResponse)[vs[1].(int)]
	}).(InstanceReferenceResponseOutput)
}

// Dataproc job config.
type JobPlacement struct {
	// Optional. Cluster labels to identify a cluster where the job will be submitted.
	ClusterLabels map[string]string `pulumi:"clusterLabels"`
	// The name of the cluster where the job will be submitted.
	ClusterName string `pulumi:"clusterName"`
}

// JobPlacementInput is an input type that accepts JobPlacementArgs and JobPlacementOutput values.
// You can construct a concrete instance of `JobPlacementInput` via:
//
//	JobPlacementArgs{...}
type JobPlacementInput interface {
	pulumi.Input

	ToJobPlacementOutput() JobPlacementOutput
	ToJobPlacementOutputWithContext(context.Context) JobPlacementOutput
}

// Dataproc job config.
type JobPlacementArgs struct {
	// Optional. Cluster labels to identify a cluster where the job will be submitted.
	ClusterLabels pulumi.StringMapInput `pulumi:"clusterLabels"`
	// The name of the cluster where the job will be submitted.
	ClusterName pulumi.StringInput `pulumi:"clusterName"`
}

func (JobPlacementArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*JobPlacement)(nil)).Elem()
}

func (i JobPlacementArgs) ToJobPlacementOutput() JobPlacementOutput {
	return i.ToJobPlacementOutputWithContext(context.Background())
}

func (i JobPlacementArgs) ToJobPlacementOutputWithContext(ctx context.Context) JobPlacementOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobPlacementOutput)
}

// Dataproc job config.
type JobPlacementOutput struct{ *pulumi.OutputState }

func (JobPlacementOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobPlacement)(nil)).Elem()
}

func (o JobPlacementOutput) ToJobPlacementOutput() JobPlacementOutput {
	return o
}

func (o JobPlacementOutput) ToJobPlacementOutputWithContext(ctx context.Context) JobPlacementOutput {
	return o
}

// Optional. Cluster labels to identify a cluster where the job will be submitted.
func (o JobPlacementOutput) ClusterLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v JobPlacement) map[string]string { return v.ClusterLabels }).(pulumi.StringMapOutput)
}

// The name of the cluster where the job will be submitted.
func (o JobPlacementOutput) ClusterName() pulumi.StringOutput {
	return o.ApplyT(func(v JobPlacement) string { return v.ClusterName }).(pulumi.StringOutput)
}

// Dataproc job config.
type JobPlacementResponse struct {
	// Optional. Cluster labels to identify a cluster where the job will be submitted.
	ClusterLabels map[string]string `pulumi:"clusterLabels"`
	// The name of the cluster where the job will be submitted.
	ClusterName string `pulumi:"clusterName"`
	// A cluster UUID generated by the Dataproc service when the job is submitted.
	ClusterUuid string `pulumi:"clusterUuid"`
}

// Dataproc job config.
type JobPlacementResponseOutput struct{ *pulumi.OutputState }

func (JobPlacementResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobPlacementResponse)(nil)).Elem()
}

func (o JobPlacementResponseOutput) ToJobPlacementResponseOutput() JobPlacementResponseOutput {
	return o
}

func (o JobPlacementResponseOutput) ToJobPlacementResponseOutputWithContext(ctx context.Context) JobPlacementResponseOutput {
	return o
}

// Optional. Cluster labels to identify a cluster where the job will be submitted.
func (o JobPlacementResponseOutput) ClusterLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v JobPlacementResponse) map[string]string { return v.ClusterLabels }).(pulumi.StringMapOutput)
}

// The name of the cluster where the job will be submitted.
func (o JobPlacementResponseOutput) ClusterName() pulumi.StringOutput {
	return o.ApplyT(func(v JobPlacementResponse) string { return v.ClusterName }).(pulumi.StringOutput)
}

// A cluster UUID generated by the Dataproc service when the job is submitted.
func (o JobPlacementResponseOutput) ClusterUuid() pulumi.StringOutput {
	return o.ApplyT(func(v JobPlacementResponse) string { return v.ClusterUuid }).(pulumi.StringOutput)
}

// Encapsulates the full scoping used to reference a job.
type JobReference struct {
	// Optional. The job ID, which must be unique within the project.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or hyphens (-). The maximum length is 100 characters.If not specified by the caller, the job ID will be provided by the server.
	JobId *string `pulumi:"jobId"`
	// Optional. The ID of the Google Cloud Platform project that the job belongs to. If specified, must match the request project ID.
	Project *string `pulumi:"project"`
}

// JobReferenceInput is an input type that accepts JobReferenceArgs and JobReferenceOutput values.
// You can construct a concrete instance of `JobReferenceInput` via:
//
//	JobReferenceArgs{...}
type JobReferenceInput interface {
	pulumi.Input

	ToJobReferenceOutput() JobReferenceOutput
	ToJobReferenceOutputWithContext(context.Context) JobReferenceOutput
}

// Encapsulates the full scoping used to reference a job.
type JobReferenceArgs struct {
	// Optional. The job ID, which must be unique within the project.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or hyphens (-). The maximum length is 100 characters.If not specified by the caller, the job ID will be provided by the server.
	JobId pulumi.StringPtrInput `pulumi:"jobId"`
	// Optional. The ID of the Google Cloud Platform project that the job belongs to. If specified, must match the request project ID.
	Project pulumi.StringPtrInput `pulumi:"project"`
}

func (JobReferenceArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*JobReference)(nil)).Elem()
}

func (i JobReferenceArgs) ToJobReferenceOutput() JobReferenceOutput {
	return i.ToJobReferenceOutputWithContext(context.Background())
}

func (i JobReferenceArgs) ToJobReferenceOutputWithContext(ctx context.Context) JobReferenceOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobReferenceOutput)
}

func (i JobReferenceArgs) ToJobReferencePtrOutput() JobReferencePtrOutput {
	return i.ToJobReferencePtrOutputWithContext(context.Background())
}

func (i JobReferenceArgs) ToJobReferencePtrOutputWithContext(ctx context.Context) JobReferencePtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobReferenceOutput).ToJobReferencePtrOutputWithContext(ctx)
}

// JobReferencePtrInput is an input type that accepts JobReferenceArgs, JobReferencePtr and JobReferencePtrOutput values.
// You can construct a concrete instance of `JobReferencePtrInput` via:
//
//	        JobReferenceArgs{...}
//
//	or:
//
//	        nil
type JobReferencePtrInput interface {
	pulumi.Input

	ToJobReferencePtrOutput() JobReferencePtrOutput
	ToJobReferencePtrOutputWithContext(context.Context) JobReferencePtrOutput
}

type jobReferencePtrType JobReferenceArgs

func JobReferencePtr(v *JobReferenceArgs) JobReferencePtrInput {
	return (*jobReferencePtrType)(v)
}

func (*jobReferencePtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**JobReference)(nil)).Elem()
}

func (i *jobReferencePtrType) ToJobReferencePtrOutput() JobReferencePtrOutput {
	return i.ToJobReferencePtrOutputWithContext(context.Background())
}

func (i *jobReferencePtrType) ToJobReferencePtrOutputWithContext(ctx context.Context) JobReferencePtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobReferencePtrOutput)
}

// Encapsulates the full scoping used to reference a job.
type JobReferenceOutput struct{ *pulumi.OutputState }

func (JobReferenceOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobReference)(nil)).Elem()
}

func (o JobReferenceOutput) ToJobReferenceOutput() JobReferenceOutput {
	return o
}

func (o JobReferenceOutput) ToJobReferenceOutputWithContext(ctx context.Context) JobReferenceOutput {
	return o
}

func (o JobReferenceOutput) ToJobReferencePtrOutput() JobReferencePtrOutput {
	return o.ToJobReferencePtrOutputWithContext(context.Background())
}

func (o JobReferenceOutput) ToJobReferencePtrOutputWithContext(ctx context.Context) JobReferencePtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v JobReference) *JobReference {
		return &v
	}).(JobReferencePtrOutput)
}

// Optional. The job ID, which must be unique within the project.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or hyphens (-). The maximum length is 100 characters.If not specified by the caller, the job ID will be provided by the server.
func (o JobReferenceOutput) JobId() pulumi.StringPtrOutput {
	return o.ApplyT(func(v JobReference) *string { return v.JobId }).(pulumi.StringPtrOutput)
}

// Optional. The ID of the Google Cloud Platform project that the job belongs to. If specified, must match the request project ID.
func (o JobReferenceOutput) Project() pulumi.StringPtrOutput {
	return o.ApplyT(func(v JobReference) *string { return v.Project }).(pulumi.StringPtrOutput)
}

type JobReferencePtrOutput struct{ *pulumi.OutputState }

func (JobReferencePtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**JobReference)(nil)).Elem()
}

func (o JobReferencePtrOutput) ToJobReferencePtrOutput() JobReferencePtrOutput {
	return o
}

func (o JobReferencePtrOutput) ToJobReferencePtrOutputWithContext(ctx context.Context) JobReferencePtrOutput {
	return o
}

func (o JobReferencePtrOutput) Elem() JobReferenceOutput {
	return o.ApplyT(func(v *JobReference) JobReference {
		if v != nil {
			return *v
		}
		var ret JobReference
		return ret
	}).(JobReferenceOutput)
}

// Optional. The job ID, which must be unique within the project.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or hyphens (-). The maximum length is 100 characters.If not specified by the caller, the job ID will be provided by the server.
func (o JobReferencePtrOutput) JobId() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *JobReference) *string {
		if v == nil {
			return nil
		}
		return v.JobId
	}).(pulumi.StringPtrOutput)
}

// Optional. The ID of the Google Cloud Platform project that the job belongs to. If specified, must match the request project ID.
func (o JobReferencePtrOutput) Project() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *JobReference) *string {
		if v == nil {
			return nil
		}
		return v.Project
	}).(pulumi.StringPtrOutput)
}

// Encapsulates the full scoping used to reference a job.
type JobReferenceResponse struct {
	// Optional. The job ID, which must be unique within the project.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or hyphens (-). The maximum length is 100 characters.If not specified by the caller, the job ID will be provided by the server.
	JobId string `pulumi:"jobId"`
	// Optional. The ID of the Google Cloud Platform project that the job belongs to. If specified, must match the request project ID.
	Project string `pulumi:"project"`
}

// Encapsulates the full scoping used to reference a job.
type JobReferenceResponseOutput struct{ *pulumi.OutputState }

func (JobReferenceResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobReferenceResponse)(nil)).Elem()
}

func (o JobReferenceResponseOutput) ToJobReferenceResponseOutput() JobReferenceResponseOutput {
	return o
}

func (o JobReferenceResponseOutput) ToJobReferenceResponseOutputWithContext(ctx context.Context) JobReferenceResponseOutput {
	return o
}

// Optional. The job ID, which must be unique within the project.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or hyphens (-). The maximum length is 100 characters.If not specified by the caller, the job ID will be provided by the server.
func (o JobReferenceResponseOutput) JobId() pulumi.StringOutput {
	return o.ApplyT(func(v JobReferenceResponse) string { return v.JobId }).(pulumi.StringOutput)
}

// Optional. The ID of the Google Cloud Platform project that the job belongs to. If specified, must match the request project ID.
func (o JobReferenceResponseOutput) Project() pulumi.StringOutput {
	return o.ApplyT(func(v JobReferenceResponse) string { return v.Project }).(pulumi.StringOutput)
}

// Job scheduling options.
type JobScheduling struct {
	// Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.A job may be reported as thrashing if the driver exits with a non-zero code four times within a 10-minute window.Maximum value is 10.Note: This restartable job option is not supported in Dataproc workflow templates (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
	MaxFailuresPerHour *int `pulumi:"maxFailuresPerHour"`
	// Optional. Maximum total number of times a driver may be restarted as a result of the driver exiting with a non-zero code. After the maximum number is reached, the job will be reported as failed.Maximum value is 240.Note: Currently, this restartable job option is not supported in Dataproc workflow templates (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
	MaxFailuresTotal *int `pulumi:"maxFailuresTotal"`
}

// JobSchedulingInput is an input type that accepts JobSchedulingArgs and JobSchedulingOutput values.
// You can construct a concrete instance of `JobSchedulingInput` via:
//
//	JobSchedulingArgs{...}
type JobSchedulingInput interface {
	pulumi.Input

	ToJobSchedulingOutput() JobSchedulingOutput
	ToJobSchedulingOutputWithContext(context.Context) JobSchedulingOutput
}

// Job scheduling options.
type JobSchedulingArgs struct {
	// Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.A job may be reported as thrashing if the driver exits with a non-zero code four times within a 10-minute window.Maximum value is 10.Note: This restartable job option is not supported in Dataproc workflow templates (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
	MaxFailuresPerHour pulumi.IntPtrInput `pulumi:"maxFailuresPerHour"`
	// Optional. Maximum total number of times a driver may be restarted as a result of the driver exiting with a non-zero code. After the maximum number is reached, the job will be reported as failed.Maximum value is 240.Note: Currently, this restartable job option is not supported in Dataproc workflow templates (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
	MaxFailuresTotal pulumi.IntPtrInput `pulumi:"maxFailuresTotal"`
}

func (JobSchedulingArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*JobScheduling)(nil)).Elem()
}

func (i JobSchedulingArgs) ToJobSchedulingOutput() JobSchedulingOutput {
	return i.ToJobSchedulingOutputWithContext(context.Background())
}

func (i JobSchedulingArgs) ToJobSchedulingOutputWithContext(ctx context.Context) JobSchedulingOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobSchedulingOutput)
}

func (i JobSchedulingArgs) ToJobSchedulingPtrOutput() JobSchedulingPtrOutput {
	return i.ToJobSchedulingPtrOutputWithContext(context.Background())
}

func (i JobSchedulingArgs) ToJobSchedulingPtrOutputWithContext(ctx context.Context) JobSchedulingPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobSchedulingOutput).ToJobSchedulingPtrOutputWithContext(ctx)
}

// JobSchedulingPtrInput is an input type that accepts JobSchedulingArgs, JobSchedulingPtr and JobSchedulingPtrOutput values.
// You can construct a concrete instance of `JobSchedulingPtrInput` via:
//
//	        JobSchedulingArgs{...}
//
//	or:
//
//	        nil
type JobSchedulingPtrInput interface {
	pulumi.Input

	ToJobSchedulingPtrOutput() JobSchedulingPtrOutput
	ToJobSchedulingPtrOutputWithContext(context.Context) JobSchedulingPtrOutput
}

type jobSchedulingPtrType JobSchedulingArgs

func JobSchedulingPtr(v *JobSchedulingArgs) JobSchedulingPtrInput {
	return (*jobSchedulingPtrType)(v)
}

func (*jobSchedulingPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**JobScheduling)(nil)).Elem()
}

func (i *jobSchedulingPtrType) ToJobSchedulingPtrOutput() JobSchedulingPtrOutput {
	return i.ToJobSchedulingPtrOutputWithContext(context.Background())
}

func (i *jobSchedulingPtrType) ToJobSchedulingPtrOutputWithContext(ctx context.Context) JobSchedulingPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobSchedulingPtrOutput)
}

// Job scheduling options.
type JobSchedulingOutput struct{ *pulumi.OutputState }

func (JobSchedulingOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobScheduling)(nil)).Elem()
}

func (o JobSchedulingOutput) ToJobSchedulingOutput() JobSchedulingOutput {
	return o
}

func (o JobSchedulingOutput) ToJobSchedulingOutputWithContext(ctx context.Context) JobSchedulingOutput {
	return o
}

func (o JobSchedulingOutput) ToJobSchedulingPtrOutput() JobSchedulingPtrOutput {
	return o.ToJobSchedulingPtrOutputWithContext(context.Background())
}

func (o JobSchedulingOutput) ToJobSchedulingPtrOutputWithContext(ctx context.Context) JobSchedulingPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v JobScheduling) *JobScheduling {
		return &v
	}).(JobSchedulingPtrOutput)
}

// Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.A job may be reported as thrashing if the driver exits with a non-zero code four times within a 10-minute window.Maximum value is 10.Note: This restartable job option is not supported in Dataproc workflow templates (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
func (o JobSchedulingOutput) MaxFailuresPerHour() pulumi.IntPtrOutput {
	return o.ApplyT(func(v JobScheduling) *int { return v.MaxFailuresPerHour }).(pulumi.IntPtrOutput)
}

// Optional. Maximum total number of times a driver may be restarted as a result of the driver exiting with a non-zero code. After the maximum number is reached, the job will be reported as failed.Maximum value is 240.Note: Currently, this restartable job option is not supported in Dataproc workflow templates (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
func (o JobSchedulingOutput) MaxFailuresTotal() pulumi.IntPtrOutput {
	return o.ApplyT(func(v JobScheduling) *int { return v.MaxFailuresTotal }).(pulumi.IntPtrOutput)
}

type JobSchedulingPtrOutput struct{ *pulumi.OutputState }

func (JobSchedulingPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**JobScheduling)(nil)).Elem()
}

func (o JobSchedulingPtrOutput) ToJobSchedulingPtrOutput() JobSchedulingPtrOutput {
	return o
}

func (o JobSchedulingPtrOutput) ToJobSchedulingPtrOutputWithContext(ctx context.Context) JobSchedulingPtrOutput {
	return o
}

func (o JobSchedulingPtrOutput) Elem() JobSchedulingOutput {
	return o.ApplyT(func(v *JobScheduling) JobScheduling {
		if v != nil {
			return *v
		}
		var ret JobScheduling
		return ret
	}).(JobSchedulingOutput)
}

// Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.A job may be reported as thrashing if the driver exits with a non-zero code four times within a 10-minute window.Maximum value is 10.Note: This restartable job option is not supported in Dataproc workflow templates (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
func (o JobSchedulingPtrOutput) MaxFailuresPerHour() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *JobScheduling) *int {
		if v == nil {
			return nil
		}
		return v.MaxFailuresPerHour
	}).(pulumi.IntPtrOutput)
}

// Optional. Maximum total number of times a driver may be restarted as a result of the driver exiting with a non-zero code. After the maximum number is reached, the job will be reported as failed.Maximum value is 240.Note: Currently, this restartable job option is not supported in Dataproc workflow templates (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
func (o JobSchedulingPtrOutput) MaxFailuresTotal() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *JobScheduling) *int {
		if v == nil {
			return nil
		}
		return v.MaxFailuresTotal
	}).(pulumi.IntPtrOutput)
}

// Job scheduling options.
type JobSchedulingResponse struct {
	// Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.A job may be reported as thrashing if the driver exits with a non-zero code four times within a 10-minute window.Maximum value is 10.Note: This restartable job option is not supported in Dataproc workflow templates (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
	MaxFailuresPerHour int `pulumi:"maxFailuresPerHour"`
	// Optional. Maximum total number of times a driver may be restarted as a result of the driver exiting with a non-zero code. After the maximum number is reached, the job will be reported as failed.Maximum value is 240.Note: Currently, this restartable job option is not supported in Dataproc workflow templates (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
	MaxFailuresTotal int `pulumi:"maxFailuresTotal"`
}

// Job scheduling options.
type JobSchedulingResponseOutput struct{ *pulumi.OutputState }

func (JobSchedulingResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobSchedulingResponse)(nil)).Elem()
}

func (o JobSchedulingResponseOutput) ToJobSchedulingResponseOutput() JobSchedulingResponseOutput {
	return o
}

func (o JobSchedulingResponseOutput) ToJobSchedulingResponseOutputWithContext(ctx context.Context) JobSchedulingResponseOutput {
	return o
}

// Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.A job may be reported as thrashing if the driver exits with a non-zero code four times within a 10-minute window.Maximum value is 10.Note: This restartable job option is not supported in Dataproc workflow templates (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
func (o JobSchedulingResponseOutput) MaxFailuresPerHour() pulumi.IntOutput {
	return o.ApplyT(func(v JobSchedulingResponse) int { return v.MaxFailuresPerHour }).(pulumi.IntOutput)
}

// Optional. Maximum total number of times a driver may be restarted as a result of the driver exiting with a non-zero code. After the maximum number is reached, the job will be reported as failed.Maximum value is 240.Note: Currently, this restartable job option is not supported in Dataproc workflow templates (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template).
func (o JobSchedulingResponseOutput) MaxFailuresTotal() pulumi.IntOutput {
	return o.ApplyT(func(v JobSchedulingResponse) int { return v.MaxFailuresTotal }).(pulumi.IntOutput)
}

// Dataproc job status.
type JobStatusResponse struct {
	// Optional. Output only. Job state details, such as an error description if the state is ERROR.
	Details string `pulumi:"details"`
	// A state message specifying the overall job state.
	State string `pulumi:"state"`
	// The time when this state was entered.
	StateStartTime string `pulumi:"stateStartTime"`
	// Additional state information, which includes status reported by the agent.
	Substate string `pulumi:"substate"`
}

// Dataproc job status.
type JobStatusResponseOutput struct{ *pulumi.OutputState }

func (JobStatusResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobStatusResponse)(nil)).Elem()
}

func (o JobStatusResponseOutput) ToJobStatusResponseOutput() JobStatusResponseOutput {
	return o
}

func (o JobStatusResponseOutput) ToJobStatusResponseOutputWithContext(ctx context.Context) JobStatusResponseOutput {
	return o
}

// Optional. Output only. Job state details, such as an error description if the state is ERROR.
func (o JobStatusResponseOutput) Details() pulumi.StringOutput {
	return o.ApplyT(func(v JobStatusResponse) string { return v.Details }).(pulumi.StringOutput)
}

// A state message specifying the overall job state.
func (o JobStatusResponseOutput) State() pulumi.StringOutput {
	return o.ApplyT(func(v JobStatusResponse) string { return v.State }).(pulumi.StringOutput)
}

// The time when this state was entered.
func (o JobStatusResponseOutput) StateStartTime() pulumi.StringOutput {
	return o.ApplyT(func(v JobStatusResponse) string { return v.StateStartTime }).(pulumi.StringOutput)
}

// Additional state information, which includes status reported by the agent.
func (o JobStatusResponseOutput) Substate() pulumi.StringOutput {
	return o.ApplyT(func(v JobStatusResponse) string { return v.Substate }).(pulumi.StringOutput)
}

type JobStatusResponseArrayOutput struct{ *pulumi.OutputState }

func (JobStatusResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]JobStatusResponse)(nil)).Elem()
}

func (o JobStatusResponseArrayOutput) ToJobStatusResponseArrayOutput() JobStatusResponseArrayOutput {
	return o
}

func (o JobStatusResponseArrayOutput) ToJobStatusResponseArrayOutputWithContext(ctx context.Context) JobStatusResponseArrayOutput {
	return o
}

func (o JobStatusResponseArrayOutput) Index(i pulumi.IntInput) JobStatusResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) JobStatusResponse {
		return vs[0].([]JobStatusResponse)[vs[1].(int)]
	}).(JobStatusResponseOutput)
}

// Specifies Kerberos related configuration.
type KerberosConfig struct {
	// Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
	CrossRealmTrustAdminServer *string `pulumi:"crossRealmTrustAdminServer"`
	// Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
	CrossRealmTrustKdc *string `pulumi:"crossRealmTrustKdc"`
	// Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
	CrossRealmTrustRealm *string `pulumi:"crossRealmTrustRealm"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
	CrossRealmTrustSharedPasswordUri *string `pulumi:"crossRealmTrustSharedPasswordUri"`
	// Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
	EnableKerberos *bool `pulumi:"enableKerberos"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
	KdcDbKeyUri *string `pulumi:"kdcDbKeyUri"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
	KeyPasswordUri *string `pulumi:"keyPasswordUri"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
	KeystorePasswordUri *string `pulumi:"keystorePasswordUri"`
	// Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	KeystoreUri *string `pulumi:"keystoreUri"`
	// Optional. The uri of the KMS key used to encrypt various sensitive files.
	KmsKeyUri *string `pulumi:"kmsKeyUri"`
	// Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
	Realm *string `pulumi:"realm"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
	RootPrincipalPasswordUri *string `pulumi:"rootPrincipalPasswordUri"`
	// Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
	TgtLifetimeHours *int `pulumi:"tgtLifetimeHours"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
	TruststorePasswordUri *string `pulumi:"truststorePasswordUri"`
	// Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	TruststoreUri *string `pulumi:"truststoreUri"`
}

// KerberosConfigInput is an input type that accepts KerberosConfigArgs and KerberosConfigOutput values.
// You can construct a concrete instance of `KerberosConfigInput` via:
//
//	KerberosConfigArgs{...}
type KerberosConfigInput interface {
	pulumi.Input

	ToKerberosConfigOutput() KerberosConfigOutput
	ToKerberosConfigOutputWithContext(context.Context) KerberosConfigOutput
}

// Specifies Kerberos related configuration.
type KerberosConfigArgs struct {
	// Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
	CrossRealmTrustAdminServer pulumi.StringPtrInput `pulumi:"crossRealmTrustAdminServer"`
	// Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
	CrossRealmTrustKdc pulumi.StringPtrInput `pulumi:"crossRealmTrustKdc"`
	// Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
	CrossRealmTrustRealm pulumi.StringPtrInput `pulumi:"crossRealmTrustRealm"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
	CrossRealmTrustSharedPasswordUri pulumi.StringPtrInput `pulumi:"crossRealmTrustSharedPasswordUri"`
	// Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
	EnableKerberos pulumi.BoolPtrInput `pulumi:"enableKerberos"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
	KdcDbKeyUri pulumi.StringPtrInput `pulumi:"kdcDbKeyUri"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
	KeyPasswordUri pulumi.StringPtrInput `pulumi:"keyPasswordUri"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
	KeystorePasswordUri pulumi.StringPtrInput `pulumi:"keystorePasswordUri"`
	// Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	KeystoreUri pulumi.StringPtrInput `pulumi:"keystoreUri"`
	// Optional. The uri of the KMS key used to encrypt various sensitive files.
	KmsKeyUri pulumi.StringPtrInput `pulumi:"kmsKeyUri"`
	// Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
	Realm pulumi.StringPtrInput `pulumi:"realm"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
	RootPrincipalPasswordUri pulumi.StringPtrInput `pulumi:"rootPrincipalPasswordUri"`
	// Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
	TgtLifetimeHours pulumi.IntPtrInput `pulumi:"tgtLifetimeHours"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
	TruststorePasswordUri pulumi.StringPtrInput `pulumi:"truststorePasswordUri"`
	// Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	TruststoreUri pulumi.StringPtrInput `pulumi:"truststoreUri"`
}

func (KerberosConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*KerberosConfig)(nil)).Elem()
}

func (i KerberosConfigArgs) ToKerberosConfigOutput() KerberosConfigOutput {
	return i.ToKerberosConfigOutputWithContext(context.Background())
}

func (i KerberosConfigArgs) ToKerberosConfigOutputWithContext(ctx context.Context) KerberosConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(KerberosConfigOutput)
}

func (i KerberosConfigArgs) ToKerberosConfigPtrOutput() KerberosConfigPtrOutput {
	return i.ToKerberosConfigPtrOutputWithContext(context.Background())
}

func (i KerberosConfigArgs) ToKerberosConfigPtrOutputWithContext(ctx context.Context) KerberosConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(KerberosConfigOutput).ToKerberosConfigPtrOutputWithContext(ctx)
}

// KerberosConfigPtrInput is an input type that accepts KerberosConfigArgs, KerberosConfigPtr and KerberosConfigPtrOutput values.
// You can construct a concrete instance of `KerberosConfigPtrInput` via:
//
//	        KerberosConfigArgs{...}
//
//	or:
//
//	        nil
type KerberosConfigPtrInput interface {
	pulumi.Input

	ToKerberosConfigPtrOutput() KerberosConfigPtrOutput
	ToKerberosConfigPtrOutputWithContext(context.Context) KerberosConfigPtrOutput
}

type kerberosConfigPtrType KerberosConfigArgs

func KerberosConfigPtr(v *KerberosConfigArgs) KerberosConfigPtrInput {
	return (*kerberosConfigPtrType)(v)
}

func (*kerberosConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**KerberosConfig)(nil)).Elem()
}

func (i *kerberosConfigPtrType) ToKerberosConfigPtrOutput() KerberosConfigPtrOutput {
	return i.ToKerberosConfigPtrOutputWithContext(context.Background())
}

func (i *kerberosConfigPtrType) ToKerberosConfigPtrOutputWithContext(ctx context.Context) KerberosConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(KerberosConfigPtrOutput)
}

// Specifies Kerberos related configuration.
type KerberosConfigOutput struct{ *pulumi.OutputState }

func (KerberosConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*KerberosConfig)(nil)).Elem()
}

func (o KerberosConfigOutput) ToKerberosConfigOutput() KerberosConfigOutput {
	return o
}

func (o KerberosConfigOutput) ToKerberosConfigOutputWithContext(ctx context.Context) KerberosConfigOutput {
	return o
}

func (o KerberosConfigOutput) ToKerberosConfigPtrOutput() KerberosConfigPtrOutput {
	return o.ToKerberosConfigPtrOutputWithContext(context.Background())
}

func (o KerberosConfigOutput) ToKerberosConfigPtrOutputWithContext(ctx context.Context) KerberosConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v KerberosConfig) *KerberosConfig {
		return &v
	}).(KerberosConfigPtrOutput)
}

// Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
func (o KerberosConfigOutput) CrossRealmTrustAdminServer() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.CrossRealmTrustAdminServer }).(pulumi.StringPtrOutput)
}

// Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
func (o KerberosConfigOutput) CrossRealmTrustKdc() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.CrossRealmTrustKdc }).(pulumi.StringPtrOutput)
}

// Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
func (o KerberosConfigOutput) CrossRealmTrustRealm() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.CrossRealmTrustRealm }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
func (o KerberosConfigOutput) CrossRealmTrustSharedPasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.CrossRealmTrustSharedPasswordUri }).(pulumi.StringPtrOutput)
}

// Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
func (o KerberosConfigOutput) EnableKerberos() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *bool { return v.EnableKerberos }).(pulumi.BoolPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
func (o KerberosConfigOutput) KdcDbKeyUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.KdcDbKeyUri }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigOutput) KeyPasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.KeyPasswordUri }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigOutput) KeystorePasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.KeystorePasswordUri }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
func (o KerberosConfigOutput) KeystoreUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.KeystoreUri }).(pulumi.StringPtrOutput)
}

// Optional. The uri of the KMS key used to encrypt various sensitive files.
func (o KerberosConfigOutput) KmsKeyUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.KmsKeyUri }).(pulumi.StringPtrOutput)
}

// Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
func (o KerberosConfigOutput) Realm() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.Realm }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
func (o KerberosConfigOutput) RootPrincipalPasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.RootPrincipalPasswordUri }).(pulumi.StringPtrOutput)
}

// Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
func (o KerberosConfigOutput) TgtLifetimeHours() pulumi.IntPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *int { return v.TgtLifetimeHours }).(pulumi.IntPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigOutput) TruststorePasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.TruststorePasswordUri }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
func (o KerberosConfigOutput) TruststoreUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.TruststoreUri }).(pulumi.StringPtrOutput)
}

type KerberosConfigPtrOutput struct{ *pulumi.OutputState }

func (KerberosConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**KerberosConfig)(nil)).Elem()
}

func (o KerberosConfigPtrOutput) ToKerberosConfigPtrOutput() KerberosConfigPtrOutput {
	return o
}

func (o KerberosConfigPtrOutput) ToKerberosConfigPtrOutputWithContext(ctx context.Context) KerberosConfigPtrOutput {
	return o
}

func (o KerberosConfigPtrOutput) Elem() KerberosConfigOutput {
	return o.ApplyT(func(v *KerberosConfig) KerberosConfig {
		if v != nil {
			return *v
		}
		var ret KerberosConfig
		return ret
	}).(KerberosConfigOutput)
}

// Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
func (o KerberosConfigPtrOutput) CrossRealmTrustAdminServer() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.CrossRealmTrustAdminServer
	}).(pulumi.StringPtrOutput)
}

// Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
func (o KerberosConfigPtrOutput) CrossRealmTrustKdc() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.CrossRealmTrustKdc
	}).(pulumi.StringPtrOutput)
}

// Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
func (o KerberosConfigPtrOutput) CrossRealmTrustRealm() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.CrossRealmTrustRealm
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
func (o KerberosConfigPtrOutput) CrossRealmTrustSharedPasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.CrossRealmTrustSharedPasswordUri
	}).(pulumi.StringPtrOutput)
}

// Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
func (o KerberosConfigPtrOutput) EnableKerberos() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *bool {
		if v == nil {
			return nil
		}
		return v.EnableKerberos
	}).(pulumi.BoolPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
func (o KerberosConfigPtrOutput) KdcDbKeyUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.KdcDbKeyUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigPtrOutput) KeyPasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.KeyPasswordUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigPtrOutput) KeystorePasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.KeystorePasswordUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
func (o KerberosConfigPtrOutput) KeystoreUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.KeystoreUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The uri of the KMS key used to encrypt various sensitive files.
func (o KerberosConfigPtrOutput) KmsKeyUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.KmsKeyUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
func (o KerberosConfigPtrOutput) Realm() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.Realm
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
func (o KerberosConfigPtrOutput) RootPrincipalPasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.RootPrincipalPasswordUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
func (o KerberosConfigPtrOutput) TgtLifetimeHours() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *int {
		if v == nil {
			return nil
		}
		return v.TgtLifetimeHours
	}).(pulumi.IntPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigPtrOutput) TruststorePasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.TruststorePasswordUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
func (o KerberosConfigPtrOutput) TruststoreUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.TruststoreUri
	}).(pulumi.StringPtrOutput)
}

// Specifies Kerberos related configuration.
type KerberosConfigResponse struct {
	// Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
	CrossRealmTrustAdminServer string `pulumi:"crossRealmTrustAdminServer"`
	// Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
	CrossRealmTrustKdc string `pulumi:"crossRealmTrustKdc"`
	// Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
	CrossRealmTrustRealm string `pulumi:"crossRealmTrustRealm"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
	CrossRealmTrustSharedPasswordUri string `pulumi:"crossRealmTrustSharedPasswordUri"`
	// Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
	EnableKerberos bool `pulumi:"enableKerberos"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
	KdcDbKeyUri string `pulumi:"kdcDbKeyUri"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
	KeyPasswordUri string `pulumi:"keyPasswordUri"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
	KeystorePasswordUri string `pulumi:"keystorePasswordUri"`
	// Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	KeystoreUri string `pulumi:"keystoreUri"`
	// Optional. The uri of the KMS key used to encrypt various sensitive files.
	KmsKeyUri string `pulumi:"kmsKeyUri"`
	// Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
	Realm string `pulumi:"realm"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
	RootPrincipalPasswordUri string `pulumi:"rootPrincipalPasswordUri"`
	// Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
	TgtLifetimeHours int `pulumi:"tgtLifetimeHours"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
	TruststorePasswordUri string `pulumi:"truststorePasswordUri"`
	// Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	TruststoreUri string `pulumi:"truststoreUri"`
}

// Specifies Kerberos related configuration.
type KerberosConfigResponseOutput struct{ *pulumi.OutputState }

func (KerberosConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*KerberosConfigResponse)(nil)).Elem()
}

func (o KerberosConfigResponseOutput) ToKerberosConfigResponseOutput() KerberosConfigResponseOutput {
	return o
}

func (o KerberosConfigResponseOutput) ToKerberosConfigResponseOutputWithContext(ctx context.Context) KerberosConfigResponseOutput {
	return o
}

// Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
func (o KerberosConfigResponseOutput) CrossRealmTrustAdminServer() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.CrossRealmTrustAdminServer }).(pulumi.StringOutput)
}

// Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
func (o KerberosConfigResponseOutput) CrossRealmTrustKdc() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.CrossRealmTrustKdc }).(pulumi.StringOutput)
}

// Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
func (o KerberosConfigResponseOutput) CrossRealmTrustRealm() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.CrossRealmTrustRealm }).(pulumi.StringOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
func (o KerberosConfigResponseOutput) CrossRealmTrustSharedPasswordUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.CrossRealmTrustSharedPasswordUri }).(pulumi.StringOutput)
}

// Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
func (o KerberosConfigResponseOutput) EnableKerberos() pulumi.BoolOutput {
	return o.ApplyT(func(v KerberosConfigResponse) bool { return v.EnableKerberos }).(pulumi.BoolOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
func (o KerberosConfigResponseOutput) KdcDbKeyUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.KdcDbKeyUri }).(pulumi.StringOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigResponseOutput) KeyPasswordUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.KeyPasswordUri }).(pulumi.StringOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigResponseOutput) KeystorePasswordUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.KeystorePasswordUri }).(pulumi.StringOutput)
}

// Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
func (o KerberosConfigResponseOutput) KeystoreUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.KeystoreUri }).(pulumi.StringOutput)
}

// Optional. The uri of the KMS key used to encrypt various sensitive files.
func (o KerberosConfigResponseOutput) KmsKeyUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.KmsKeyUri }).(pulumi.StringOutput)
}

// Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
func (o KerberosConfigResponseOutput) Realm() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.Realm }).(pulumi.StringOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
func (o KerberosConfigResponseOutput) RootPrincipalPasswordUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.RootPrincipalPasswordUri }).(pulumi.StringOutput)
}

// Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
func (o KerberosConfigResponseOutput) TgtLifetimeHours() pulumi.IntOutput {
	return o.ApplyT(func(v KerberosConfigResponse) int { return v.TgtLifetimeHours }).(pulumi.IntOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigResponseOutput) TruststorePasswordUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.TruststorePasswordUri }).(pulumi.StringOutput)
}

// Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
func (o KerberosConfigResponseOutput) TruststoreUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.TruststoreUri }).(pulumi.StringOutput)
}

// The configuration for running the Dataproc cluster on Kubernetes.
type KubernetesClusterConfig struct {
	// The configuration for running the Dataproc cluster on GKE.
	GkeClusterConfig GkeClusterConfig `pulumi:"gkeClusterConfig"`
	// Optional. A namespace within the Kubernetes cluster to deploy into. If this namespace does not exist, it is created. If it exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it. If not specified, the name of the Dataproc Cluster is used.
	KubernetesNamespace *string `pulumi:"kubernetesNamespace"`
	// Optional. The software configuration for this Dataproc cluster running on Kubernetes.
	KubernetesSoftwareConfig *KubernetesSoftwareConfig `pulumi:"kubernetesSoftwareConfig"`
}

// KubernetesClusterConfigInput is an input type that accepts KubernetesClusterConfigArgs and KubernetesClusterConfigOutput values.
// You can construct a concrete instance of `KubernetesClusterConfigInput` via:
//
//	KubernetesClusterConfigArgs{...}
type KubernetesClusterConfigInput interface {
	pulumi.Input

	ToKubernetesClusterConfigOutput() KubernetesClusterConfigOutput
	ToKubernetesClusterConfigOutputWithContext(context.Context) KubernetesClusterConfigOutput
}

// The configuration for running the Dataproc cluster on Kubernetes.
type KubernetesClusterConfigArgs struct {
	// The configuration for running the Dataproc cluster on GKE.
	GkeClusterConfig GkeClusterConfigInput `pulumi:"gkeClusterConfig"`
	// Optional. A namespace within the Kubernetes cluster to deploy into. If this namespace does not exist, it is created. If it exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it. If not specified, the name of the Dataproc Cluster is used.
	KubernetesNamespace pulumi.StringPtrInput `pulumi:"kubernetesNamespace"`
	// Optional. The software configuration for this Dataproc cluster running on Kubernetes.
	KubernetesSoftwareConfig KubernetesSoftwareConfigPtrInput `pulumi:"kubernetesSoftwareConfig"`
}

func (KubernetesClusterConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*KubernetesClusterConfig)(nil)).Elem()
}

func (i KubernetesClusterConfigArgs) ToKubernetesClusterConfigOutput() KubernetesClusterConfigOutput {
	return i.ToKubernetesClusterConfigOutputWithContext(context.Background())
}

func (i KubernetesClusterConfigArgs) ToKubernetesClusterConfigOutputWithContext(ctx context.Context) KubernetesClusterConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(KubernetesClusterConfigOutput)
}

func (i KubernetesClusterConfigArgs) ToKubernetesClusterConfigPtrOutput() KubernetesClusterConfigPtrOutput {
	return i.ToKubernetesClusterConfigPtrOutputWithContext(context.Background())
}

func (i KubernetesClusterConfigArgs) ToKubernetesClusterConfigPtrOutputWithContext(ctx context.Context) KubernetesClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(KubernetesClusterConfigOutput).ToKubernetesClusterConfigPtrOutputWithContext(ctx)
}

// KubernetesClusterConfigPtrInput is an input type that accepts KubernetesClusterConfigArgs, KubernetesClusterConfigPtr and KubernetesClusterConfigPtrOutput values.
// You can construct a concrete instance of `KubernetesClusterConfigPtrInput` via:
//
//	        KubernetesClusterConfigArgs{...}
//
//	or:
//
//	        nil
type KubernetesClusterConfigPtrInput interface {
	pulumi.Input

	ToKubernetesClusterConfigPtrOutput() KubernetesClusterConfigPtrOutput
	ToKubernetesClusterConfigPtrOutputWithContext(context.Context) KubernetesClusterConfigPtrOutput
}

type kubernetesClusterConfigPtrType KubernetesClusterConfigArgs

func KubernetesClusterConfigPtr(v *KubernetesClusterConfigArgs) KubernetesClusterConfigPtrInput {
	return (*kubernetesClusterConfigPtrType)(v)
}

func (*kubernetesClusterConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**KubernetesClusterConfig)(nil)).Elem()
}

func (i *kubernetesClusterConfigPtrType) ToKubernetesClusterConfigPtrOutput() KubernetesClusterConfigPtrOutput {
	return i.ToKubernetesClusterConfigPtrOutputWithContext(context.Background())
}

func (i *kubernetesClusterConfigPtrType) ToKubernetesClusterConfigPtrOutputWithContext(ctx context.Context) KubernetesClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(KubernetesClusterConfigPtrOutput)
}

// The configuration for running the Dataproc cluster on Kubernetes.
type KubernetesClusterConfigOutput struct{ *pulumi.OutputState }

func (KubernetesClusterConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*KubernetesClusterConfig)(nil)).Elem()
}

func (o KubernetesClusterConfigOutput) ToKubernetesClusterConfigOutput() KubernetesClusterConfigOutput {
	return o
}

func (o KubernetesClusterConfigOutput) ToKubernetesClusterConfigOutputWithContext(ctx context.Context) KubernetesClusterConfigOutput {
	return o
}

func (o KubernetesClusterConfigOutput) ToKubernetesClusterConfigPtrOutput() KubernetesClusterConfigPtrOutput {
	return o.ToKubernetesClusterConfigPtrOutputWithContext(context.Background())
}

func (o KubernetesClusterConfigOutput) ToKubernetesClusterConfigPtrOutputWithContext(ctx context.Context) KubernetesClusterConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v KubernetesClusterConfig) *KubernetesClusterConfig {
		return &v
	}).(KubernetesClusterConfigPtrOutput)
}

// The configuration for running the Dataproc cluster on GKE.
func (o KubernetesClusterConfigOutput) GkeClusterConfig() GkeClusterConfigOutput {
	return o.ApplyT(func(v KubernetesClusterConfig) GkeClusterConfig { return v.GkeClusterConfig }).(GkeClusterConfigOutput)
}

// Optional. A namespace within the Kubernetes cluster to deploy into. If this namespace does not exist, it is created. If it exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it. If not specified, the name of the Dataproc Cluster is used.
func (o KubernetesClusterConfigOutput) KubernetesNamespace() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KubernetesClusterConfig) *string { return v.KubernetesNamespace }).(pulumi.StringPtrOutput)
}

// Optional. The software configuration for this Dataproc cluster running on Kubernetes.
func (o KubernetesClusterConfigOutput) KubernetesSoftwareConfig() KubernetesSoftwareConfigPtrOutput {
	return o.ApplyT(func(v KubernetesClusterConfig) *KubernetesSoftwareConfig { return v.KubernetesSoftwareConfig }).(KubernetesSoftwareConfigPtrOutput)
}

type KubernetesClusterConfigPtrOutput struct{ *pulumi.OutputState }

func (KubernetesClusterConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**KubernetesClusterConfig)(nil)).Elem()
}

func (o KubernetesClusterConfigPtrOutput) ToKubernetesClusterConfigPtrOutput() KubernetesClusterConfigPtrOutput {
	return o
}

func (o KubernetesClusterConfigPtrOutput) ToKubernetesClusterConfigPtrOutputWithContext(ctx context.Context) KubernetesClusterConfigPtrOutput {
	return o
}

func (o KubernetesClusterConfigPtrOutput) Elem() KubernetesClusterConfigOutput {
	return o.ApplyT(func(v *KubernetesClusterConfig) KubernetesClusterConfig {
		if v != nil {
			return *v
		}
		var ret KubernetesClusterConfig
		return ret
	}).(KubernetesClusterConfigOutput)
}

// The configuration for running the Dataproc cluster on GKE.
func (o KubernetesClusterConfigPtrOutput) GkeClusterConfig() GkeClusterConfigPtrOutput {
	return o.ApplyT(func(v *KubernetesClusterConfig) *GkeClusterConfig {
		if v == nil {
			return nil
		}
		return &v.GkeClusterConfig
	}).(GkeClusterConfigPtrOutput)
}

// Optional. A namespace within the Kubernetes cluster to deploy into. If this namespace does not exist, it is created. If it exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it. If not specified, the name of the Dataproc Cluster is used.
func (o KubernetesClusterConfigPtrOutput) KubernetesNamespace() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KubernetesClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.KubernetesNamespace
	}).(pulumi.StringPtrOutput)
}

// Optional. The software configuration for this Dataproc cluster running on Kubernetes.
func (o KubernetesClusterConfigPtrOutput) KubernetesSoftwareConfig() KubernetesSoftwareConfigPtrOutput {
	return o.ApplyT(func(v *KubernetesClusterConfig) *KubernetesSoftwareConfig {
		if v == nil {
			return nil
		}
		return v.KubernetesSoftwareConfig
	}).(KubernetesSoftwareConfigPtrOutput)
}

// The configuration for running the Dataproc cluster on Kubernetes.
type KubernetesClusterConfigResponse struct {
	// The configuration for running the Dataproc cluster on GKE.
	GkeClusterConfig GkeClusterConfigResponse `pulumi:"gkeClusterConfig"`
	// Optional. A namespace within the Kubernetes cluster to deploy into. If this namespace does not exist, it is created. If it exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it. If not specified, the name of the Dataproc Cluster is used.
	KubernetesNamespace string `pulumi:"kubernetesNamespace"`
	// Optional. The software configuration for this Dataproc cluster running on Kubernetes.
	KubernetesSoftwareConfig KubernetesSoftwareConfigResponse `pulumi:"kubernetesSoftwareConfig"`
}

// The configuration for running the Dataproc cluster on Kubernetes.
type KubernetesClusterConfigResponseOutput struct{ *pulumi.OutputState }

func (KubernetesClusterConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*KubernetesClusterConfigResponse)(nil)).Elem()
}

func (o KubernetesClusterConfigResponseOutput) ToKubernetesClusterConfigResponseOutput() KubernetesClusterConfigResponseOutput {
	return o
}

func (o KubernetesClusterConfigResponseOutput) ToKubernetesClusterConfigResponseOutputWithContext(ctx context.Context) KubernetesClusterConfigResponseOutput {
	return o
}

// The configuration for running the Dataproc cluster on GKE.
func (o KubernetesClusterConfigResponseOutput) GkeClusterConfig() GkeClusterConfigResponseOutput {
	return o.ApplyT(func(v KubernetesClusterConfigResponse) GkeClusterConfigResponse { return v.GkeClusterConfig }).(GkeClusterConfigResponseOutput)
}

// Optional. A namespace within the Kubernetes cluster to deploy into. If this namespace does not exist, it is created. If it exists, Dataproc verifies that another Dataproc VirtualCluster is not installed into it. If not specified, the name of the Dataproc Cluster is used.
func (o KubernetesClusterConfigResponseOutput) KubernetesNamespace() pulumi.StringOutput {
	return o.ApplyT(func(v KubernetesClusterConfigResponse) string { return v.KubernetesNamespace }).(pulumi.StringOutput)
}

// Optional. The software configuration for this Dataproc cluster running on Kubernetes.
func (o KubernetesClusterConfigResponseOutput) KubernetesSoftwareConfig() KubernetesSoftwareConfigResponseOutput {
	return o.ApplyT(func(v KubernetesClusterConfigResponse) KubernetesSoftwareConfigResponse {
		return v.KubernetesSoftwareConfig
	}).(KubernetesSoftwareConfigResponseOutput)
}

// The software configuration for this Dataproc cluster running on Kubernetes.
type KubernetesSoftwareConfig struct {
	// The components that should be installed in this Dataproc cluster. The key must be a string from the KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
	ComponentVersion map[string]string `pulumi:"componentVersion"`
	// The properties to set on daemon config files.Property keys are specified in prefix:property format, for example spark:spark.kubernetes.container.image. The following are supported prefixes and their mappings: spark: spark-defaults.confFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
	Properties map[string]string `pulumi:"properties"`
}

// KubernetesSoftwareConfigInput is an input type that accepts KubernetesSoftwareConfigArgs and KubernetesSoftwareConfigOutput values.
// You can construct a concrete instance of `KubernetesSoftwareConfigInput` via:
//
//	KubernetesSoftwareConfigArgs{...}
type KubernetesSoftwareConfigInput interface {
	pulumi.Input

	ToKubernetesSoftwareConfigOutput() KubernetesSoftwareConfigOutput
	ToKubernetesSoftwareConfigOutputWithContext(context.Context) KubernetesSoftwareConfigOutput
}

// The software configuration for this Dataproc cluster running on Kubernetes.
type KubernetesSoftwareConfigArgs struct {
	// The components that should be installed in this Dataproc cluster. The key must be a string from the KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
	ComponentVersion pulumi.StringMapInput `pulumi:"componentVersion"`
	// The properties to set on daemon config files.Property keys are specified in prefix:property format, for example spark:spark.kubernetes.container.image. The following are supported prefixes and their mappings: spark: spark-defaults.confFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
	Properties pulumi.StringMapInput `pulumi:"properties"`
}

func (KubernetesSoftwareConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*KubernetesSoftwareConfig)(nil)).Elem()
}

func (i KubernetesSoftwareConfigArgs) ToKubernetesSoftwareConfigOutput() KubernetesSoftwareConfigOutput {
	return i.ToKubernetesSoftwareConfigOutputWithContext(context.Background())
}

func (i KubernetesSoftwareConfigArgs) ToKubernetesSoftwareConfigOutputWithContext(ctx context.Context) KubernetesSoftwareConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(KubernetesSoftwareConfigOutput)
}

func (i KubernetesSoftwareConfigArgs) ToKubernetesSoftwareConfigPtrOutput() KubernetesSoftwareConfigPtrOutput {
	return i.ToKubernetesSoftwareConfigPtrOutputWithContext(context.Background())
}

func (i KubernetesSoftwareConfigArgs) ToKubernetesSoftwareConfigPtrOutputWithContext(ctx context.Context) KubernetesSoftwareConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(KubernetesSoftwareConfigOutput).ToKubernetesSoftwareConfigPtrOutputWithContext(ctx)
}

// KubernetesSoftwareConfigPtrInput is an input type that accepts KubernetesSoftwareConfigArgs, KubernetesSoftwareConfigPtr and KubernetesSoftwareConfigPtrOutput values.
// You can construct a concrete instance of `KubernetesSoftwareConfigPtrInput` via:
//
//	        KubernetesSoftwareConfigArgs{...}
//
//	or:
//
//	        nil
type KubernetesSoftwareConfigPtrInput interface {
	pulumi.Input

	ToKubernetesSoftwareConfigPtrOutput() KubernetesSoftwareConfigPtrOutput
	ToKubernetesSoftwareConfigPtrOutputWithContext(context.Context) KubernetesSoftwareConfigPtrOutput
}

type kubernetesSoftwareConfigPtrType KubernetesSoftwareConfigArgs

func KubernetesSoftwareConfigPtr(v *KubernetesSoftwareConfigArgs) KubernetesSoftwareConfigPtrInput {
	return (*kubernetesSoftwareConfigPtrType)(v)
}

func (*kubernetesSoftwareConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**KubernetesSoftwareConfig)(nil)).Elem()
}

func (i *kubernetesSoftwareConfigPtrType) ToKubernetesSoftwareConfigPtrOutput() KubernetesSoftwareConfigPtrOutput {
	return i.ToKubernetesSoftwareConfigPtrOutputWithContext(context.Background())
}

func (i *kubernetesSoftwareConfigPtrType) ToKubernetesSoftwareConfigPtrOutputWithContext(ctx context.Context) KubernetesSoftwareConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(KubernetesSoftwareConfigPtrOutput)
}

// The software configuration for this Dataproc cluster running on Kubernetes.
type KubernetesSoftwareConfigOutput struct{ *pulumi.OutputState }

func (KubernetesSoftwareConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*KubernetesSoftwareConfig)(nil)).Elem()
}

func (o KubernetesSoftwareConfigOutput) ToKubernetesSoftwareConfigOutput() KubernetesSoftwareConfigOutput {
	return o
}

func (o KubernetesSoftwareConfigOutput) ToKubernetesSoftwareConfigOutputWithContext(ctx context.Context) KubernetesSoftwareConfigOutput {
	return o
}

func (o KubernetesSoftwareConfigOutput) ToKubernetesSoftwareConfigPtrOutput() KubernetesSoftwareConfigPtrOutput {
	return o.ToKubernetesSoftwareConfigPtrOutputWithContext(context.Background())
}

func (o KubernetesSoftwareConfigOutput) ToKubernetesSoftwareConfigPtrOutputWithContext(ctx context.Context) KubernetesSoftwareConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v KubernetesSoftwareConfig) *KubernetesSoftwareConfig {
		return &v
	}).(KubernetesSoftwareConfigPtrOutput)
}

// The components that should be installed in this Dataproc cluster. The key must be a string from the KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
func (o KubernetesSoftwareConfigOutput) ComponentVersion() pulumi.StringMapOutput {
	return o.ApplyT(func(v KubernetesSoftwareConfig) map[string]string { return v.ComponentVersion }).(pulumi.StringMapOutput)
}

// The properties to set on daemon config files.Property keys are specified in prefix:property format, for example spark:spark.kubernetes.container.image. The following are supported prefixes and their mappings: spark: spark-defaults.confFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
func (o KubernetesSoftwareConfigOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v KubernetesSoftwareConfig) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

type KubernetesSoftwareConfigPtrOutput struct{ *pulumi.OutputState }

func (KubernetesSoftwareConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**KubernetesSoftwareConfig)(nil)).Elem()
}

func (o KubernetesSoftwareConfigPtrOutput) ToKubernetesSoftwareConfigPtrOutput() KubernetesSoftwareConfigPtrOutput {
	return o
}

func (o KubernetesSoftwareConfigPtrOutput) ToKubernetesSoftwareConfigPtrOutputWithContext(ctx context.Context) KubernetesSoftwareConfigPtrOutput {
	return o
}

func (o KubernetesSoftwareConfigPtrOutput) Elem() KubernetesSoftwareConfigOutput {
	return o.ApplyT(func(v *KubernetesSoftwareConfig) KubernetesSoftwareConfig {
		if v != nil {
			return *v
		}
		var ret KubernetesSoftwareConfig
		return ret
	}).(KubernetesSoftwareConfigOutput)
}

// The components that should be installed in this Dataproc cluster. The key must be a string from the KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
func (o KubernetesSoftwareConfigPtrOutput) ComponentVersion() pulumi.StringMapOutput {
	return o.ApplyT(func(v *KubernetesSoftwareConfig) map[string]string {
		if v == nil {
			return nil
		}
		return v.ComponentVersion
	}).(pulumi.StringMapOutput)
}

// The properties to set on daemon config files.Property keys are specified in prefix:property format, for example spark:spark.kubernetes.container.image. The following are supported prefixes and their mappings: spark: spark-defaults.confFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
func (o KubernetesSoftwareConfigPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *KubernetesSoftwareConfig) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// The software configuration for this Dataproc cluster running on Kubernetes.
type KubernetesSoftwareConfigResponse struct {
	// The components that should be installed in this Dataproc cluster. The key must be a string from the KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
	ComponentVersion map[string]string `pulumi:"componentVersion"`
	// The properties to set on daemon config files.Property keys are specified in prefix:property format, for example spark:spark.kubernetes.container.image. The following are supported prefixes and their mappings: spark: spark-defaults.confFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
	Properties map[string]string `pulumi:"properties"`
}

// The software configuration for this Dataproc cluster running on Kubernetes.
type KubernetesSoftwareConfigResponseOutput struct{ *pulumi.OutputState }

func (KubernetesSoftwareConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*KubernetesSoftwareConfigResponse)(nil)).Elem()
}

func (o KubernetesSoftwareConfigResponseOutput) ToKubernetesSoftwareConfigResponseOutput() KubernetesSoftwareConfigResponseOutput {
	return o
}

func (o KubernetesSoftwareConfigResponseOutput) ToKubernetesSoftwareConfigResponseOutputWithContext(ctx context.Context) KubernetesSoftwareConfigResponseOutput {
	return o
}

// The components that should be installed in this Dataproc cluster. The key must be a string from the KubernetesComponent enumeration. The value is the version of the software to be installed. At least one entry must be specified.
func (o KubernetesSoftwareConfigResponseOutput) ComponentVersion() pulumi.StringMapOutput {
	return o.ApplyT(func(v KubernetesSoftwareConfigResponse) map[string]string { return v.ComponentVersion }).(pulumi.StringMapOutput)
}

// The properties to set on daemon config files.Property keys are specified in prefix:property format, for example spark:spark.kubernetes.container.image. The following are supported prefixes and their mappings: spark: spark-defaults.confFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
func (o KubernetesSoftwareConfigResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v KubernetesSoftwareConfigResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// Specifies the cluster auto-delete schedule configuration.
type LifecycleConfig struct {
	// Optional. The time when cluster will be auto-deleted (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	AutoDeleteTime *string `pulumi:"autoDeleteTime"`
	// Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	AutoDeleteTtl *string `pulumi:"autoDeleteTtl"`
	// Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	IdleDeleteTtl *string `pulumi:"idleDeleteTtl"`
}

// LifecycleConfigInput is an input type that accepts LifecycleConfigArgs and LifecycleConfigOutput values.
// You can construct a concrete instance of `LifecycleConfigInput` via:
//
//	LifecycleConfigArgs{...}
type LifecycleConfigInput interface {
	pulumi.Input

	ToLifecycleConfigOutput() LifecycleConfigOutput
	ToLifecycleConfigOutputWithContext(context.Context) LifecycleConfigOutput
}

// Specifies the cluster auto-delete schedule configuration.
type LifecycleConfigArgs struct {
	// Optional. The time when cluster will be auto-deleted (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	AutoDeleteTime pulumi.StringPtrInput `pulumi:"autoDeleteTime"`
	// Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	AutoDeleteTtl pulumi.StringPtrInput `pulumi:"autoDeleteTtl"`
	// Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	IdleDeleteTtl pulumi.StringPtrInput `pulumi:"idleDeleteTtl"`
}

func (LifecycleConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*LifecycleConfig)(nil)).Elem()
}

func (i LifecycleConfigArgs) ToLifecycleConfigOutput() LifecycleConfigOutput {
	return i.ToLifecycleConfigOutputWithContext(context.Background())
}

func (i LifecycleConfigArgs) ToLifecycleConfigOutputWithContext(ctx context.Context) LifecycleConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LifecycleConfigOutput)
}

func (i LifecycleConfigArgs) ToLifecycleConfigPtrOutput() LifecycleConfigPtrOutput {
	return i.ToLifecycleConfigPtrOutputWithContext(context.Background())
}

func (i LifecycleConfigArgs) ToLifecycleConfigPtrOutputWithContext(ctx context.Context) LifecycleConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LifecycleConfigOutput).ToLifecycleConfigPtrOutputWithContext(ctx)
}

// LifecycleConfigPtrInput is an input type that accepts LifecycleConfigArgs, LifecycleConfigPtr and LifecycleConfigPtrOutput values.
// You can construct a concrete instance of `LifecycleConfigPtrInput` via:
//
//	        LifecycleConfigArgs{...}
//
//	or:
//
//	        nil
type LifecycleConfigPtrInput interface {
	pulumi.Input

	ToLifecycleConfigPtrOutput() LifecycleConfigPtrOutput
	ToLifecycleConfigPtrOutputWithContext(context.Context) LifecycleConfigPtrOutput
}

type lifecycleConfigPtrType LifecycleConfigArgs

func LifecycleConfigPtr(v *LifecycleConfigArgs) LifecycleConfigPtrInput {
	return (*lifecycleConfigPtrType)(v)
}

func (*lifecycleConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**LifecycleConfig)(nil)).Elem()
}

func (i *lifecycleConfigPtrType) ToLifecycleConfigPtrOutput() LifecycleConfigPtrOutput {
	return i.ToLifecycleConfigPtrOutputWithContext(context.Background())
}

func (i *lifecycleConfigPtrType) ToLifecycleConfigPtrOutputWithContext(ctx context.Context) LifecycleConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LifecycleConfigPtrOutput)
}

// Specifies the cluster auto-delete schedule configuration.
type LifecycleConfigOutput struct{ *pulumi.OutputState }

func (LifecycleConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*LifecycleConfig)(nil)).Elem()
}

func (o LifecycleConfigOutput) ToLifecycleConfigOutput() LifecycleConfigOutput {
	return o
}

func (o LifecycleConfigOutput) ToLifecycleConfigOutputWithContext(ctx context.Context) LifecycleConfigOutput {
	return o
}

func (o LifecycleConfigOutput) ToLifecycleConfigPtrOutput() LifecycleConfigPtrOutput {
	return o.ToLifecycleConfigPtrOutputWithContext(context.Background())
}

func (o LifecycleConfigOutput) ToLifecycleConfigPtrOutputWithContext(ctx context.Context) LifecycleConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v LifecycleConfig) *LifecycleConfig {
		return &v
	}).(LifecycleConfigPtrOutput)
}

// Optional. The time when cluster will be auto-deleted (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigOutput) AutoDeleteTime() pulumi.StringPtrOutput {
	return o.ApplyT(func(v LifecycleConfig) *string { return v.AutoDeleteTime }).(pulumi.StringPtrOutput)
}

// Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigOutput) AutoDeleteTtl() pulumi.StringPtrOutput {
	return o.ApplyT(func(v LifecycleConfig) *string { return v.AutoDeleteTtl }).(pulumi.StringPtrOutput)
}

// Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigOutput) IdleDeleteTtl() pulumi.StringPtrOutput {
	return o.ApplyT(func(v LifecycleConfig) *string { return v.IdleDeleteTtl }).(pulumi.StringPtrOutput)
}

type LifecycleConfigPtrOutput struct{ *pulumi.OutputState }

func (LifecycleConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**LifecycleConfig)(nil)).Elem()
}

func (o LifecycleConfigPtrOutput) ToLifecycleConfigPtrOutput() LifecycleConfigPtrOutput {
	return o
}

func (o LifecycleConfigPtrOutput) ToLifecycleConfigPtrOutputWithContext(ctx context.Context) LifecycleConfigPtrOutput {
	return o
}

func (o LifecycleConfigPtrOutput) Elem() LifecycleConfigOutput {
	return o.ApplyT(func(v *LifecycleConfig) LifecycleConfig {
		if v != nil {
			return *v
		}
		var ret LifecycleConfig
		return ret
	}).(LifecycleConfigOutput)
}

// Optional. The time when cluster will be auto-deleted (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigPtrOutput) AutoDeleteTime() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LifecycleConfig) *string {
		if v == nil {
			return nil
		}
		return v.AutoDeleteTime
	}).(pulumi.StringPtrOutput)
}

// Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigPtrOutput) AutoDeleteTtl() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LifecycleConfig) *string {
		if v == nil {
			return nil
		}
		return v.AutoDeleteTtl
	}).(pulumi.StringPtrOutput)
}

// Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigPtrOutput) IdleDeleteTtl() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LifecycleConfig) *string {
		if v == nil {
			return nil
		}
		return v.IdleDeleteTtl
	}).(pulumi.StringPtrOutput)
}

// Specifies the cluster auto-delete schedule configuration.
type LifecycleConfigResponse struct {
	// Optional. The time when cluster will be auto-deleted (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	AutoDeleteTime string `pulumi:"autoDeleteTime"`
	// Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	AutoDeleteTtl string `pulumi:"autoDeleteTtl"`
	// Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	IdleDeleteTtl string `pulumi:"idleDeleteTtl"`
	// The time when cluster became idle (most recent job finished) and became eligible for deletion due to idleness (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	IdleStartTime string `pulumi:"idleStartTime"`
}

// Specifies the cluster auto-delete schedule configuration.
type LifecycleConfigResponseOutput struct{ *pulumi.OutputState }

func (LifecycleConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*LifecycleConfigResponse)(nil)).Elem()
}

func (o LifecycleConfigResponseOutput) ToLifecycleConfigResponseOutput() LifecycleConfigResponseOutput {
	return o
}

func (o LifecycleConfigResponseOutput) ToLifecycleConfigResponseOutputWithContext(ctx context.Context) LifecycleConfigResponseOutput {
	return o
}

// Optional. The time when cluster will be auto-deleted (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigResponseOutput) AutoDeleteTime() pulumi.StringOutput {
	return o.ApplyT(func(v LifecycleConfigResponse) string { return v.AutoDeleteTime }).(pulumi.StringOutput)
}

// Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigResponseOutput) AutoDeleteTtl() pulumi.StringOutput {
	return o.ApplyT(func(v LifecycleConfigResponse) string { return v.AutoDeleteTtl }).(pulumi.StringOutput)
}

// Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigResponseOutput) IdleDeleteTtl() pulumi.StringOutput {
	return o.ApplyT(func(v LifecycleConfigResponse) string { return v.IdleDeleteTtl }).(pulumi.StringOutput)
}

// The time when cluster became idle (most recent job finished) and became eligible for deletion due to idleness (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigResponseOutput) IdleStartTime() pulumi.StringOutput {
	return o.ApplyT(func(v LifecycleConfigResponse) string { return v.IdleStartTime }).(pulumi.StringOutput)
}

// The runtime logging config of the job.
type LoggingConfig struct {
	// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: - 'com.google = FATAL' - 'root = INFO' - 'org.apache = DEBUG'
	DriverLogLevels map[string]string `pulumi:"driverLogLevels"`
}

// LoggingConfigInput is an input type that accepts LoggingConfigArgs and LoggingConfigOutput values.
// You can construct a concrete instance of `LoggingConfigInput` via:
//
//	LoggingConfigArgs{...}
type LoggingConfigInput interface {
	pulumi.Input

	ToLoggingConfigOutput() LoggingConfigOutput
	ToLoggingConfigOutputWithContext(context.Context) LoggingConfigOutput
}

// The runtime logging config of the job.
type LoggingConfigArgs struct {
	// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: - 'com.google = FATAL' - 'root = INFO' - 'org.apache = DEBUG'
	DriverLogLevels pulumi.StringMapInput `pulumi:"driverLogLevels"`
}

func (LoggingConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*LoggingConfig)(nil)).Elem()
}

func (i LoggingConfigArgs) ToLoggingConfigOutput() LoggingConfigOutput {
	return i.ToLoggingConfigOutputWithContext(context.Background())
}

func (i LoggingConfigArgs) ToLoggingConfigOutputWithContext(ctx context.Context) LoggingConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LoggingConfigOutput)
}

func (i LoggingConfigArgs) ToLoggingConfigPtrOutput() LoggingConfigPtrOutput {
	return i.ToLoggingConfigPtrOutputWithContext(context.Background())
}

func (i LoggingConfigArgs) ToLoggingConfigPtrOutputWithContext(ctx context.Context) LoggingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LoggingConfigOutput).ToLoggingConfigPtrOutputWithContext(ctx)
}

// LoggingConfigPtrInput is an input type that accepts LoggingConfigArgs, LoggingConfigPtr and LoggingConfigPtrOutput values.
// You can construct a concrete instance of `LoggingConfigPtrInput` via:
//
//	        LoggingConfigArgs{...}
//
//	or:
//
//	        nil
type LoggingConfigPtrInput interface {
	pulumi.Input

	ToLoggingConfigPtrOutput() LoggingConfigPtrOutput
	ToLoggingConfigPtrOutputWithContext(context.Context) LoggingConfigPtrOutput
}

type loggingConfigPtrType LoggingConfigArgs

func LoggingConfigPtr(v *LoggingConfigArgs) LoggingConfigPtrInput {
	return (*loggingConfigPtrType)(v)
}

func (*loggingConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**LoggingConfig)(nil)).Elem()
}

func (i *loggingConfigPtrType) ToLoggingConfigPtrOutput() LoggingConfigPtrOutput {
	return i.ToLoggingConfigPtrOutputWithContext(context.Background())
}

func (i *loggingConfigPtrType) ToLoggingConfigPtrOutputWithContext(ctx context.Context) LoggingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LoggingConfigPtrOutput)
}

// The runtime logging config of the job.
type LoggingConfigOutput struct{ *pulumi.OutputState }

func (LoggingConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*LoggingConfig)(nil)).Elem()
}

func (o LoggingConfigOutput) ToLoggingConfigOutput() LoggingConfigOutput {
	return o
}

func (o LoggingConfigOutput) ToLoggingConfigOutputWithContext(ctx context.Context) LoggingConfigOutput {
	return o
}

func (o LoggingConfigOutput) ToLoggingConfigPtrOutput() LoggingConfigPtrOutput {
	return o.ToLoggingConfigPtrOutputWithContext(context.Background())
}

func (o LoggingConfigOutput) ToLoggingConfigPtrOutputWithContext(ctx context.Context) LoggingConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v LoggingConfig) *LoggingConfig {
		return &v
	}).(LoggingConfigPtrOutput)
}

// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: - 'com.google = FATAL' - 'root = INFO' - 'org.apache = DEBUG'
func (o LoggingConfigOutput) DriverLogLevels() pulumi.StringMapOutput {
	return o.ApplyT(func(v LoggingConfig) map[string]string { return v.DriverLogLevels }).(pulumi.StringMapOutput)
}

type LoggingConfigPtrOutput struct{ *pulumi.OutputState }

func (LoggingConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**LoggingConfig)(nil)).Elem()
}

func (o LoggingConfigPtrOutput) ToLoggingConfigPtrOutput() LoggingConfigPtrOutput {
	return o
}

func (o LoggingConfigPtrOutput) ToLoggingConfigPtrOutputWithContext(ctx context.Context) LoggingConfigPtrOutput {
	return o
}

func (o LoggingConfigPtrOutput) Elem() LoggingConfigOutput {
	return o.ApplyT(func(v *LoggingConfig) LoggingConfig {
		if v != nil {
			return *v
		}
		var ret LoggingConfig
		return ret
	}).(LoggingConfigOutput)
}

// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: - 'com.google = FATAL' - 'root = INFO' - 'org.apache = DEBUG'
func (o LoggingConfigPtrOutput) DriverLogLevels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *LoggingConfig) map[string]string {
		if v == nil {
			return nil
		}
		return v.DriverLogLevels
	}).(pulumi.StringMapOutput)
}

// The runtime logging config of the job.
type LoggingConfigResponse struct {
	// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: - 'com.google = FATAL' - 'root = INFO' - 'org.apache = DEBUG'
	DriverLogLevels map[string]string `pulumi:"driverLogLevels"`
}

// The runtime logging config of the job.
type LoggingConfigResponseOutput struct{ *pulumi.OutputState }

func (LoggingConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*LoggingConfigResponse)(nil)).Elem()
}

func (o LoggingConfigResponseOutput) ToLoggingConfigResponseOutput() LoggingConfigResponseOutput {
	return o
}

func (o LoggingConfigResponseOutput) ToLoggingConfigResponseOutputWithContext(ctx context.Context) LoggingConfigResponseOutput {
	return o
}

// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: - 'com.google = FATAL' - 'root = INFO' - 'org.apache = DEBUG'
func (o LoggingConfigResponseOutput) DriverLogLevels() pulumi.StringMapOutput {
	return o.ApplyT(func(v LoggingConfigResponse) map[string]string { return v.DriverLogLevels }).(pulumi.StringMapOutput)
}

// Cluster that is managed by the workflow.
type ManagedCluster struct {
	// The cluster name prefix. A unique cluster name will be formed by appending a random suffix.The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
	ClusterName string `pulumi:"clusterName"`
	// The cluster configuration.
	Config ClusterConfig `pulumi:"config"`
	// Optional. The labels to associate with this cluster.Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given cluster.
	Labels map[string]string `pulumi:"labels"`
}

// ManagedClusterInput is an input type that accepts ManagedClusterArgs and ManagedClusterOutput values.
// You can construct a concrete instance of `ManagedClusterInput` via:
//
//	ManagedClusterArgs{...}
type ManagedClusterInput interface {
	pulumi.Input

	ToManagedClusterOutput() ManagedClusterOutput
	ToManagedClusterOutputWithContext(context.Context) ManagedClusterOutput
}

// Cluster that is managed by the workflow.
type ManagedClusterArgs struct {
	// The cluster name prefix. A unique cluster name will be formed by appending a random suffix.The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
	ClusterName pulumi.StringInput `pulumi:"clusterName"`
	// The cluster configuration.
	Config ClusterConfigInput `pulumi:"config"`
	// Optional. The labels to associate with this cluster.Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given cluster.
	Labels pulumi.StringMapInput `pulumi:"labels"`
}

func (ManagedClusterArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ManagedCluster)(nil)).Elem()
}

func (i ManagedClusterArgs) ToManagedClusterOutput() ManagedClusterOutput {
	return i.ToManagedClusterOutputWithContext(context.Background())
}

func (i ManagedClusterArgs) ToManagedClusterOutputWithContext(ctx context.Context) ManagedClusterOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ManagedClusterOutput)
}

func (i ManagedClusterArgs) ToManagedClusterPtrOutput() ManagedClusterPtrOutput {
	return i.ToManagedClusterPtrOutputWithContext(context.Background())
}

func (i ManagedClusterArgs) ToManagedClusterPtrOutputWithContext(ctx context.Context) ManagedClusterPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ManagedClusterOutput).ToManagedClusterPtrOutputWithContext(ctx)
}

// ManagedClusterPtrInput is an input type that accepts ManagedClusterArgs, ManagedClusterPtr and ManagedClusterPtrOutput values.
// You can construct a concrete instance of `ManagedClusterPtrInput` via:
//
//	        ManagedClusterArgs{...}
//
//	or:
//
//	        nil
type ManagedClusterPtrInput interface {
	pulumi.Input

	ToManagedClusterPtrOutput() ManagedClusterPtrOutput
	ToManagedClusterPtrOutputWithContext(context.Context) ManagedClusterPtrOutput
}

type managedClusterPtrType ManagedClusterArgs

func ManagedClusterPtr(v *ManagedClusterArgs) ManagedClusterPtrInput {
	return (*managedClusterPtrType)(v)
}

func (*managedClusterPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ManagedCluster)(nil)).Elem()
}

func (i *managedClusterPtrType) ToManagedClusterPtrOutput() ManagedClusterPtrOutput {
	return i.ToManagedClusterPtrOutputWithContext(context.Background())
}

func (i *managedClusterPtrType) ToManagedClusterPtrOutputWithContext(ctx context.Context) ManagedClusterPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ManagedClusterPtrOutput)
}

// Cluster that is managed by the workflow.
type ManagedClusterOutput struct{ *pulumi.OutputState }

func (ManagedClusterOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ManagedCluster)(nil)).Elem()
}

func (o ManagedClusterOutput) ToManagedClusterOutput() ManagedClusterOutput {
	return o
}

func (o ManagedClusterOutput) ToManagedClusterOutputWithContext(ctx context.Context) ManagedClusterOutput {
	return o
}

func (o ManagedClusterOutput) ToManagedClusterPtrOutput() ManagedClusterPtrOutput {
	return o.ToManagedClusterPtrOutputWithContext(context.Background())
}

func (o ManagedClusterOutput) ToManagedClusterPtrOutputWithContext(ctx context.Context) ManagedClusterPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ManagedCluster) *ManagedCluster {
		return &v
	}).(ManagedClusterPtrOutput)
}

// The cluster name prefix. A unique cluster name will be formed by appending a random suffix.The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
func (o ManagedClusterOutput) ClusterName() pulumi.StringOutput {
	return o.ApplyT(func(v ManagedCluster) string { return v.ClusterName }).(pulumi.StringOutput)
}

// The cluster configuration.
func (o ManagedClusterOutput) Config() ClusterConfigOutput {
	return o.ApplyT(func(v ManagedCluster) ClusterConfig { return v.Config }).(ClusterConfigOutput)
}

// Optional. The labels to associate with this cluster.Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given cluster.
func (o ManagedClusterOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v ManagedCluster) map[string]string { return v.Labels }).(pulumi.StringMapOutput)
}

type ManagedClusterPtrOutput struct{ *pulumi.OutputState }

func (ManagedClusterPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ManagedCluster)(nil)).Elem()
}

func (o ManagedClusterPtrOutput) ToManagedClusterPtrOutput() ManagedClusterPtrOutput {
	return o
}

func (o ManagedClusterPtrOutput) ToManagedClusterPtrOutputWithContext(ctx context.Context) ManagedClusterPtrOutput {
	return o
}

func (o ManagedClusterPtrOutput) Elem() ManagedClusterOutput {
	return o.ApplyT(func(v *ManagedCluster) ManagedCluster {
		if v != nil {
			return *v
		}
		var ret ManagedCluster
		return ret
	}).(ManagedClusterOutput)
}

// The cluster name prefix. A unique cluster name will be formed by appending a random suffix.The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
func (o ManagedClusterPtrOutput) ClusterName() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ManagedCluster) *string {
		if v == nil {
			return nil
		}
		return &v.ClusterName
	}).(pulumi.StringPtrOutput)
}

// The cluster configuration.
func (o ManagedClusterPtrOutput) Config() ClusterConfigPtrOutput {
	return o.ApplyT(func(v *ManagedCluster) *ClusterConfig {
		if v == nil {
			return nil
		}
		return &v.Config
	}).(ClusterConfigPtrOutput)
}

// Optional. The labels to associate with this cluster.Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given cluster.
func (o ManagedClusterPtrOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *ManagedCluster) map[string]string {
		if v == nil {
			return nil
		}
		return v.Labels
	}).(pulumi.StringMapOutput)
}

// Cluster that is managed by the workflow.
type ManagedClusterResponse struct {
	// The cluster name prefix. A unique cluster name will be formed by appending a random suffix.The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
	ClusterName string `pulumi:"clusterName"`
	// The cluster configuration.
	Config ClusterConfigResponse `pulumi:"config"`
	// Optional. The labels to associate with this cluster.Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given cluster.
	Labels map[string]string `pulumi:"labels"`
}

// Cluster that is managed by the workflow.
type ManagedClusterResponseOutput struct{ *pulumi.OutputState }

func (ManagedClusterResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ManagedClusterResponse)(nil)).Elem()
}

func (o ManagedClusterResponseOutput) ToManagedClusterResponseOutput() ManagedClusterResponseOutput {
	return o
}

func (o ManagedClusterResponseOutput) ToManagedClusterResponseOutputWithContext(ctx context.Context) ManagedClusterResponseOutput {
	return o
}

// The cluster name prefix. A unique cluster name will be formed by appending a random suffix.The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
func (o ManagedClusterResponseOutput) ClusterName() pulumi.StringOutput {
	return o.ApplyT(func(v ManagedClusterResponse) string { return v.ClusterName }).(pulumi.StringOutput)
}

// The cluster configuration.
func (o ManagedClusterResponseOutput) Config() ClusterConfigResponseOutput {
	return o.ApplyT(func(v ManagedClusterResponse) ClusterConfigResponse { return v.Config }).(ClusterConfigResponseOutput)
}

// Optional. The labels to associate with this cluster.Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given cluster.
func (o ManagedClusterResponseOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v ManagedClusterResponse) map[string]string { return v.Labels }).(pulumi.StringMapOutput)
}

// Specifies the resources used to actively manage an instance group.
type ManagedGroupConfigResponse struct {
	// The name of the Instance Group Manager for this group.
	InstanceGroupManagerName string `pulumi:"instanceGroupManagerName"`
	// The name of the Instance Template used for the Managed Instance Group.
	InstanceTemplateName string `pulumi:"instanceTemplateName"`
}

// Specifies the resources used to actively manage an instance group.
type ManagedGroupConfigResponseOutput struct{ *pulumi.OutputState }

func (ManagedGroupConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ManagedGroupConfigResponse)(nil)).Elem()
}

func (o ManagedGroupConfigResponseOutput) ToManagedGroupConfigResponseOutput() ManagedGroupConfigResponseOutput {
	return o
}

func (o ManagedGroupConfigResponseOutput) ToManagedGroupConfigResponseOutputWithContext(ctx context.Context) ManagedGroupConfigResponseOutput {
	return o
}

// The name of the Instance Group Manager for this group.
func (o ManagedGroupConfigResponseOutput) InstanceGroupManagerName() pulumi.StringOutput {
	return o.ApplyT(func(v ManagedGroupConfigResponse) string { return v.InstanceGroupManagerName }).(pulumi.StringOutput)
}

// The name of the Instance Template used for the Managed Instance Group.
func (o ManagedGroupConfigResponseOutput) InstanceTemplateName() pulumi.StringOutput {
	return o.ApplyT(func(v ManagedGroupConfigResponse) string { return v.InstanceTemplateName }).(pulumi.StringOutput)
}

// Specifies a Metastore configuration.
type MetastoreConfig struct {
	// Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[dataproc_region]/services/[service-name]
	DataprocMetastoreService string `pulumi:"dataprocMetastoreService"`
}

// MetastoreConfigInput is an input type that accepts MetastoreConfigArgs and MetastoreConfigOutput values.
// You can construct a concrete instance of `MetastoreConfigInput` via:
//
//	MetastoreConfigArgs{...}
type MetastoreConfigInput interface {
	pulumi.Input

	ToMetastoreConfigOutput() MetastoreConfigOutput
	ToMetastoreConfigOutputWithContext(context.Context) MetastoreConfigOutput
}

// Specifies a Metastore configuration.
type MetastoreConfigArgs struct {
	// Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[dataproc_region]/services/[service-name]
	DataprocMetastoreService pulumi.StringInput `pulumi:"dataprocMetastoreService"`
}

func (MetastoreConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*MetastoreConfig)(nil)).Elem()
}

func (i MetastoreConfigArgs) ToMetastoreConfigOutput() MetastoreConfigOutput {
	return i.ToMetastoreConfigOutputWithContext(context.Background())
}

func (i MetastoreConfigArgs) ToMetastoreConfigOutputWithContext(ctx context.Context) MetastoreConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(MetastoreConfigOutput)
}

func (i MetastoreConfigArgs) ToMetastoreConfigPtrOutput() MetastoreConfigPtrOutput {
	return i.ToMetastoreConfigPtrOutputWithContext(context.Background())
}

func (i MetastoreConfigArgs) ToMetastoreConfigPtrOutputWithContext(ctx context.Context) MetastoreConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(MetastoreConfigOutput).ToMetastoreConfigPtrOutputWithContext(ctx)
}

// MetastoreConfigPtrInput is an input type that accepts MetastoreConfigArgs, MetastoreConfigPtr and MetastoreConfigPtrOutput values.
// You can construct a concrete instance of `MetastoreConfigPtrInput` via:
//
//	        MetastoreConfigArgs{...}
//
//	or:
//
//	        nil
type MetastoreConfigPtrInput interface {
	pulumi.Input

	ToMetastoreConfigPtrOutput() MetastoreConfigPtrOutput
	ToMetastoreConfigPtrOutputWithContext(context.Context) MetastoreConfigPtrOutput
}

type metastoreConfigPtrType MetastoreConfigArgs

func MetastoreConfigPtr(v *MetastoreConfigArgs) MetastoreConfigPtrInput {
	return (*metastoreConfigPtrType)(v)
}

func (*metastoreConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**MetastoreConfig)(nil)).Elem()
}

func (i *metastoreConfigPtrType) ToMetastoreConfigPtrOutput() MetastoreConfigPtrOutput {
	return i.ToMetastoreConfigPtrOutputWithContext(context.Background())
}

func (i *metastoreConfigPtrType) ToMetastoreConfigPtrOutputWithContext(ctx context.Context) MetastoreConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(MetastoreConfigPtrOutput)
}

// Specifies a Metastore configuration.
type MetastoreConfigOutput struct{ *pulumi.OutputState }

func (MetastoreConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*MetastoreConfig)(nil)).Elem()
}

func (o MetastoreConfigOutput) ToMetastoreConfigOutput() MetastoreConfigOutput {
	return o
}

func (o MetastoreConfigOutput) ToMetastoreConfigOutputWithContext(ctx context.Context) MetastoreConfigOutput {
	return o
}

func (o MetastoreConfigOutput) ToMetastoreConfigPtrOutput() MetastoreConfigPtrOutput {
	return o.ToMetastoreConfigPtrOutputWithContext(context.Background())
}

func (o MetastoreConfigOutput) ToMetastoreConfigPtrOutputWithContext(ctx context.Context) MetastoreConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v MetastoreConfig) *MetastoreConfig {
		return &v
	}).(MetastoreConfigPtrOutput)
}

// Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[dataproc_region]/services/[service-name]
func (o MetastoreConfigOutput) DataprocMetastoreService() pulumi.StringOutput {
	return o.ApplyT(func(v MetastoreConfig) string { return v.DataprocMetastoreService }).(pulumi.StringOutput)
}

type MetastoreConfigPtrOutput struct{ *pulumi.OutputState }

func (MetastoreConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**MetastoreConfig)(nil)).Elem()
}

func (o MetastoreConfigPtrOutput) ToMetastoreConfigPtrOutput() MetastoreConfigPtrOutput {
	return o
}

func (o MetastoreConfigPtrOutput) ToMetastoreConfigPtrOutputWithContext(ctx context.Context) MetastoreConfigPtrOutput {
	return o
}

func (o MetastoreConfigPtrOutput) Elem() MetastoreConfigOutput {
	return o.ApplyT(func(v *MetastoreConfig) MetastoreConfig {
		if v != nil {
			return *v
		}
		var ret MetastoreConfig
		return ret
	}).(MetastoreConfigOutput)
}

// Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[dataproc_region]/services/[service-name]
func (o MetastoreConfigPtrOutput) DataprocMetastoreService() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *MetastoreConfig) *string {
		if v == nil {
			return nil
		}
		return &v.DataprocMetastoreService
	}).(pulumi.StringPtrOutput)
}

// Specifies a Metastore configuration.
type MetastoreConfigResponse struct {
	// Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[dataproc_region]/services/[service-name]
	DataprocMetastoreService string `pulumi:"dataprocMetastoreService"`
}

// Specifies a Metastore configuration.
type MetastoreConfigResponseOutput struct{ *pulumi.OutputState }

func (MetastoreConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*MetastoreConfigResponse)(nil)).Elem()
}

func (o MetastoreConfigResponseOutput) ToMetastoreConfigResponseOutput() MetastoreConfigResponseOutput {
	return o
}

func (o MetastoreConfigResponseOutput) ToMetastoreConfigResponseOutputWithContext(ctx context.Context) MetastoreConfigResponseOutput {
	return o
}

// Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[dataproc_region]/services/[service-name]
func (o MetastoreConfigResponseOutput) DataprocMetastoreService() pulumi.StringOutput {
	return o.ApplyT(func(v MetastoreConfigResponse) string { return v.DataprocMetastoreService }).(pulumi.StringOutput)
}

// A Dataproc custom metric.
type Metric struct {
	// Optional. Specify one or more Custom metrics (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics) to collect for the metric course (for the SPARK metric source (any Spark metric (https://spark.apache.org/docs/latest/monitoring.html#metrics) can be specified).Provide metrics in the following format: METRIC_SOURCE: INSTANCE:GROUP:METRIC Use camelcase as appropriate.Examples: yarn:ResourceManager:QueueMetrics:AppsCompleted spark:driver:DAGScheduler:job.allJobs sparkHistoryServer:JVM:Memory:NonHeapMemoryUsage.committed hiveserver2:JVM:Memory:NonHeapMemoryUsage.used Notes: Only the specified overridden metrics are collected for the metric source. For example, if one or more spark:executive metrics are listed as metric overrides, other SPARK metrics are not collected. The collection of the metrics for other enabled custom metric sources is unaffected. For example, if both SPARK andd YARN metric sources are enabled, and overrides are provided for Spark metrics only, all YARN metrics are collected.
	MetricOverrides []string `pulumi:"metricOverrides"`
	// A standard set of metrics is collected unless metricOverrides are specified for the metric source (see Custom metrics (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics) for more information).
	MetricSource MetricMetricSource `pulumi:"metricSource"`
}

// MetricInput is an input type that accepts MetricArgs and MetricOutput values.
// You can construct a concrete instance of `MetricInput` via:
//
//	MetricArgs{...}
type MetricInput interface {
	pulumi.Input

	ToMetricOutput() MetricOutput
	ToMetricOutputWithContext(context.Context) MetricOutput
}

// A Dataproc custom metric.
type MetricArgs struct {
	// Optional. Specify one or more Custom metrics (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics) to collect for the metric course (for the SPARK metric source (any Spark metric (https://spark.apache.org/docs/latest/monitoring.html#metrics) can be specified).Provide metrics in the following format: METRIC_SOURCE: INSTANCE:GROUP:METRIC Use camelcase as appropriate.Examples: yarn:ResourceManager:QueueMetrics:AppsCompleted spark:driver:DAGScheduler:job.allJobs sparkHistoryServer:JVM:Memory:NonHeapMemoryUsage.committed hiveserver2:JVM:Memory:NonHeapMemoryUsage.used Notes: Only the specified overridden metrics are collected for the metric source. For example, if one or more spark:executive metrics are listed as metric overrides, other SPARK metrics are not collected. The collection of the metrics for other enabled custom metric sources is unaffected. For example, if both SPARK andd YARN metric sources are enabled, and overrides are provided for Spark metrics only, all YARN metrics are collected.
	MetricOverrides pulumi.StringArrayInput `pulumi:"metricOverrides"`
	// A standard set of metrics is collected unless metricOverrides are specified for the metric source (see Custom metrics (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics) for more information).
	MetricSource MetricMetricSourceInput `pulumi:"metricSource"`
}

func (MetricArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*Metric)(nil)).Elem()
}

func (i MetricArgs) ToMetricOutput() MetricOutput {
	return i.ToMetricOutputWithContext(context.Background())
}

func (i MetricArgs) ToMetricOutputWithContext(ctx context.Context) MetricOutput {
	return pulumi.ToOutputWithContext(ctx, i).(MetricOutput)
}

// MetricArrayInput is an input type that accepts MetricArray and MetricArrayOutput values.
// You can construct a concrete instance of `MetricArrayInput` via:
//
//	MetricArray{ MetricArgs{...} }
type MetricArrayInput interface {
	pulumi.Input

	ToMetricArrayOutput() MetricArrayOutput
	ToMetricArrayOutputWithContext(context.Context) MetricArrayOutput
}

type MetricArray []MetricInput

func (MetricArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]Metric)(nil)).Elem()
}

func (i MetricArray) ToMetricArrayOutput() MetricArrayOutput {
	return i.ToMetricArrayOutputWithContext(context.Background())
}

func (i MetricArray) ToMetricArrayOutputWithContext(ctx context.Context) MetricArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(MetricArrayOutput)
}

// A Dataproc custom metric.
type MetricOutput struct{ *pulumi.OutputState }

func (MetricOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*Metric)(nil)).Elem()
}

func (o MetricOutput) ToMetricOutput() MetricOutput {
	return o
}

func (o MetricOutput) ToMetricOutputWithContext(ctx context.Context) MetricOutput {
	return o
}

// Optional. Specify one or more Custom metrics (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics) to collect for the metric course (for the SPARK metric source (any Spark metric (https://spark.apache.org/docs/latest/monitoring.html#metrics) can be specified).Provide metrics in the following format: METRIC_SOURCE: INSTANCE:GROUP:METRIC Use camelcase as appropriate.Examples: yarn:ResourceManager:QueueMetrics:AppsCompleted spark:driver:DAGScheduler:job.allJobs sparkHistoryServer:JVM:Memory:NonHeapMemoryUsage.committed hiveserver2:JVM:Memory:NonHeapMemoryUsage.used Notes: Only the specified overridden metrics are collected for the metric source. For example, if one or more spark:executive metrics are listed as metric overrides, other SPARK metrics are not collected. The collection of the metrics for other enabled custom metric sources is unaffected. For example, if both SPARK andd YARN metric sources are enabled, and overrides are provided for Spark metrics only, all YARN metrics are collected.
func (o MetricOutput) MetricOverrides() pulumi.StringArrayOutput {
	return o.ApplyT(func(v Metric) []string { return v.MetricOverrides }).(pulumi.StringArrayOutput)
}

// A standard set of metrics is collected unless metricOverrides are specified for the metric source (see Custom metrics (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics) for more information).
func (o MetricOutput) MetricSource() MetricMetricSourceOutput {
	return o.ApplyT(func(v Metric) MetricMetricSource { return v.MetricSource }).(MetricMetricSourceOutput)
}

type MetricArrayOutput struct{ *pulumi.OutputState }

func (MetricArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]Metric)(nil)).Elem()
}

func (o MetricArrayOutput) ToMetricArrayOutput() MetricArrayOutput {
	return o
}

func (o MetricArrayOutput) ToMetricArrayOutputWithContext(ctx context.Context) MetricArrayOutput {
	return o
}

func (o MetricArrayOutput) Index(i pulumi.IntInput) MetricOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) Metric {
		return vs[0].([]Metric)[vs[1].(int)]
	}).(MetricOutput)
}

// A Dataproc custom metric.
type MetricResponse struct {
	// Optional. Specify one or more Custom metrics (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics) to collect for the metric course (for the SPARK metric source (any Spark metric (https://spark.apache.org/docs/latest/monitoring.html#metrics) can be specified).Provide metrics in the following format: METRIC_SOURCE: INSTANCE:GROUP:METRIC Use camelcase as appropriate.Examples: yarn:ResourceManager:QueueMetrics:AppsCompleted spark:driver:DAGScheduler:job.allJobs sparkHistoryServer:JVM:Memory:NonHeapMemoryUsage.committed hiveserver2:JVM:Memory:NonHeapMemoryUsage.used Notes: Only the specified overridden metrics are collected for the metric source. For example, if one or more spark:executive metrics are listed as metric overrides, other SPARK metrics are not collected. The collection of the metrics for other enabled custom metric sources is unaffected. For example, if both SPARK andd YARN metric sources are enabled, and overrides are provided for Spark metrics only, all YARN metrics are collected.
	MetricOverrides []string `pulumi:"metricOverrides"`
	// A standard set of metrics is collected unless metricOverrides are specified for the metric source (see Custom metrics (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics) for more information).
	MetricSource string `pulumi:"metricSource"`
}

// A Dataproc custom metric.
type MetricResponseOutput struct{ *pulumi.OutputState }

func (MetricResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*MetricResponse)(nil)).Elem()
}

func (o MetricResponseOutput) ToMetricResponseOutput() MetricResponseOutput {
	return o
}

func (o MetricResponseOutput) ToMetricResponseOutputWithContext(ctx context.Context) MetricResponseOutput {
	return o
}

// Optional. Specify one or more Custom metrics (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics) to collect for the metric course (for the SPARK metric source (any Spark metric (https://spark.apache.org/docs/latest/monitoring.html#metrics) can be specified).Provide metrics in the following format: METRIC_SOURCE: INSTANCE:GROUP:METRIC Use camelcase as appropriate.Examples: yarn:ResourceManager:QueueMetrics:AppsCompleted spark:driver:DAGScheduler:job.allJobs sparkHistoryServer:JVM:Memory:NonHeapMemoryUsage.committed hiveserver2:JVM:Memory:NonHeapMemoryUsage.used Notes: Only the specified overridden metrics are collected for the metric source. For example, if one or more spark:executive metrics are listed as metric overrides, other SPARK metrics are not collected. The collection of the metrics for other enabled custom metric sources is unaffected. For example, if both SPARK andd YARN metric sources are enabled, and overrides are provided for Spark metrics only, all YARN metrics are collected.
func (o MetricResponseOutput) MetricOverrides() pulumi.StringArrayOutput {
	return o.ApplyT(func(v MetricResponse) []string { return v.MetricOverrides }).(pulumi.StringArrayOutput)
}

// A standard set of metrics is collected unless metricOverrides are specified for the metric source (see Custom metrics (https://cloud.google.com/dataproc/docs/guides/dataproc-metrics#custom_metrics) for more information).
func (o MetricResponseOutput) MetricSource() pulumi.StringOutput {
	return o.ApplyT(func(v MetricResponse) string { return v.MetricSource }).(pulumi.StringOutput)
}

type MetricResponseArrayOutput struct{ *pulumi.OutputState }

func (MetricResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]MetricResponse)(nil)).Elem()
}

func (o MetricResponseArrayOutput) ToMetricResponseArrayOutput() MetricResponseArrayOutput {
	return o
}

func (o MetricResponseArrayOutput) ToMetricResponseArrayOutputWithContext(ctx context.Context) MetricResponseArrayOutput {
	return o
}

func (o MetricResponseArrayOutput) Index(i pulumi.IntInput) MetricResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) MetricResponse {
		return vs[0].([]MetricResponse)[vs[1].(int)]
	}).(MetricResponseOutput)
}

// Deprecated. Used only for the deprecated beta. A full, namespace-isolated deployment target for an existing GKE cluster.
type NamespacedGkeDeploymentTarget struct {
	// Optional. A namespace within the GKE cluster to deploy into.
	ClusterNamespace *string `pulumi:"clusterNamespace"`
	// Optional. The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
	TargetGkeCluster *string `pulumi:"targetGkeCluster"`
}

// NamespacedGkeDeploymentTargetInput is an input type that accepts NamespacedGkeDeploymentTargetArgs and NamespacedGkeDeploymentTargetOutput values.
// You can construct a concrete instance of `NamespacedGkeDeploymentTargetInput` via:
//
//	NamespacedGkeDeploymentTargetArgs{...}
type NamespacedGkeDeploymentTargetInput interface {
	pulumi.Input

	ToNamespacedGkeDeploymentTargetOutput() NamespacedGkeDeploymentTargetOutput
	ToNamespacedGkeDeploymentTargetOutputWithContext(context.Context) NamespacedGkeDeploymentTargetOutput
}

// Deprecated. Used only for the deprecated beta. A full, namespace-isolated deployment target for an existing GKE cluster.
type NamespacedGkeDeploymentTargetArgs struct {
	// Optional. A namespace within the GKE cluster to deploy into.
	ClusterNamespace pulumi.StringPtrInput `pulumi:"clusterNamespace"`
	// Optional. The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
	TargetGkeCluster pulumi.StringPtrInput `pulumi:"targetGkeCluster"`
}

func (NamespacedGkeDeploymentTargetArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*NamespacedGkeDeploymentTarget)(nil)).Elem()
}

func (i NamespacedGkeDeploymentTargetArgs) ToNamespacedGkeDeploymentTargetOutput() NamespacedGkeDeploymentTargetOutput {
	return i.ToNamespacedGkeDeploymentTargetOutputWithContext(context.Background())
}

func (i NamespacedGkeDeploymentTargetArgs) ToNamespacedGkeDeploymentTargetOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NamespacedGkeDeploymentTargetOutput)
}

func (i NamespacedGkeDeploymentTargetArgs) ToNamespacedGkeDeploymentTargetPtrOutput() NamespacedGkeDeploymentTargetPtrOutput {
	return i.ToNamespacedGkeDeploymentTargetPtrOutputWithContext(context.Background())
}

func (i NamespacedGkeDeploymentTargetArgs) ToNamespacedGkeDeploymentTargetPtrOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NamespacedGkeDeploymentTargetOutput).ToNamespacedGkeDeploymentTargetPtrOutputWithContext(ctx)
}

// NamespacedGkeDeploymentTargetPtrInput is an input type that accepts NamespacedGkeDeploymentTargetArgs, NamespacedGkeDeploymentTargetPtr and NamespacedGkeDeploymentTargetPtrOutput values.
// You can construct a concrete instance of `NamespacedGkeDeploymentTargetPtrInput` via:
//
//	        NamespacedGkeDeploymentTargetArgs{...}
//
//	or:
//
//	        nil
type NamespacedGkeDeploymentTargetPtrInput interface {
	pulumi.Input

	ToNamespacedGkeDeploymentTargetPtrOutput() NamespacedGkeDeploymentTargetPtrOutput
	ToNamespacedGkeDeploymentTargetPtrOutputWithContext(context.Context) NamespacedGkeDeploymentTargetPtrOutput
}

type namespacedGkeDeploymentTargetPtrType NamespacedGkeDeploymentTargetArgs

func NamespacedGkeDeploymentTargetPtr(v *NamespacedGkeDeploymentTargetArgs) NamespacedGkeDeploymentTargetPtrInput {
	return (*namespacedGkeDeploymentTargetPtrType)(v)
}

func (*namespacedGkeDeploymentTargetPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**NamespacedGkeDeploymentTarget)(nil)).Elem()
}

func (i *namespacedGkeDeploymentTargetPtrType) ToNamespacedGkeDeploymentTargetPtrOutput() NamespacedGkeDeploymentTargetPtrOutput {
	return i.ToNamespacedGkeDeploymentTargetPtrOutputWithContext(context.Background())
}

func (i *namespacedGkeDeploymentTargetPtrType) ToNamespacedGkeDeploymentTargetPtrOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NamespacedGkeDeploymentTargetPtrOutput)
}

// Deprecated. Used only for the deprecated beta. A full, namespace-isolated deployment target for an existing GKE cluster.
type NamespacedGkeDeploymentTargetOutput struct{ *pulumi.OutputState }

func (NamespacedGkeDeploymentTargetOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NamespacedGkeDeploymentTarget)(nil)).Elem()
}

func (o NamespacedGkeDeploymentTargetOutput) ToNamespacedGkeDeploymentTargetOutput() NamespacedGkeDeploymentTargetOutput {
	return o
}

func (o NamespacedGkeDeploymentTargetOutput) ToNamespacedGkeDeploymentTargetOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetOutput {
	return o
}

func (o NamespacedGkeDeploymentTargetOutput) ToNamespacedGkeDeploymentTargetPtrOutput() NamespacedGkeDeploymentTargetPtrOutput {
	return o.ToNamespacedGkeDeploymentTargetPtrOutputWithContext(context.Background())
}

func (o NamespacedGkeDeploymentTargetOutput) ToNamespacedGkeDeploymentTargetPtrOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v NamespacedGkeDeploymentTarget) *NamespacedGkeDeploymentTarget {
		return &v
	}).(NamespacedGkeDeploymentTargetPtrOutput)
}

// Optional. A namespace within the GKE cluster to deploy into.
func (o NamespacedGkeDeploymentTargetOutput) ClusterNamespace() pulumi.StringPtrOutput {
	return o.ApplyT(func(v NamespacedGkeDeploymentTarget) *string { return v.ClusterNamespace }).(pulumi.StringPtrOutput)
}

// Optional. The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
func (o NamespacedGkeDeploymentTargetOutput) TargetGkeCluster() pulumi.StringPtrOutput {
	return o.ApplyT(func(v NamespacedGkeDeploymentTarget) *string { return v.TargetGkeCluster }).(pulumi.StringPtrOutput)
}

type NamespacedGkeDeploymentTargetPtrOutput struct{ *pulumi.OutputState }

func (NamespacedGkeDeploymentTargetPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**NamespacedGkeDeploymentTarget)(nil)).Elem()
}

func (o NamespacedGkeDeploymentTargetPtrOutput) ToNamespacedGkeDeploymentTargetPtrOutput() NamespacedGkeDeploymentTargetPtrOutput {
	return o
}

func (o NamespacedGkeDeploymentTargetPtrOutput) ToNamespacedGkeDeploymentTargetPtrOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetPtrOutput {
	return o
}

func (o NamespacedGkeDeploymentTargetPtrOutput) Elem() NamespacedGkeDeploymentTargetOutput {
	return o.ApplyT(func(v *NamespacedGkeDeploymentTarget) NamespacedGkeDeploymentTarget {
		if v != nil {
			return *v
		}
		var ret NamespacedGkeDeploymentTarget
		return ret
	}).(NamespacedGkeDeploymentTargetOutput)
}

// Optional. A namespace within the GKE cluster to deploy into.
func (o NamespacedGkeDeploymentTargetPtrOutput) ClusterNamespace() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *NamespacedGkeDeploymentTarget) *string {
		if v == nil {
			return nil
		}
		return v.ClusterNamespace
	}).(pulumi.StringPtrOutput)
}

// Optional. The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
func (o NamespacedGkeDeploymentTargetPtrOutput) TargetGkeCluster() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *NamespacedGkeDeploymentTarget) *string {
		if v == nil {
			return nil
		}
		return v.TargetGkeCluster
	}).(pulumi.StringPtrOutput)
}

// Deprecated. Used only for the deprecated beta. A full, namespace-isolated deployment target for an existing GKE cluster.
type NamespacedGkeDeploymentTargetResponse struct {
	// Optional. A namespace within the GKE cluster to deploy into.
	ClusterNamespace string `pulumi:"clusterNamespace"`
	// Optional. The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
	TargetGkeCluster string `pulumi:"targetGkeCluster"`
}

// Deprecated. Used only for the deprecated beta. A full, namespace-isolated deployment target for an existing GKE cluster.
type NamespacedGkeDeploymentTargetResponseOutput struct{ *pulumi.OutputState }

func (NamespacedGkeDeploymentTargetResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NamespacedGkeDeploymentTargetResponse)(nil)).Elem()
}

func (o NamespacedGkeDeploymentTargetResponseOutput) ToNamespacedGkeDeploymentTargetResponseOutput() NamespacedGkeDeploymentTargetResponseOutput {
	return o
}

func (o NamespacedGkeDeploymentTargetResponseOutput) ToNamespacedGkeDeploymentTargetResponseOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetResponseOutput {
	return o
}

// Optional. A namespace within the GKE cluster to deploy into.
func (o NamespacedGkeDeploymentTargetResponseOutput) ClusterNamespace() pulumi.StringOutput {
	return o.ApplyT(func(v NamespacedGkeDeploymentTargetResponse) string { return v.ClusterNamespace }).(pulumi.StringOutput)
}

// Optional. The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
func (o NamespacedGkeDeploymentTargetResponseOutput) TargetGkeCluster() pulumi.StringOutput {
	return o.ApplyT(func(v NamespacedGkeDeploymentTargetResponse) string { return v.TargetGkeCluster }).(pulumi.StringOutput)
}

// Dataproc Node Group. The Dataproc NodeGroup resource is not related to the Dataproc NodeGroupAffinity resource.
type NodeGroupType struct {
	// Optional. Node group labels. Label keys must consist of from 1 to 63 characters and conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values can be empty. If specified, they must consist of from 1 to 63 characters and conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). The node group must have no more than 32 labelsn.
	Labels map[string]string `pulumi:"labels"`
	// The Node group resource name (https://aip.dev/122).
	Name *string `pulumi:"name"`
	// Optional. The node group instance group configuration.
	NodeGroupConfig *InstanceGroupConfig `pulumi:"nodeGroupConfig"`
	// Node group roles.
	Roles []NodeGroupRolesItem `pulumi:"roles"`
}

// NodeGroupTypeInput is an input type that accepts NodeGroupTypeArgs and NodeGroupTypeOutput values.
// You can construct a concrete instance of `NodeGroupTypeInput` via:
//
//	NodeGroupTypeArgs{...}
type NodeGroupTypeInput interface {
	pulumi.Input

	ToNodeGroupTypeOutput() NodeGroupTypeOutput
	ToNodeGroupTypeOutputWithContext(context.Context) NodeGroupTypeOutput
}

// Dataproc Node Group. The Dataproc NodeGroup resource is not related to the Dataproc NodeGroupAffinity resource.
type NodeGroupTypeArgs struct {
	// Optional. Node group labels. Label keys must consist of from 1 to 63 characters and conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values can be empty. If specified, they must consist of from 1 to 63 characters and conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). The node group must have no more than 32 labelsn.
	Labels pulumi.StringMapInput `pulumi:"labels"`
	// The Node group resource name (https://aip.dev/122).
	Name pulumi.StringPtrInput `pulumi:"name"`
	// Optional. The node group instance group configuration.
	NodeGroupConfig InstanceGroupConfigPtrInput `pulumi:"nodeGroupConfig"`
	// Node group roles.
	Roles NodeGroupRolesItemArrayInput `pulumi:"roles"`
}

func (NodeGroupTypeArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeGroupType)(nil)).Elem()
}

func (i NodeGroupTypeArgs) ToNodeGroupTypeOutput() NodeGroupTypeOutput {
	return i.ToNodeGroupTypeOutputWithContext(context.Background())
}

func (i NodeGroupTypeArgs) ToNodeGroupTypeOutputWithContext(ctx context.Context) NodeGroupTypeOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodeGroupTypeOutput)
}

// Dataproc Node Group. The Dataproc NodeGroup resource is not related to the Dataproc NodeGroupAffinity resource.
type NodeGroupTypeOutput struct{ *pulumi.OutputState }

func (NodeGroupTypeOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeGroupType)(nil)).Elem()
}

func (o NodeGroupTypeOutput) ToNodeGroupTypeOutput() NodeGroupTypeOutput {
	return o
}

func (o NodeGroupTypeOutput) ToNodeGroupTypeOutputWithContext(ctx context.Context) NodeGroupTypeOutput {
	return o
}

// Optional. Node group labels. Label keys must consist of from 1 to 63 characters and conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values can be empty. If specified, they must consist of from 1 to 63 characters and conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). The node group must have no more than 32 labelsn.
func (o NodeGroupTypeOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v NodeGroupType) map[string]string { return v.Labels }).(pulumi.StringMapOutput)
}

// The Node group resource name (https://aip.dev/122).
func (o NodeGroupTypeOutput) Name() pulumi.StringPtrOutput {
	return o.ApplyT(func(v NodeGroupType) *string { return v.Name }).(pulumi.StringPtrOutput)
}

// Optional. The node group instance group configuration.
func (o NodeGroupTypeOutput) NodeGroupConfig() InstanceGroupConfigPtrOutput {
	return o.ApplyT(func(v NodeGroupType) *InstanceGroupConfig { return v.NodeGroupConfig }).(InstanceGroupConfigPtrOutput)
}

// Node group roles.
func (o NodeGroupTypeOutput) Roles() NodeGroupRolesItemArrayOutput {
	return o.ApplyT(func(v NodeGroupType) []NodeGroupRolesItem { return v.Roles }).(NodeGroupRolesItemArrayOutput)
}

// Node Group Affinity for clusters using sole-tenant node groups. The Dataproc NodeGroupAffinity resource is not related to the Dataproc NodeGroup resource.
type NodeGroupAffinity struct {
	// The URI of a sole-tenant node group resource (https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups) that the cluster will be created on.A full URL, partial URI, or node group name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/nodeGroups/node-group-1 projects/[project_id]/zones/[zone]/nodeGroups/node-group-1 node-group-1
	NodeGroupUri string `pulumi:"nodeGroupUri"`
}

// NodeGroupAffinityInput is an input type that accepts NodeGroupAffinityArgs and NodeGroupAffinityOutput values.
// You can construct a concrete instance of `NodeGroupAffinityInput` via:
//
//	NodeGroupAffinityArgs{...}
type NodeGroupAffinityInput interface {
	pulumi.Input

	ToNodeGroupAffinityOutput() NodeGroupAffinityOutput
	ToNodeGroupAffinityOutputWithContext(context.Context) NodeGroupAffinityOutput
}

// Node Group Affinity for clusters using sole-tenant node groups. The Dataproc NodeGroupAffinity resource is not related to the Dataproc NodeGroup resource.
type NodeGroupAffinityArgs struct {
	// The URI of a sole-tenant node group resource (https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups) that the cluster will be created on.A full URL, partial URI, or node group name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/nodeGroups/node-group-1 projects/[project_id]/zones/[zone]/nodeGroups/node-group-1 node-group-1
	NodeGroupUri pulumi.StringInput `pulumi:"nodeGroupUri"`
}

func (NodeGroupAffinityArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeGroupAffinity)(nil)).Elem()
}

func (i NodeGroupAffinityArgs) ToNodeGroupAffinityOutput() NodeGroupAffinityOutput {
	return i.ToNodeGroupAffinityOutputWithContext(context.Background())
}

func (i NodeGroupAffinityArgs) ToNodeGroupAffinityOutputWithContext(ctx context.Context) NodeGroupAffinityOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodeGroupAffinityOutput)
}

func (i NodeGroupAffinityArgs) ToNodeGroupAffinityPtrOutput() NodeGroupAffinityPtrOutput {
	return i.ToNodeGroupAffinityPtrOutputWithContext(context.Background())
}

func (i NodeGroupAffinityArgs) ToNodeGroupAffinityPtrOutputWithContext(ctx context.Context) NodeGroupAffinityPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodeGroupAffinityOutput).ToNodeGroupAffinityPtrOutputWithContext(ctx)
}

// NodeGroupAffinityPtrInput is an input type that accepts NodeGroupAffinityArgs, NodeGroupAffinityPtr and NodeGroupAffinityPtrOutput values.
// You can construct a concrete instance of `NodeGroupAffinityPtrInput` via:
//
//	        NodeGroupAffinityArgs{...}
//
//	or:
//
//	        nil
type NodeGroupAffinityPtrInput interface {
	pulumi.Input

	ToNodeGroupAffinityPtrOutput() NodeGroupAffinityPtrOutput
	ToNodeGroupAffinityPtrOutputWithContext(context.Context) NodeGroupAffinityPtrOutput
}

type nodeGroupAffinityPtrType NodeGroupAffinityArgs

func NodeGroupAffinityPtr(v *NodeGroupAffinityArgs) NodeGroupAffinityPtrInput {
	return (*nodeGroupAffinityPtrType)(v)
}

func (*nodeGroupAffinityPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**NodeGroupAffinity)(nil)).Elem()
}

func (i *nodeGroupAffinityPtrType) ToNodeGroupAffinityPtrOutput() NodeGroupAffinityPtrOutput {
	return i.ToNodeGroupAffinityPtrOutputWithContext(context.Background())
}

func (i *nodeGroupAffinityPtrType) ToNodeGroupAffinityPtrOutputWithContext(ctx context.Context) NodeGroupAffinityPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodeGroupAffinityPtrOutput)
}

// Node Group Affinity for clusters using sole-tenant node groups. The Dataproc NodeGroupAffinity resource is not related to the Dataproc NodeGroup resource.
type NodeGroupAffinityOutput struct{ *pulumi.OutputState }

func (NodeGroupAffinityOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeGroupAffinity)(nil)).Elem()
}

func (o NodeGroupAffinityOutput) ToNodeGroupAffinityOutput() NodeGroupAffinityOutput {
	return o
}

func (o NodeGroupAffinityOutput) ToNodeGroupAffinityOutputWithContext(ctx context.Context) NodeGroupAffinityOutput {
	return o
}

func (o NodeGroupAffinityOutput) ToNodeGroupAffinityPtrOutput() NodeGroupAffinityPtrOutput {
	return o.ToNodeGroupAffinityPtrOutputWithContext(context.Background())
}

func (o NodeGroupAffinityOutput) ToNodeGroupAffinityPtrOutputWithContext(ctx context.Context) NodeGroupAffinityPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v NodeGroupAffinity) *NodeGroupAffinity {
		return &v
	}).(NodeGroupAffinityPtrOutput)
}

// The URI of a sole-tenant node group resource (https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups) that the cluster will be created on.A full URL, partial URI, or node group name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/nodeGroups/node-group-1 projects/[project_id]/zones/[zone]/nodeGroups/node-group-1 node-group-1
func (o NodeGroupAffinityOutput) NodeGroupUri() pulumi.StringOutput {
	return o.ApplyT(func(v NodeGroupAffinity) string { return v.NodeGroupUri }).(pulumi.StringOutput)
}

type NodeGroupAffinityPtrOutput struct{ *pulumi.OutputState }

func (NodeGroupAffinityPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**NodeGroupAffinity)(nil)).Elem()
}

func (o NodeGroupAffinityPtrOutput) ToNodeGroupAffinityPtrOutput() NodeGroupAffinityPtrOutput {
	return o
}

func (o NodeGroupAffinityPtrOutput) ToNodeGroupAffinityPtrOutputWithContext(ctx context.Context) NodeGroupAffinityPtrOutput {
	return o
}

func (o NodeGroupAffinityPtrOutput) Elem() NodeGroupAffinityOutput {
	return o.ApplyT(func(v *NodeGroupAffinity) NodeGroupAffinity {
		if v != nil {
			return *v
		}
		var ret NodeGroupAffinity
		return ret
	}).(NodeGroupAffinityOutput)
}

// The URI of a sole-tenant node group resource (https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups) that the cluster will be created on.A full URL, partial URI, or node group name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/nodeGroups/node-group-1 projects/[project_id]/zones/[zone]/nodeGroups/node-group-1 node-group-1
func (o NodeGroupAffinityPtrOutput) NodeGroupUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *NodeGroupAffinity) *string {
		if v == nil {
			return nil
		}
		return &v.NodeGroupUri
	}).(pulumi.StringPtrOutput)
}

// Node Group Affinity for clusters using sole-tenant node groups. The Dataproc NodeGroupAffinity resource is not related to the Dataproc NodeGroup resource.
type NodeGroupAffinityResponse struct {
	// The URI of a sole-tenant node group resource (https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups) that the cluster will be created on.A full URL, partial URI, or node group name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/nodeGroups/node-group-1 projects/[project_id]/zones/[zone]/nodeGroups/node-group-1 node-group-1
	NodeGroupUri string `pulumi:"nodeGroupUri"`
}

// Node Group Affinity for clusters using sole-tenant node groups. The Dataproc NodeGroupAffinity resource is not related to the Dataproc NodeGroup resource.
type NodeGroupAffinityResponseOutput struct{ *pulumi.OutputState }

func (NodeGroupAffinityResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeGroupAffinityResponse)(nil)).Elem()
}

func (o NodeGroupAffinityResponseOutput) ToNodeGroupAffinityResponseOutput() NodeGroupAffinityResponseOutput {
	return o
}

func (o NodeGroupAffinityResponseOutput) ToNodeGroupAffinityResponseOutputWithContext(ctx context.Context) NodeGroupAffinityResponseOutput {
	return o
}

// The URI of a sole-tenant node group resource (https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups) that the cluster will be created on.A full URL, partial URI, or node group name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]/nodeGroups/node-group-1 projects/[project_id]/zones/[zone]/nodeGroups/node-group-1 node-group-1
func (o NodeGroupAffinityResponseOutput) NodeGroupUri() pulumi.StringOutput {
	return o.ApplyT(func(v NodeGroupAffinityResponse) string { return v.NodeGroupUri }).(pulumi.StringOutput)
}

// Dataproc Node Group. The Dataproc NodeGroup resource is not related to the Dataproc NodeGroupAffinity resource.
type NodeGroupResponse struct {
	// Optional. Node group labels. Label keys must consist of from 1 to 63 characters and conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values can be empty. If specified, they must consist of from 1 to 63 characters and conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). The node group must have no more than 32 labelsn.
	Labels map[string]string `pulumi:"labels"`
	// The Node group resource name (https://aip.dev/122).
	Name string `pulumi:"name"`
	// Optional. The node group instance group configuration.
	NodeGroupConfig InstanceGroupConfigResponse `pulumi:"nodeGroupConfig"`
	// Node group roles.
	Roles []string `pulumi:"roles"`
}

// Dataproc Node Group. The Dataproc NodeGroup resource is not related to the Dataproc NodeGroupAffinity resource.
type NodeGroupResponseOutput struct{ *pulumi.OutputState }

func (NodeGroupResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeGroupResponse)(nil)).Elem()
}

func (o NodeGroupResponseOutput) ToNodeGroupResponseOutput() NodeGroupResponseOutput {
	return o
}

func (o NodeGroupResponseOutput) ToNodeGroupResponseOutputWithContext(ctx context.Context) NodeGroupResponseOutput {
	return o
}

// Optional. Node group labels. Label keys must consist of from 1 to 63 characters and conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values can be empty. If specified, they must consist of from 1 to 63 characters and conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). The node group must have no more than 32 labelsn.
func (o NodeGroupResponseOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v NodeGroupResponse) map[string]string { return v.Labels }).(pulumi.StringMapOutput)
}

// The Node group resource name (https://aip.dev/122).
func (o NodeGroupResponseOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v NodeGroupResponse) string { return v.Name }).(pulumi.StringOutput)
}

// Optional. The node group instance group configuration.
func (o NodeGroupResponseOutput) NodeGroupConfig() InstanceGroupConfigResponseOutput {
	return o.ApplyT(func(v NodeGroupResponse) InstanceGroupConfigResponse { return v.NodeGroupConfig }).(InstanceGroupConfigResponseOutput)
}

// Node group roles.
func (o NodeGroupResponseOutput) Roles() pulumi.StringArrayOutput {
	return o.ApplyT(func(v NodeGroupResponse) []string { return v.Roles }).(pulumi.StringArrayOutput)
}

// Specifies an executable to run on a fully configured node and a timeout period for executable completion.
type NodeInitializationAction struct {
	// Cloud Storage URI of executable file.
	ExecutableFile string `pulumi:"executableFile"`
	// Optional. Amount of time executable has to complete. Default is 10 minutes (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
	ExecutionTimeout *string `pulumi:"executionTimeout"`
}

// NodeInitializationActionInput is an input type that accepts NodeInitializationActionArgs and NodeInitializationActionOutput values.
// You can construct a concrete instance of `NodeInitializationActionInput` via:
//
//	NodeInitializationActionArgs{...}
type NodeInitializationActionInput interface {
	pulumi.Input

	ToNodeInitializationActionOutput() NodeInitializationActionOutput
	ToNodeInitializationActionOutputWithContext(context.Context) NodeInitializationActionOutput
}

// Specifies an executable to run on a fully configured node and a timeout period for executable completion.
type NodeInitializationActionArgs struct {
	// Cloud Storage URI of executable file.
	ExecutableFile pulumi.StringInput `pulumi:"executableFile"`
	// Optional. Amount of time executable has to complete. Default is 10 minutes (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
	ExecutionTimeout pulumi.StringPtrInput `pulumi:"executionTimeout"`
}

func (NodeInitializationActionArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeInitializationAction)(nil)).Elem()
}

func (i NodeInitializationActionArgs) ToNodeInitializationActionOutput() NodeInitializationActionOutput {
	return i.ToNodeInitializationActionOutputWithContext(context.Background())
}

func (i NodeInitializationActionArgs) ToNodeInitializationActionOutputWithContext(ctx context.Context) NodeInitializationActionOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodeInitializationActionOutput)
}

// NodeInitializationActionArrayInput is an input type that accepts NodeInitializationActionArray and NodeInitializationActionArrayOutput values.
// You can construct a concrete instance of `NodeInitializationActionArrayInput` via:
//
//	NodeInitializationActionArray{ NodeInitializationActionArgs{...} }
type NodeInitializationActionArrayInput interface {
	pulumi.Input

	ToNodeInitializationActionArrayOutput() NodeInitializationActionArrayOutput
	ToNodeInitializationActionArrayOutputWithContext(context.Context) NodeInitializationActionArrayOutput
}

type NodeInitializationActionArray []NodeInitializationActionInput

func (NodeInitializationActionArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]NodeInitializationAction)(nil)).Elem()
}

func (i NodeInitializationActionArray) ToNodeInitializationActionArrayOutput() NodeInitializationActionArrayOutput {
	return i.ToNodeInitializationActionArrayOutputWithContext(context.Background())
}

func (i NodeInitializationActionArray) ToNodeInitializationActionArrayOutputWithContext(ctx context.Context) NodeInitializationActionArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodeInitializationActionArrayOutput)
}

// Specifies an executable to run on a fully configured node and a timeout period for executable completion.
type NodeInitializationActionOutput struct{ *pulumi.OutputState }

func (NodeInitializationActionOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeInitializationAction)(nil)).Elem()
}

func (o NodeInitializationActionOutput) ToNodeInitializationActionOutput() NodeInitializationActionOutput {
	return o
}

func (o NodeInitializationActionOutput) ToNodeInitializationActionOutputWithContext(ctx context.Context) NodeInitializationActionOutput {
	return o
}

// Cloud Storage URI of executable file.
func (o NodeInitializationActionOutput) ExecutableFile() pulumi.StringOutput {
	return o.ApplyT(func(v NodeInitializationAction) string { return v.ExecutableFile }).(pulumi.StringOutput)
}

// Optional. Amount of time executable has to complete. Default is 10 minutes (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
func (o NodeInitializationActionOutput) ExecutionTimeout() pulumi.StringPtrOutput {
	return o.ApplyT(func(v NodeInitializationAction) *string { return v.ExecutionTimeout }).(pulumi.StringPtrOutput)
}

type NodeInitializationActionArrayOutput struct{ *pulumi.OutputState }

func (NodeInitializationActionArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]NodeInitializationAction)(nil)).Elem()
}

func (o NodeInitializationActionArrayOutput) ToNodeInitializationActionArrayOutput() NodeInitializationActionArrayOutput {
	return o
}

func (o NodeInitializationActionArrayOutput) ToNodeInitializationActionArrayOutputWithContext(ctx context.Context) NodeInitializationActionArrayOutput {
	return o
}

func (o NodeInitializationActionArrayOutput) Index(i pulumi.IntInput) NodeInitializationActionOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) NodeInitializationAction {
		return vs[0].([]NodeInitializationAction)[vs[1].(int)]
	}).(NodeInitializationActionOutput)
}

// Specifies an executable to run on a fully configured node and a timeout period for executable completion.
type NodeInitializationActionResponse struct {
	// Cloud Storage URI of executable file.
	ExecutableFile string `pulumi:"executableFile"`
	// Optional. Amount of time executable has to complete. Default is 10 minutes (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
	ExecutionTimeout string `pulumi:"executionTimeout"`
}

// Specifies an executable to run on a fully configured node and a timeout period for executable completion.
type NodeInitializationActionResponseOutput struct{ *pulumi.OutputState }

func (NodeInitializationActionResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeInitializationActionResponse)(nil)).Elem()
}

func (o NodeInitializationActionResponseOutput) ToNodeInitializationActionResponseOutput() NodeInitializationActionResponseOutput {
	return o
}

func (o NodeInitializationActionResponseOutput) ToNodeInitializationActionResponseOutputWithContext(ctx context.Context) NodeInitializationActionResponseOutput {
	return o
}

// Cloud Storage URI of executable file.
func (o NodeInitializationActionResponseOutput) ExecutableFile() pulumi.StringOutput {
	return o.ApplyT(func(v NodeInitializationActionResponse) string { return v.ExecutableFile }).(pulumi.StringOutput)
}

// Optional. Amount of time executable has to complete. Default is 10 minutes (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
func (o NodeInitializationActionResponseOutput) ExecutionTimeout() pulumi.StringOutput {
	return o.ApplyT(func(v NodeInitializationActionResponse) string { return v.ExecutionTimeout }).(pulumi.StringOutput)
}

type NodeInitializationActionResponseArrayOutput struct{ *pulumi.OutputState }

func (NodeInitializationActionResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]NodeInitializationActionResponse)(nil)).Elem()
}

func (o NodeInitializationActionResponseArrayOutput) ToNodeInitializationActionResponseArrayOutput() NodeInitializationActionResponseArrayOutput {
	return o
}

func (o NodeInitializationActionResponseArrayOutput) ToNodeInitializationActionResponseArrayOutputWithContext(ctx context.Context) NodeInitializationActionResponseArrayOutput {
	return o
}

func (o NodeInitializationActionResponseArrayOutput) Index(i pulumi.IntInput) NodeInitializationActionResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) NodeInitializationActionResponse {
		return vs[0].([]NodeInitializationActionResponse)[vs[1].(int)]
	}).(NodeInitializationActionResponseOutput)
}

// A job executed by the workflow.
type OrderedJob struct {
	// Optional. Job is a Hadoop job.
	HadoopJob *HadoopJob `pulumi:"hadoopJob"`
	// Optional. Job is a Hive job.
	HiveJob *HiveJob `pulumi:"hiveJob"`
	// Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
	Labels map[string]string `pulumi:"labels"`
	// Optional. Job is a Pig job.
	PigJob *PigJob `pulumi:"pigJob"`
	// Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
	PrerequisiteStepIds []string `pulumi:"prerequisiteStepIds"`
	// Optional. Job is a Presto job.
	PrestoJob *PrestoJob `pulumi:"prestoJob"`
	// Optional. Job is a PySpark job.
	PysparkJob *PySparkJob `pulumi:"pysparkJob"`
	// Optional. Job scheduling configuration.
	Scheduling *JobScheduling `pulumi:"scheduling"`
	// Optional. Job is a Spark job.
	SparkJob *SparkJob `pulumi:"sparkJob"`
	// Optional. Job is a SparkR job.
	SparkRJob *SparkRJob `pulumi:"sparkRJob"`
	// Optional. Job is a SparkSql job.
	SparkSqlJob *SparkSqlJob `pulumi:"sparkSqlJob"`
	// The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
	StepId string `pulumi:"stepId"`
	// Optional. Job is a Trino job.
	TrinoJob *TrinoJob `pulumi:"trinoJob"`
}

// OrderedJobInput is an input type that accepts OrderedJobArgs and OrderedJobOutput values.
// You can construct a concrete instance of `OrderedJobInput` via:
//
//	OrderedJobArgs{...}
type OrderedJobInput interface {
	pulumi.Input

	ToOrderedJobOutput() OrderedJobOutput
	ToOrderedJobOutputWithContext(context.Context) OrderedJobOutput
}

// A job executed by the workflow.
type OrderedJobArgs struct {
	// Optional. Job is a Hadoop job.
	HadoopJob HadoopJobPtrInput `pulumi:"hadoopJob"`
	// Optional. Job is a Hive job.
	HiveJob HiveJobPtrInput `pulumi:"hiveJob"`
	// Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
	Labels pulumi.StringMapInput `pulumi:"labels"`
	// Optional. Job is a Pig job.
	PigJob PigJobPtrInput `pulumi:"pigJob"`
	// Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
	PrerequisiteStepIds pulumi.StringArrayInput `pulumi:"prerequisiteStepIds"`
	// Optional. Job is a Presto job.
	PrestoJob PrestoJobPtrInput `pulumi:"prestoJob"`
	// Optional. Job is a PySpark job.
	PysparkJob PySparkJobPtrInput `pulumi:"pysparkJob"`
	// Optional. Job scheduling configuration.
	Scheduling JobSchedulingPtrInput `pulumi:"scheduling"`
	// Optional. Job is a Spark job.
	SparkJob SparkJobPtrInput `pulumi:"sparkJob"`
	// Optional. Job is a SparkR job.
	SparkRJob SparkRJobPtrInput `pulumi:"sparkRJob"`
	// Optional. Job is a SparkSql job.
	SparkSqlJob SparkSqlJobPtrInput `pulumi:"sparkSqlJob"`
	// The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
	StepId pulumi.StringInput `pulumi:"stepId"`
	// Optional. Job is a Trino job.
	TrinoJob TrinoJobPtrInput `pulumi:"trinoJob"`
}

func (OrderedJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*OrderedJob)(nil)).Elem()
}

func (i OrderedJobArgs) ToOrderedJobOutput() OrderedJobOutput {
	return i.ToOrderedJobOutputWithContext(context.Background())
}

func (i OrderedJobArgs) ToOrderedJobOutputWithContext(ctx context.Context) OrderedJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(OrderedJobOutput)
}

// OrderedJobArrayInput is an input type that accepts OrderedJobArray and OrderedJobArrayOutput values.
// You can construct a concrete instance of `OrderedJobArrayInput` via:
//
//	OrderedJobArray{ OrderedJobArgs{...} }
type OrderedJobArrayInput interface {
	pulumi.Input

	ToOrderedJobArrayOutput() OrderedJobArrayOutput
	ToOrderedJobArrayOutputWithContext(context.Context) OrderedJobArrayOutput
}

type OrderedJobArray []OrderedJobInput

func (OrderedJobArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]OrderedJob)(nil)).Elem()
}

func (i OrderedJobArray) ToOrderedJobArrayOutput() OrderedJobArrayOutput {
	return i.ToOrderedJobArrayOutputWithContext(context.Background())
}

func (i OrderedJobArray) ToOrderedJobArrayOutputWithContext(ctx context.Context) OrderedJobArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(OrderedJobArrayOutput)
}

// A job executed by the workflow.
type OrderedJobOutput struct{ *pulumi.OutputState }

func (OrderedJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*OrderedJob)(nil)).Elem()
}

func (o OrderedJobOutput) ToOrderedJobOutput() OrderedJobOutput {
	return o
}

func (o OrderedJobOutput) ToOrderedJobOutputWithContext(ctx context.Context) OrderedJobOutput {
	return o
}

// Optional. Job is a Hadoop job.
func (o OrderedJobOutput) HadoopJob() HadoopJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *HadoopJob { return v.HadoopJob }).(HadoopJobPtrOutput)
}

// Optional. Job is a Hive job.
func (o OrderedJobOutput) HiveJob() HiveJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *HiveJob { return v.HiveJob }).(HiveJobPtrOutput)
}

// Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
func (o OrderedJobOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v OrderedJob) map[string]string { return v.Labels }).(pulumi.StringMapOutput)
}

// Optional. Job is a Pig job.
func (o OrderedJobOutput) PigJob() PigJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *PigJob { return v.PigJob }).(PigJobPtrOutput)
}

// Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
func (o OrderedJobOutput) PrerequisiteStepIds() pulumi.StringArrayOutput {
	return o.ApplyT(func(v OrderedJob) []string { return v.PrerequisiteStepIds }).(pulumi.StringArrayOutput)
}

// Optional. Job is a Presto job.
func (o OrderedJobOutput) PrestoJob() PrestoJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *PrestoJob { return v.PrestoJob }).(PrestoJobPtrOutput)
}

// Optional. Job is a PySpark job.
func (o OrderedJobOutput) PysparkJob() PySparkJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *PySparkJob { return v.PysparkJob }).(PySparkJobPtrOutput)
}

// Optional. Job scheduling configuration.
func (o OrderedJobOutput) Scheduling() JobSchedulingPtrOutput {
	return o.ApplyT(func(v OrderedJob) *JobScheduling { return v.Scheduling }).(JobSchedulingPtrOutput)
}

// Optional. Job is a Spark job.
func (o OrderedJobOutput) SparkJob() SparkJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *SparkJob { return v.SparkJob }).(SparkJobPtrOutput)
}

// Optional. Job is a SparkR job.
func (o OrderedJobOutput) SparkRJob() SparkRJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *SparkRJob { return v.SparkRJob }).(SparkRJobPtrOutput)
}

// Optional. Job is a SparkSql job.
func (o OrderedJobOutput) SparkSqlJob() SparkSqlJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *SparkSqlJob { return v.SparkSqlJob }).(SparkSqlJobPtrOutput)
}

// The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
func (o OrderedJobOutput) StepId() pulumi.StringOutput {
	return o.ApplyT(func(v OrderedJob) string { return v.StepId }).(pulumi.StringOutput)
}

// Optional. Job is a Trino job.
func (o OrderedJobOutput) TrinoJob() TrinoJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *TrinoJob { return v.TrinoJob }).(TrinoJobPtrOutput)
}

type OrderedJobArrayOutput struct{ *pulumi.OutputState }

func (OrderedJobArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]OrderedJob)(nil)).Elem()
}

func (o OrderedJobArrayOutput) ToOrderedJobArrayOutput() OrderedJobArrayOutput {
	return o
}

func (o OrderedJobArrayOutput) ToOrderedJobArrayOutputWithContext(ctx context.Context) OrderedJobArrayOutput {
	return o
}

func (o OrderedJobArrayOutput) Index(i pulumi.IntInput) OrderedJobOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) OrderedJob {
		return vs[0].([]OrderedJob)[vs[1].(int)]
	}).(OrderedJobOutput)
}

// A job executed by the workflow.
type OrderedJobResponse struct {
	// Optional. Job is a Hadoop job.
	HadoopJob HadoopJobResponse `pulumi:"hadoopJob"`
	// Optional. Job is a Hive job.
	HiveJob HiveJobResponse `pulumi:"hiveJob"`
	// Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
	Labels map[string]string `pulumi:"labels"`
	// Optional. Job is a Pig job.
	PigJob PigJobResponse `pulumi:"pigJob"`
	// Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
	PrerequisiteStepIds []string `pulumi:"prerequisiteStepIds"`
	// Optional. Job is a Presto job.
	PrestoJob PrestoJobResponse `pulumi:"prestoJob"`
	// Optional. Job is a PySpark job.
	PysparkJob PySparkJobResponse `pulumi:"pysparkJob"`
	// Optional. Job scheduling configuration.
	Scheduling JobSchedulingResponse `pulumi:"scheduling"`
	// Optional. Job is a Spark job.
	SparkJob SparkJobResponse `pulumi:"sparkJob"`
	// Optional. Job is a SparkR job.
	SparkRJob SparkRJobResponse `pulumi:"sparkRJob"`
	// Optional. Job is a SparkSql job.
	SparkSqlJob SparkSqlJobResponse `pulumi:"sparkSqlJob"`
	// The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
	StepId string `pulumi:"stepId"`
	// Optional. Job is a Trino job.
	TrinoJob TrinoJobResponse `pulumi:"trinoJob"`
}

// A job executed by the workflow.
type OrderedJobResponseOutput struct{ *pulumi.OutputState }

func (OrderedJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*OrderedJobResponse)(nil)).Elem()
}

func (o OrderedJobResponseOutput) ToOrderedJobResponseOutput() OrderedJobResponseOutput {
	return o
}

func (o OrderedJobResponseOutput) ToOrderedJobResponseOutputWithContext(ctx context.Context) OrderedJobResponseOutput {
	return o
}

// Optional. Job is a Hadoop job.
func (o OrderedJobResponseOutput) HadoopJob() HadoopJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) HadoopJobResponse { return v.HadoopJob }).(HadoopJobResponseOutput)
}

// Optional. Job is a Hive job.
func (o OrderedJobResponseOutput) HiveJob() HiveJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) HiveJobResponse { return v.HiveJob }).(HiveJobResponseOutput)
}

// Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
func (o OrderedJobResponseOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v OrderedJobResponse) map[string]string { return v.Labels }).(pulumi.StringMapOutput)
}

// Optional. Job is a Pig job.
func (o OrderedJobResponseOutput) PigJob() PigJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) PigJobResponse { return v.PigJob }).(PigJobResponseOutput)
}

// Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
func (o OrderedJobResponseOutput) PrerequisiteStepIds() pulumi.StringArrayOutput {
	return o.ApplyT(func(v OrderedJobResponse) []string { return v.PrerequisiteStepIds }).(pulumi.StringArrayOutput)
}

// Optional. Job is a Presto job.
func (o OrderedJobResponseOutput) PrestoJob() PrestoJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) PrestoJobResponse { return v.PrestoJob }).(PrestoJobResponseOutput)
}

// Optional. Job is a PySpark job.
func (o OrderedJobResponseOutput) PysparkJob() PySparkJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) PySparkJobResponse { return v.PysparkJob }).(PySparkJobResponseOutput)
}

// Optional. Job scheduling configuration.
func (o OrderedJobResponseOutput) Scheduling() JobSchedulingResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) JobSchedulingResponse { return v.Scheduling }).(JobSchedulingResponseOutput)
}

// Optional. Job is a Spark job.
func (o OrderedJobResponseOutput) SparkJob() SparkJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) SparkJobResponse { return v.SparkJob }).(SparkJobResponseOutput)
}

// Optional. Job is a SparkR job.
func (o OrderedJobResponseOutput) SparkRJob() SparkRJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) SparkRJobResponse { return v.SparkRJob }).(SparkRJobResponseOutput)
}

// Optional. Job is a SparkSql job.
func (o OrderedJobResponseOutput) SparkSqlJob() SparkSqlJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) SparkSqlJobResponse { return v.SparkSqlJob }).(SparkSqlJobResponseOutput)
}

// The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
func (o OrderedJobResponseOutput) StepId() pulumi.StringOutput {
	return o.ApplyT(func(v OrderedJobResponse) string { return v.StepId }).(pulumi.StringOutput)
}

// Optional. Job is a Trino job.
func (o OrderedJobResponseOutput) TrinoJob() TrinoJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) TrinoJobResponse { return v.TrinoJob }).(TrinoJobResponseOutput)
}

type OrderedJobResponseArrayOutput struct{ *pulumi.OutputState }

func (OrderedJobResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]OrderedJobResponse)(nil)).Elem()
}

func (o OrderedJobResponseArrayOutput) ToOrderedJobResponseArrayOutput() OrderedJobResponseArrayOutput {
	return o
}

func (o OrderedJobResponseArrayOutput) ToOrderedJobResponseArrayOutputWithContext(ctx context.Context) OrderedJobResponseArrayOutput {
	return o
}

func (o OrderedJobResponseArrayOutput) Index(i pulumi.IntInput) OrderedJobResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) OrderedJobResponse {
		return vs[0].([]OrderedJobResponse)[vs[1].(int)]
	}).(OrderedJobResponseOutput)
}

// Configuration for parameter validation.
type ParameterValidation struct {
	// Validation based on regular expressions.
	Regex *RegexValidation `pulumi:"regex"`
	// Validation based on a list of allowed values.
	Values *ValueValidation `pulumi:"values"`
}

// ParameterValidationInput is an input type that accepts ParameterValidationArgs and ParameterValidationOutput values.
// You can construct a concrete instance of `ParameterValidationInput` via:
//
//	ParameterValidationArgs{...}
type ParameterValidationInput interface {
	pulumi.Input

	ToParameterValidationOutput() ParameterValidationOutput
	ToParameterValidationOutputWithContext(context.Context) ParameterValidationOutput
}

// Configuration for parameter validation.
type ParameterValidationArgs struct {
	// Validation based on regular expressions.
	Regex RegexValidationPtrInput `pulumi:"regex"`
	// Validation based on a list of allowed values.
	Values ValueValidationPtrInput `pulumi:"values"`
}

func (ParameterValidationArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ParameterValidation)(nil)).Elem()
}

func (i ParameterValidationArgs) ToParameterValidationOutput() ParameterValidationOutput {
	return i.ToParameterValidationOutputWithContext(context.Background())
}

func (i ParameterValidationArgs) ToParameterValidationOutputWithContext(ctx context.Context) ParameterValidationOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ParameterValidationOutput)
}

func (i ParameterValidationArgs) ToParameterValidationPtrOutput() ParameterValidationPtrOutput {
	return i.ToParameterValidationPtrOutputWithContext(context.Background())
}

func (i ParameterValidationArgs) ToParameterValidationPtrOutputWithContext(ctx context.Context) ParameterValidationPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ParameterValidationOutput).ToParameterValidationPtrOutputWithContext(ctx)
}

// ParameterValidationPtrInput is an input type that accepts ParameterValidationArgs, ParameterValidationPtr and ParameterValidationPtrOutput values.
// You can construct a concrete instance of `ParameterValidationPtrInput` via:
//
//	        ParameterValidationArgs{...}
//
//	or:
//
//	        nil
type ParameterValidationPtrInput interface {
	pulumi.Input

	ToParameterValidationPtrOutput() ParameterValidationPtrOutput
	ToParameterValidationPtrOutputWithContext(context.Context) ParameterValidationPtrOutput
}

type parameterValidationPtrType ParameterValidationArgs

func ParameterValidationPtr(v *ParameterValidationArgs) ParameterValidationPtrInput {
	return (*parameterValidationPtrType)(v)
}

func (*parameterValidationPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ParameterValidation)(nil)).Elem()
}

func (i *parameterValidationPtrType) ToParameterValidationPtrOutput() ParameterValidationPtrOutput {
	return i.ToParameterValidationPtrOutputWithContext(context.Background())
}

func (i *parameterValidationPtrType) ToParameterValidationPtrOutputWithContext(ctx context.Context) ParameterValidationPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ParameterValidationPtrOutput)
}

// Configuration for parameter validation.
type ParameterValidationOutput struct{ *pulumi.OutputState }

func (ParameterValidationOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ParameterValidation)(nil)).Elem()
}

func (o ParameterValidationOutput) ToParameterValidationOutput() ParameterValidationOutput {
	return o
}

func (o ParameterValidationOutput) ToParameterValidationOutputWithContext(ctx context.Context) ParameterValidationOutput {
	return o
}

func (o ParameterValidationOutput) ToParameterValidationPtrOutput() ParameterValidationPtrOutput {
	return o.ToParameterValidationPtrOutputWithContext(context.Background())
}

func (o ParameterValidationOutput) ToParameterValidationPtrOutputWithContext(ctx context.Context) ParameterValidationPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ParameterValidation) *ParameterValidation {
		return &v
	}).(ParameterValidationPtrOutput)
}

// Validation based on regular expressions.
func (o ParameterValidationOutput) Regex() RegexValidationPtrOutput {
	return o.ApplyT(func(v ParameterValidation) *RegexValidation { return v.Regex }).(RegexValidationPtrOutput)
}

// Validation based on a list of allowed values.
func (o ParameterValidationOutput) Values() ValueValidationPtrOutput {
	return o.ApplyT(func(v ParameterValidation) *ValueValidation { return v.Values }).(ValueValidationPtrOutput)
}

type ParameterValidationPtrOutput struct{ *pulumi.OutputState }

func (ParameterValidationPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ParameterValidation)(nil)).Elem()
}

func (o ParameterValidationPtrOutput) ToParameterValidationPtrOutput() ParameterValidationPtrOutput {
	return o
}

func (o ParameterValidationPtrOutput) ToParameterValidationPtrOutputWithContext(ctx context.Context) ParameterValidationPtrOutput {
	return o
}

func (o ParameterValidationPtrOutput) Elem() ParameterValidationOutput {
	return o.ApplyT(func(v *ParameterValidation) ParameterValidation {
		if v != nil {
			return *v
		}
		var ret ParameterValidation
		return ret
	}).(ParameterValidationOutput)
}

// Validation based on regular expressions.
func (o ParameterValidationPtrOutput) Regex() RegexValidationPtrOutput {
	return o.ApplyT(func(v *ParameterValidation) *RegexValidation {
		if v == nil {
			return nil
		}
		return v.Regex
	}).(RegexValidationPtrOutput)
}

// Validation based on a list of allowed values.
func (o ParameterValidationPtrOutput) Values() ValueValidationPtrOutput {
	return o.ApplyT(func(v *ParameterValidation) *ValueValidation {
		if v == nil {
			return nil
		}
		return v.Values
	}).(ValueValidationPtrOutput)
}

// Configuration for parameter validation.
type ParameterValidationResponse struct {
	// Validation based on regular expressions.
	Regex RegexValidationResponse `pulumi:"regex"`
	// Validation based on a list of allowed values.
	Values ValueValidationResponse `pulumi:"values"`
}

// Configuration for parameter validation.
type ParameterValidationResponseOutput struct{ *pulumi.OutputState }

func (ParameterValidationResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ParameterValidationResponse)(nil)).Elem()
}

func (o ParameterValidationResponseOutput) ToParameterValidationResponseOutput() ParameterValidationResponseOutput {
	return o
}

func (o ParameterValidationResponseOutput) ToParameterValidationResponseOutputWithContext(ctx context.Context) ParameterValidationResponseOutput {
	return o
}

// Validation based on regular expressions.
func (o ParameterValidationResponseOutput) Regex() RegexValidationResponseOutput {
	return o.ApplyT(func(v ParameterValidationResponse) RegexValidationResponse { return v.Regex }).(RegexValidationResponseOutput)
}

// Validation based on a list of allowed values.
func (o ParameterValidationResponseOutput) Values() ValueValidationResponseOutput {
	return o.ApplyT(func(v ParameterValidationResponse) ValueValidationResponse { return v.Values }).(ValueValidationResponseOutput)
}

// Auxiliary services configuration for a workload.
type PeripheralsConfig struct {
	// Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
	MetastoreService *string `pulumi:"metastoreService"`
	// Optional. The Spark History Server configuration for the workload.
	SparkHistoryServerConfig *SparkHistoryServerConfig `pulumi:"sparkHistoryServerConfig"`
}

// PeripheralsConfigInput is an input type that accepts PeripheralsConfigArgs and PeripheralsConfigOutput values.
// You can construct a concrete instance of `PeripheralsConfigInput` via:
//
//	PeripheralsConfigArgs{...}
type PeripheralsConfigInput interface {
	pulumi.Input

	ToPeripheralsConfigOutput() PeripheralsConfigOutput
	ToPeripheralsConfigOutputWithContext(context.Context) PeripheralsConfigOutput
}

// Auxiliary services configuration for a workload.
type PeripheralsConfigArgs struct {
	// Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
	MetastoreService pulumi.StringPtrInput `pulumi:"metastoreService"`
	// Optional. The Spark History Server configuration for the workload.
	SparkHistoryServerConfig SparkHistoryServerConfigPtrInput `pulumi:"sparkHistoryServerConfig"`
}

func (PeripheralsConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*PeripheralsConfig)(nil)).Elem()
}

func (i PeripheralsConfigArgs) ToPeripheralsConfigOutput() PeripheralsConfigOutput {
	return i.ToPeripheralsConfigOutputWithContext(context.Background())
}

func (i PeripheralsConfigArgs) ToPeripheralsConfigOutputWithContext(ctx context.Context) PeripheralsConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PeripheralsConfigOutput)
}

func (i PeripheralsConfigArgs) ToPeripheralsConfigPtrOutput() PeripheralsConfigPtrOutput {
	return i.ToPeripheralsConfigPtrOutputWithContext(context.Background())
}

func (i PeripheralsConfigArgs) ToPeripheralsConfigPtrOutputWithContext(ctx context.Context) PeripheralsConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PeripheralsConfigOutput).ToPeripheralsConfigPtrOutputWithContext(ctx)
}

// PeripheralsConfigPtrInput is an input type that accepts PeripheralsConfigArgs, PeripheralsConfigPtr and PeripheralsConfigPtrOutput values.
// You can construct a concrete instance of `PeripheralsConfigPtrInput` via:
//
//	        PeripheralsConfigArgs{...}
//
//	or:
//
//	        nil
type PeripheralsConfigPtrInput interface {
	pulumi.Input

	ToPeripheralsConfigPtrOutput() PeripheralsConfigPtrOutput
	ToPeripheralsConfigPtrOutputWithContext(context.Context) PeripheralsConfigPtrOutput
}

type peripheralsConfigPtrType PeripheralsConfigArgs

func PeripheralsConfigPtr(v *PeripheralsConfigArgs) PeripheralsConfigPtrInput {
	return (*peripheralsConfigPtrType)(v)
}

func (*peripheralsConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**PeripheralsConfig)(nil)).Elem()
}

func (i *peripheralsConfigPtrType) ToPeripheralsConfigPtrOutput() PeripheralsConfigPtrOutput {
	return i.ToPeripheralsConfigPtrOutputWithContext(context.Background())
}

func (i *peripheralsConfigPtrType) ToPeripheralsConfigPtrOutputWithContext(ctx context.Context) PeripheralsConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PeripheralsConfigPtrOutput)
}

// Auxiliary services configuration for a workload.
type PeripheralsConfigOutput struct{ *pulumi.OutputState }

func (PeripheralsConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PeripheralsConfig)(nil)).Elem()
}

func (o PeripheralsConfigOutput) ToPeripheralsConfigOutput() PeripheralsConfigOutput {
	return o
}

func (o PeripheralsConfigOutput) ToPeripheralsConfigOutputWithContext(ctx context.Context) PeripheralsConfigOutput {
	return o
}

func (o PeripheralsConfigOutput) ToPeripheralsConfigPtrOutput() PeripheralsConfigPtrOutput {
	return o.ToPeripheralsConfigPtrOutputWithContext(context.Background())
}

func (o PeripheralsConfigOutput) ToPeripheralsConfigPtrOutputWithContext(ctx context.Context) PeripheralsConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v PeripheralsConfig) *PeripheralsConfig {
		return &v
	}).(PeripheralsConfigPtrOutput)
}

// Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
func (o PeripheralsConfigOutput) MetastoreService() pulumi.StringPtrOutput {
	return o.ApplyT(func(v PeripheralsConfig) *string { return v.MetastoreService }).(pulumi.StringPtrOutput)
}

// Optional. The Spark History Server configuration for the workload.
func (o PeripheralsConfigOutput) SparkHistoryServerConfig() SparkHistoryServerConfigPtrOutput {
	return o.ApplyT(func(v PeripheralsConfig) *SparkHistoryServerConfig { return v.SparkHistoryServerConfig }).(SparkHistoryServerConfigPtrOutput)
}

type PeripheralsConfigPtrOutput struct{ *pulumi.OutputState }

func (PeripheralsConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**PeripheralsConfig)(nil)).Elem()
}

func (o PeripheralsConfigPtrOutput) ToPeripheralsConfigPtrOutput() PeripheralsConfigPtrOutput {
	return o
}

func (o PeripheralsConfigPtrOutput) ToPeripheralsConfigPtrOutputWithContext(ctx context.Context) PeripheralsConfigPtrOutput {
	return o
}

func (o PeripheralsConfigPtrOutput) Elem() PeripheralsConfigOutput {
	return o.ApplyT(func(v *PeripheralsConfig) PeripheralsConfig {
		if v != nil {
			return *v
		}
		var ret PeripheralsConfig
		return ret
	}).(PeripheralsConfigOutput)
}

// Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
func (o PeripheralsConfigPtrOutput) MetastoreService() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *PeripheralsConfig) *string {
		if v == nil {
			return nil
		}
		return v.MetastoreService
	}).(pulumi.StringPtrOutput)
}

// Optional. The Spark History Server configuration for the workload.
func (o PeripheralsConfigPtrOutput) SparkHistoryServerConfig() SparkHistoryServerConfigPtrOutput {
	return o.ApplyT(func(v *PeripheralsConfig) *SparkHistoryServerConfig {
		if v == nil {
			return nil
		}
		return v.SparkHistoryServerConfig
	}).(SparkHistoryServerConfigPtrOutput)
}

// Auxiliary services configuration for a workload.
type PeripheralsConfigResponse struct {
	// Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
	MetastoreService string `pulumi:"metastoreService"`
	// Optional. The Spark History Server configuration for the workload.
	SparkHistoryServerConfig SparkHistoryServerConfigResponse `pulumi:"sparkHistoryServerConfig"`
}

// Auxiliary services configuration for a workload.
type PeripheralsConfigResponseOutput struct{ *pulumi.OutputState }

func (PeripheralsConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PeripheralsConfigResponse)(nil)).Elem()
}

func (o PeripheralsConfigResponseOutput) ToPeripheralsConfigResponseOutput() PeripheralsConfigResponseOutput {
	return o
}

func (o PeripheralsConfigResponseOutput) ToPeripheralsConfigResponseOutputWithContext(ctx context.Context) PeripheralsConfigResponseOutput {
	return o
}

// Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
func (o PeripheralsConfigResponseOutput) MetastoreService() pulumi.StringOutput {
	return o.ApplyT(func(v PeripheralsConfigResponse) string { return v.MetastoreService }).(pulumi.StringOutput)
}

// Optional. The Spark History Server configuration for the workload.
func (o PeripheralsConfigResponseOutput) SparkHistoryServerConfig() SparkHistoryServerConfigResponseOutput {
	return o.ApplyT(func(v PeripheralsConfigResponse) SparkHistoryServerConfigResponse { return v.SparkHistoryServerConfig }).(SparkHistoryServerConfigResponseOutput)
}

// A Dataproc job for running Apache Pig (https://pig.apache.org/) queries on YARN.
type PigJob struct {
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure *bool `pulumi:"continueOnFailure"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains the Pig queries.
	QueryFileUri *string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList *QueryList `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
	ScriptVariables map[string]string `pulumi:"scriptVariables"`
}

// PigJobInput is an input type that accepts PigJobArgs and PigJobOutput values.
// You can construct a concrete instance of `PigJobInput` via:
//
//	PigJobArgs{...}
type PigJobInput interface {
	pulumi.Input

	ToPigJobOutput() PigJobOutput
	ToPigJobOutputWithContext(context.Context) PigJobOutput
}

// A Dataproc job for running Apache Pig (https://pig.apache.org/) queries on YARN.
type PigJobArgs struct {
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure pulumi.BoolPtrInput `pulumi:"continueOnFailure"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
	Properties pulumi.StringMapInput `pulumi:"properties"`
	// The HCFS URI of the script that contains the Pig queries.
	QueryFileUri pulumi.StringPtrInput `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListPtrInput `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
	ScriptVariables pulumi.StringMapInput `pulumi:"scriptVariables"`
}

func (PigJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*PigJob)(nil)).Elem()
}

func (i PigJobArgs) ToPigJobOutput() PigJobOutput {
	return i.ToPigJobOutputWithContext(context.Background())
}

func (i PigJobArgs) ToPigJobOutputWithContext(ctx context.Context) PigJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PigJobOutput)
}

func (i PigJobArgs) ToPigJobPtrOutput() PigJobPtrOutput {
	return i.ToPigJobPtrOutputWithContext(context.Background())
}

func (i PigJobArgs) ToPigJobPtrOutputWithContext(ctx context.Context) PigJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PigJobOutput).ToPigJobPtrOutputWithContext(ctx)
}

// PigJobPtrInput is an input type that accepts PigJobArgs, PigJobPtr and PigJobPtrOutput values.
// You can construct a concrete instance of `PigJobPtrInput` via:
//
//	        PigJobArgs{...}
//
//	or:
//
//	        nil
type PigJobPtrInput interface {
	pulumi.Input

	ToPigJobPtrOutput() PigJobPtrOutput
	ToPigJobPtrOutputWithContext(context.Context) PigJobPtrOutput
}

type pigJobPtrType PigJobArgs

func PigJobPtr(v *PigJobArgs) PigJobPtrInput {
	return (*pigJobPtrType)(v)
}

func (*pigJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**PigJob)(nil)).Elem()
}

func (i *pigJobPtrType) ToPigJobPtrOutput() PigJobPtrOutput {
	return i.ToPigJobPtrOutputWithContext(context.Background())
}

func (i *pigJobPtrType) ToPigJobPtrOutputWithContext(ctx context.Context) PigJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PigJobPtrOutput)
}

// A Dataproc job for running Apache Pig (https://pig.apache.org/) queries on YARN.
type PigJobOutput struct{ *pulumi.OutputState }

func (PigJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PigJob)(nil)).Elem()
}

func (o PigJobOutput) ToPigJobOutput() PigJobOutput {
	return o
}

func (o PigJobOutput) ToPigJobOutputWithContext(ctx context.Context) PigJobOutput {
	return o
}

func (o PigJobOutput) ToPigJobPtrOutput() PigJobPtrOutput {
	return o.ToPigJobPtrOutputWithContext(context.Background())
}

func (o PigJobOutput) ToPigJobPtrOutputWithContext(ctx context.Context) PigJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v PigJob) *PigJob {
		return &v
	}).(PigJobPtrOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o PigJobOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v PigJob) *bool { return v.ContinueOnFailure }).(pulumi.BoolPtrOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
func (o PigJobOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PigJob) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o PigJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v PigJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
func (o PigJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v PigJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains the Pig queries.
func (o PigJobOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v PigJob) *string { return v.QueryFileUri }).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o PigJobOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v PigJob) *QueryList { return v.QueryList }).(QueryListPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
func (o PigJobOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v PigJob) map[string]string { return v.ScriptVariables }).(pulumi.StringMapOutput)
}

type PigJobPtrOutput struct{ *pulumi.OutputState }

func (PigJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**PigJob)(nil)).Elem()
}

func (o PigJobPtrOutput) ToPigJobPtrOutput() PigJobPtrOutput {
	return o
}

func (o PigJobPtrOutput) ToPigJobPtrOutputWithContext(ctx context.Context) PigJobPtrOutput {
	return o
}

func (o PigJobPtrOutput) Elem() PigJobOutput {
	return o.ApplyT(func(v *PigJob) PigJob {
		if v != nil {
			return *v
		}
		var ret PigJob
		return ret
	}).(PigJobOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o PigJobPtrOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *PigJob) *bool {
		if v == nil {
			return nil
		}
		return v.ContinueOnFailure
	}).(pulumi.BoolPtrOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
func (o PigJobPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PigJob) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o PigJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *PigJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
func (o PigJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *PigJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains the Pig queries.
func (o PigJobPtrOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *PigJob) *string {
		if v == nil {
			return nil
		}
		return v.QueryFileUri
	}).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o PigJobPtrOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v *PigJob) *QueryList {
		if v == nil {
			return nil
		}
		return v.QueryList
	}).(QueryListPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
func (o PigJobPtrOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v *PigJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.ScriptVariables
	}).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Pig (https://pig.apache.org/) queries on YARN.
type PigJobResponse struct {
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure bool `pulumi:"continueOnFailure"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains the Pig queries.
	QueryFileUri string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListResponse `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
	ScriptVariables map[string]string `pulumi:"scriptVariables"`
}

// A Dataproc job for running Apache Pig (https://pig.apache.org/) queries on YARN.
type PigJobResponseOutput struct{ *pulumi.OutputState }

func (PigJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PigJobResponse)(nil)).Elem()
}

func (o PigJobResponseOutput) ToPigJobResponseOutput() PigJobResponseOutput {
	return o
}

func (o PigJobResponseOutput) ToPigJobResponseOutputWithContext(ctx context.Context) PigJobResponseOutput {
	return o
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o PigJobResponseOutput) ContinueOnFailure() pulumi.BoolOutput {
	return o.ApplyT(func(v PigJobResponse) bool { return v.ContinueOnFailure }).(pulumi.BoolOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
func (o PigJobResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PigJobResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o PigJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v PigJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
func (o PigJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v PigJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains the Pig queries.
func (o PigJobResponseOutput) QueryFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v PigJobResponse) string { return v.QueryFileUri }).(pulumi.StringOutput)
}

// A list of queries.
func (o PigJobResponseOutput) QueryList() QueryListResponseOutput {
	return o.ApplyT(func(v PigJobResponse) QueryListResponse { return v.QueryList }).(QueryListResponseOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
func (o PigJobResponseOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v PigJobResponse) map[string]string { return v.ScriptVariables }).(pulumi.StringMapOutput)
}

// A Dataproc job for running Presto (https://prestosql.io/) queries. IMPORTANT: The Dataproc Presto Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/presto) must be enabled when the cluster is created to submit a Presto job to the cluster.
type PrestoJob struct {
	// Optional. Presto client tags to attach to this query
	ClientTags []string `pulumi:"clientTags"`
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure *bool `pulumi:"continueOnFailure"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
	OutputFormat *string `pulumi:"outputFormat"`
	// Optional. A mapping of property names to values. Used to set Presto session properties (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri *string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList *QueryList `pulumi:"queryList"`
}

// PrestoJobInput is an input type that accepts PrestoJobArgs and PrestoJobOutput values.
// You can construct a concrete instance of `PrestoJobInput` via:
//
//	PrestoJobArgs{...}
type PrestoJobInput interface {
	pulumi.Input

	ToPrestoJobOutput() PrestoJobOutput
	ToPrestoJobOutputWithContext(context.Context) PrestoJobOutput
}

// A Dataproc job for running Presto (https://prestosql.io/) queries. IMPORTANT: The Dataproc Presto Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/presto) must be enabled when the cluster is created to submit a Presto job to the cluster.
type PrestoJobArgs struct {
	// Optional. Presto client tags to attach to this query
	ClientTags pulumi.StringArrayInput `pulumi:"clientTags"`
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure pulumi.BoolPtrInput `pulumi:"continueOnFailure"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
	OutputFormat pulumi.StringPtrInput `pulumi:"outputFormat"`
	// Optional. A mapping of property names to values. Used to set Presto session properties (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
	Properties pulumi.StringMapInput `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri pulumi.StringPtrInput `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListPtrInput `pulumi:"queryList"`
}

func (PrestoJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*PrestoJob)(nil)).Elem()
}

func (i PrestoJobArgs) ToPrestoJobOutput() PrestoJobOutput {
	return i.ToPrestoJobOutputWithContext(context.Background())
}

func (i PrestoJobArgs) ToPrestoJobOutputWithContext(ctx context.Context) PrestoJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PrestoJobOutput)
}

func (i PrestoJobArgs) ToPrestoJobPtrOutput() PrestoJobPtrOutput {
	return i.ToPrestoJobPtrOutputWithContext(context.Background())
}

func (i PrestoJobArgs) ToPrestoJobPtrOutputWithContext(ctx context.Context) PrestoJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PrestoJobOutput).ToPrestoJobPtrOutputWithContext(ctx)
}

// PrestoJobPtrInput is an input type that accepts PrestoJobArgs, PrestoJobPtr and PrestoJobPtrOutput values.
// You can construct a concrete instance of `PrestoJobPtrInput` via:
//
//	        PrestoJobArgs{...}
//
//	or:
//
//	        nil
type PrestoJobPtrInput interface {
	pulumi.Input

	ToPrestoJobPtrOutput() PrestoJobPtrOutput
	ToPrestoJobPtrOutputWithContext(context.Context) PrestoJobPtrOutput
}

type prestoJobPtrType PrestoJobArgs

func PrestoJobPtr(v *PrestoJobArgs) PrestoJobPtrInput {
	return (*prestoJobPtrType)(v)
}

func (*prestoJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**PrestoJob)(nil)).Elem()
}

func (i *prestoJobPtrType) ToPrestoJobPtrOutput() PrestoJobPtrOutput {
	return i.ToPrestoJobPtrOutputWithContext(context.Background())
}

func (i *prestoJobPtrType) ToPrestoJobPtrOutputWithContext(ctx context.Context) PrestoJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PrestoJobPtrOutput)
}

// A Dataproc job for running Presto (https://prestosql.io/) queries. IMPORTANT: The Dataproc Presto Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/presto) must be enabled when the cluster is created to submit a Presto job to the cluster.
type PrestoJobOutput struct{ *pulumi.OutputState }

func (PrestoJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PrestoJob)(nil)).Elem()
}

func (o PrestoJobOutput) ToPrestoJobOutput() PrestoJobOutput {
	return o
}

func (o PrestoJobOutput) ToPrestoJobOutputWithContext(ctx context.Context) PrestoJobOutput {
	return o
}

func (o PrestoJobOutput) ToPrestoJobPtrOutput() PrestoJobPtrOutput {
	return o.ToPrestoJobPtrOutputWithContext(context.Background())
}

func (o PrestoJobOutput) ToPrestoJobPtrOutputWithContext(ctx context.Context) PrestoJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v PrestoJob) *PrestoJob {
		return &v
	}).(PrestoJobPtrOutput)
}

// Optional. Presto client tags to attach to this query
func (o PrestoJobOutput) ClientTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PrestoJob) []string { return v.ClientTags }).(pulumi.StringArrayOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o PrestoJobOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v PrestoJob) *bool { return v.ContinueOnFailure }).(pulumi.BoolPtrOutput)
}

// Optional. The runtime log config for job execution.
func (o PrestoJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v PrestoJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
func (o PrestoJobOutput) OutputFormat() pulumi.StringPtrOutput {
	return o.ApplyT(func(v PrestoJob) *string { return v.OutputFormat }).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values. Used to set Presto session properties (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
func (o PrestoJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v PrestoJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o PrestoJobOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v PrestoJob) *string { return v.QueryFileUri }).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o PrestoJobOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v PrestoJob) *QueryList { return v.QueryList }).(QueryListPtrOutput)
}

type PrestoJobPtrOutput struct{ *pulumi.OutputState }

func (PrestoJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**PrestoJob)(nil)).Elem()
}

func (o PrestoJobPtrOutput) ToPrestoJobPtrOutput() PrestoJobPtrOutput {
	return o
}

func (o PrestoJobPtrOutput) ToPrestoJobPtrOutputWithContext(ctx context.Context) PrestoJobPtrOutput {
	return o
}

func (o PrestoJobPtrOutput) Elem() PrestoJobOutput {
	return o.ApplyT(func(v *PrestoJob) PrestoJob {
		if v != nil {
			return *v
		}
		var ret PrestoJob
		return ret
	}).(PrestoJobOutput)
}

// Optional. Presto client tags to attach to this query
func (o PrestoJobPtrOutput) ClientTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PrestoJob) []string {
		if v == nil {
			return nil
		}
		return v.ClientTags
	}).(pulumi.StringArrayOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o PrestoJobPtrOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *PrestoJob) *bool {
		if v == nil {
			return nil
		}
		return v.ContinueOnFailure
	}).(pulumi.BoolPtrOutput)
}

// Optional. The runtime log config for job execution.
func (o PrestoJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *PrestoJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
func (o PrestoJobPtrOutput) OutputFormat() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *PrestoJob) *string {
		if v == nil {
			return nil
		}
		return v.OutputFormat
	}).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values. Used to set Presto session properties (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
func (o PrestoJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *PrestoJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o PrestoJobPtrOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *PrestoJob) *string {
		if v == nil {
			return nil
		}
		return v.QueryFileUri
	}).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o PrestoJobPtrOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v *PrestoJob) *QueryList {
		if v == nil {
			return nil
		}
		return v.QueryList
	}).(QueryListPtrOutput)
}

// A Dataproc job for running Presto (https://prestosql.io/) queries. IMPORTANT: The Dataproc Presto Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/presto) must be enabled when the cluster is created to submit a Presto job to the cluster.
type PrestoJobResponse struct {
	// Optional. Presto client tags to attach to this query
	ClientTags []string `pulumi:"clientTags"`
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure bool `pulumi:"continueOnFailure"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
	OutputFormat string `pulumi:"outputFormat"`
	// Optional. A mapping of property names to values. Used to set Presto session properties (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListResponse `pulumi:"queryList"`
}

// A Dataproc job for running Presto (https://prestosql.io/) queries. IMPORTANT: The Dataproc Presto Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/presto) must be enabled when the cluster is created to submit a Presto job to the cluster.
type PrestoJobResponseOutput struct{ *pulumi.OutputState }

func (PrestoJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PrestoJobResponse)(nil)).Elem()
}

func (o PrestoJobResponseOutput) ToPrestoJobResponseOutput() PrestoJobResponseOutput {
	return o
}

func (o PrestoJobResponseOutput) ToPrestoJobResponseOutputWithContext(ctx context.Context) PrestoJobResponseOutput {
	return o
}

// Optional. Presto client tags to attach to this query
func (o PrestoJobResponseOutput) ClientTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PrestoJobResponse) []string { return v.ClientTags }).(pulumi.StringArrayOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o PrestoJobResponseOutput) ContinueOnFailure() pulumi.BoolOutput {
	return o.ApplyT(func(v PrestoJobResponse) bool { return v.ContinueOnFailure }).(pulumi.BoolOutput)
}

// Optional. The runtime log config for job execution.
func (o PrestoJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v PrestoJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
func (o PrestoJobResponseOutput) OutputFormat() pulumi.StringOutput {
	return o.ApplyT(func(v PrestoJobResponse) string { return v.OutputFormat }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values. Used to set Presto session properties (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
func (o PrestoJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v PrestoJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o PrestoJobResponseOutput) QueryFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v PrestoJobResponse) string { return v.QueryFileUri }).(pulumi.StringOutput)
}

// A list of queries.
func (o PrestoJobResponseOutput) QueryList() QueryListResponseOutput {
	return o.ApplyT(func(v PrestoJobResponse) QueryListResponse { return v.QueryList }).(QueryListResponseOutput)
}

// A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload.
type PySparkBatch struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
	MainPythonFileUri string `pulumi:"mainPythonFileUri"`
	// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris []string `pulumi:"pythonFileUris"`
}

// PySparkBatchInput is an input type that accepts PySparkBatchArgs and PySparkBatchOutput values.
// You can construct a concrete instance of `PySparkBatchInput` via:
//
//	PySparkBatchArgs{...}
type PySparkBatchInput interface {
	pulumi.Input

	ToPySparkBatchOutput() PySparkBatchOutput
	ToPySparkBatchOutputWithContext(context.Context) PySparkBatchOutput
}

// A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload.
type PySparkBatchArgs struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
	MainPythonFileUri pulumi.StringInput `pulumi:"mainPythonFileUri"`
	// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris pulumi.StringArrayInput `pulumi:"pythonFileUris"`
}

func (PySparkBatchArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*PySparkBatch)(nil)).Elem()
}

func (i PySparkBatchArgs) ToPySparkBatchOutput() PySparkBatchOutput {
	return i.ToPySparkBatchOutputWithContext(context.Background())
}

func (i PySparkBatchArgs) ToPySparkBatchOutputWithContext(ctx context.Context) PySparkBatchOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PySparkBatchOutput)
}

func (i PySparkBatchArgs) ToPySparkBatchPtrOutput() PySparkBatchPtrOutput {
	return i.ToPySparkBatchPtrOutputWithContext(context.Background())
}

func (i PySparkBatchArgs) ToPySparkBatchPtrOutputWithContext(ctx context.Context) PySparkBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PySparkBatchOutput).ToPySparkBatchPtrOutputWithContext(ctx)
}

// PySparkBatchPtrInput is an input type that accepts PySparkBatchArgs, PySparkBatchPtr and PySparkBatchPtrOutput values.
// You can construct a concrete instance of `PySparkBatchPtrInput` via:
//
//	        PySparkBatchArgs{...}
//
//	or:
//
//	        nil
type PySparkBatchPtrInput interface {
	pulumi.Input

	ToPySparkBatchPtrOutput() PySparkBatchPtrOutput
	ToPySparkBatchPtrOutputWithContext(context.Context) PySparkBatchPtrOutput
}

type pySparkBatchPtrType PySparkBatchArgs

func PySparkBatchPtr(v *PySparkBatchArgs) PySparkBatchPtrInput {
	return (*pySparkBatchPtrType)(v)
}

func (*pySparkBatchPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**PySparkBatch)(nil)).Elem()
}

func (i *pySparkBatchPtrType) ToPySparkBatchPtrOutput() PySparkBatchPtrOutput {
	return i.ToPySparkBatchPtrOutputWithContext(context.Background())
}

func (i *pySparkBatchPtrType) ToPySparkBatchPtrOutputWithContext(ctx context.Context) PySparkBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PySparkBatchPtrOutput)
}

// A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload.
type PySparkBatchOutput struct{ *pulumi.OutputState }

func (PySparkBatchOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PySparkBatch)(nil)).Elem()
}

func (o PySparkBatchOutput) ToPySparkBatchOutput() PySparkBatchOutput {
	return o
}

func (o PySparkBatchOutput) ToPySparkBatchOutputWithContext(ctx context.Context) PySparkBatchOutput {
	return o
}

func (o PySparkBatchOutput) ToPySparkBatchPtrOutput() PySparkBatchPtrOutput {
	return o.ToPySparkBatchPtrOutputWithContext(context.Background())
}

func (o PySparkBatchOutput) ToPySparkBatchPtrOutputWithContext(ctx context.Context) PySparkBatchPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v PySparkBatch) *PySparkBatch {
		return &v
	}).(PySparkBatchPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o PySparkBatchOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatch) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o PySparkBatchOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatch) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o PySparkBatchOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatch) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
func (o PySparkBatchOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatch) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
func (o PySparkBatchOutput) MainPythonFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v PySparkBatch) string { return v.MainPythonFileUri }).(pulumi.StringOutput)
}

// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
func (o PySparkBatchOutput) PythonFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatch) []string { return v.PythonFileUris }).(pulumi.StringArrayOutput)
}

type PySparkBatchPtrOutput struct{ *pulumi.OutputState }

func (PySparkBatchPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**PySparkBatch)(nil)).Elem()
}

func (o PySparkBatchPtrOutput) ToPySparkBatchPtrOutput() PySparkBatchPtrOutput {
	return o
}

func (o PySparkBatchPtrOutput) ToPySparkBatchPtrOutputWithContext(ctx context.Context) PySparkBatchPtrOutput {
	return o
}

func (o PySparkBatchPtrOutput) Elem() PySparkBatchOutput {
	return o.ApplyT(func(v *PySparkBatch) PySparkBatch {
		if v != nil {
			return *v
		}
		var ret PySparkBatch
		return ret
	}).(PySparkBatchOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o PySparkBatchPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o PySparkBatchPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o PySparkBatchPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
func (o PySparkBatchPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
func (o PySparkBatchPtrOutput) MainPythonFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *PySparkBatch) *string {
		if v == nil {
			return nil
		}
		return &v.MainPythonFileUri
	}).(pulumi.StringPtrOutput)
}

// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
func (o PySparkBatchPtrOutput) PythonFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.PythonFileUris
	}).(pulumi.StringArrayOutput)
}

// A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload.
type PySparkBatchResponse struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
	MainPythonFileUri string `pulumi:"mainPythonFileUri"`
	// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris []string `pulumi:"pythonFileUris"`
}

// A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload.
type PySparkBatchResponseOutput struct{ *pulumi.OutputState }

func (PySparkBatchResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PySparkBatchResponse)(nil)).Elem()
}

func (o PySparkBatchResponseOutput) ToPySparkBatchResponseOutput() PySparkBatchResponseOutput {
	return o
}

func (o PySparkBatchResponseOutput) ToPySparkBatchResponseOutputWithContext(ctx context.Context) PySparkBatchResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o PySparkBatchResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatchResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o PySparkBatchResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatchResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o PySparkBatchResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatchResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
func (o PySparkBatchResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatchResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
func (o PySparkBatchResponseOutput) MainPythonFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v PySparkBatchResponse) string { return v.MainPythonFileUri }).(pulumi.StringOutput)
}

// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
func (o PySparkBatchResponseOutput) PythonFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatchResponse) []string { return v.PythonFileUris }).(pulumi.StringArrayOutput)
}

// A Dataproc job for running Apache PySpark (https://spark.apache.org/docs/0.9.0/python-programming-guide.html) applications on YARN.
type PySparkJob struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
	MainPythonFileUri string `pulumi:"mainPythonFileUri"`
	// Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties map[string]string `pulumi:"properties"`
	// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris []string `pulumi:"pythonFileUris"`
}

// PySparkJobInput is an input type that accepts PySparkJobArgs and PySparkJobOutput values.
// You can construct a concrete instance of `PySparkJobInput` via:
//
//	PySparkJobArgs{...}
type PySparkJobInput interface {
	pulumi.Input

	ToPySparkJobOutput() PySparkJobOutput
	ToPySparkJobOutputWithContext(context.Context) PySparkJobOutput
}

// A Dataproc job for running Apache PySpark (https://spark.apache.org/docs/0.9.0/python-programming-guide.html) applications on YARN.
type PySparkJobArgs struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
	MainPythonFileUri pulumi.StringInput `pulumi:"mainPythonFileUri"`
	// Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties pulumi.StringMapInput `pulumi:"properties"`
	// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris pulumi.StringArrayInput `pulumi:"pythonFileUris"`
}

func (PySparkJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*PySparkJob)(nil)).Elem()
}

func (i PySparkJobArgs) ToPySparkJobOutput() PySparkJobOutput {
	return i.ToPySparkJobOutputWithContext(context.Background())
}

func (i PySparkJobArgs) ToPySparkJobOutputWithContext(ctx context.Context) PySparkJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PySparkJobOutput)
}

func (i PySparkJobArgs) ToPySparkJobPtrOutput() PySparkJobPtrOutput {
	return i.ToPySparkJobPtrOutputWithContext(context.Background())
}

func (i PySparkJobArgs) ToPySparkJobPtrOutputWithContext(ctx context.Context) PySparkJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PySparkJobOutput).ToPySparkJobPtrOutputWithContext(ctx)
}

// PySparkJobPtrInput is an input type that accepts PySparkJobArgs, PySparkJobPtr and PySparkJobPtrOutput values.
// You can construct a concrete instance of `PySparkJobPtrInput` via:
//
//	        PySparkJobArgs{...}
//
//	or:
//
//	        nil
type PySparkJobPtrInput interface {
	pulumi.Input

	ToPySparkJobPtrOutput() PySparkJobPtrOutput
	ToPySparkJobPtrOutputWithContext(context.Context) PySparkJobPtrOutput
}

type pySparkJobPtrType PySparkJobArgs

func PySparkJobPtr(v *PySparkJobArgs) PySparkJobPtrInput {
	return (*pySparkJobPtrType)(v)
}

func (*pySparkJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**PySparkJob)(nil)).Elem()
}

func (i *pySparkJobPtrType) ToPySparkJobPtrOutput() PySparkJobPtrOutput {
	return i.ToPySparkJobPtrOutputWithContext(context.Background())
}

func (i *pySparkJobPtrType) ToPySparkJobPtrOutputWithContext(ctx context.Context) PySparkJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PySparkJobPtrOutput)
}

// A Dataproc job for running Apache PySpark (https://spark.apache.org/docs/0.9.0/python-programming-guide.html) applications on YARN.
type PySparkJobOutput struct{ *pulumi.OutputState }

func (PySparkJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PySparkJob)(nil)).Elem()
}

func (o PySparkJobOutput) ToPySparkJobOutput() PySparkJobOutput {
	return o
}

func (o PySparkJobOutput) ToPySparkJobOutputWithContext(ctx context.Context) PySparkJobOutput {
	return o
}

func (o PySparkJobOutput) ToPySparkJobPtrOutput() PySparkJobPtrOutput {
	return o.ToPySparkJobPtrOutputWithContext(context.Background())
}

func (o PySparkJobOutput) ToPySparkJobPtrOutputWithContext(ctx context.Context) PySparkJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v PySparkJob) *PySparkJob {
		return &v
	}).(PySparkJobPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o PySparkJobOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJob) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o PySparkJobOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJob) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o PySparkJobOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJob) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
func (o PySparkJobOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJob) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o PySparkJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v PySparkJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
func (o PySparkJobOutput) MainPythonFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v PySparkJob) string { return v.MainPythonFileUri }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o PySparkJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v PySparkJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
func (o PySparkJobOutput) PythonFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJob) []string { return v.PythonFileUris }).(pulumi.StringArrayOutput)
}

type PySparkJobPtrOutput struct{ *pulumi.OutputState }

func (PySparkJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**PySparkJob)(nil)).Elem()
}

func (o PySparkJobPtrOutput) ToPySparkJobPtrOutput() PySparkJobPtrOutput {
	return o
}

func (o PySparkJobPtrOutput) ToPySparkJobPtrOutputWithContext(ctx context.Context) PySparkJobPtrOutput {
	return o
}

func (o PySparkJobPtrOutput) Elem() PySparkJobOutput {
	return o.ApplyT(func(v *PySparkJob) PySparkJob {
		if v != nil {
			return *v
		}
		var ret PySparkJob
		return ret
	}).(PySparkJobOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o PySparkJobPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkJob) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o PySparkJobPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkJob) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o PySparkJobPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkJob) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
func (o PySparkJobPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkJob) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o PySparkJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *PySparkJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
func (o PySparkJobPtrOutput) MainPythonFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *PySparkJob) *string {
		if v == nil {
			return nil
		}
		return &v.MainPythonFileUri
	}).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o PySparkJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *PySparkJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
func (o PySparkJobPtrOutput) PythonFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkJob) []string {
		if v == nil {
			return nil
		}
		return v.PythonFileUris
	}).(pulumi.StringArrayOutput)
}

// A Dataproc job for running Apache PySpark (https://spark.apache.org/docs/0.9.0/python-programming-guide.html) applications on YARN.
type PySparkJobResponse struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
	MainPythonFileUri string `pulumi:"mainPythonFileUri"`
	// Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties map[string]string `pulumi:"properties"`
	// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris []string `pulumi:"pythonFileUris"`
}

// A Dataproc job for running Apache PySpark (https://spark.apache.org/docs/0.9.0/python-programming-guide.html) applications on YARN.
type PySparkJobResponseOutput struct{ *pulumi.OutputState }

func (PySparkJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PySparkJobResponse)(nil)).Elem()
}

func (o PySparkJobResponseOutput) ToPySparkJobResponseOutput() PySparkJobResponseOutput {
	return o
}

func (o PySparkJobResponseOutput) ToPySparkJobResponseOutputWithContext(ctx context.Context) PySparkJobResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o PySparkJobResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJobResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o PySparkJobResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJobResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o PySparkJobResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJobResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
func (o PySparkJobResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJobResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o PySparkJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v PySparkJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
func (o PySparkJobResponseOutput) MainPythonFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v PySparkJobResponse) string { return v.MainPythonFileUri }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o PySparkJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v PySparkJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
func (o PySparkJobResponseOutput) PythonFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJobResponse) []string { return v.PythonFileUris }).(pulumi.StringArrayOutput)
}

// A list of queries to run on a cluster.
type QueryList struct {
	// The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
	Queries []string `pulumi:"queries"`
}

// QueryListInput is an input type that accepts QueryListArgs and QueryListOutput values.
// You can construct a concrete instance of `QueryListInput` via:
//
//	QueryListArgs{...}
type QueryListInput interface {
	pulumi.Input

	ToQueryListOutput() QueryListOutput
	ToQueryListOutputWithContext(context.Context) QueryListOutput
}

// A list of queries to run on a cluster.
type QueryListArgs struct {
	// The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
	Queries pulumi.StringArrayInput `pulumi:"queries"`
}

func (QueryListArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*QueryList)(nil)).Elem()
}

func (i QueryListArgs) ToQueryListOutput() QueryListOutput {
	return i.ToQueryListOutputWithContext(context.Background())
}

func (i QueryListArgs) ToQueryListOutputWithContext(ctx context.Context) QueryListOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QueryListOutput)
}

func (i QueryListArgs) ToQueryListPtrOutput() QueryListPtrOutput {
	return i.ToQueryListPtrOutputWithContext(context.Background())
}

func (i QueryListArgs) ToQueryListPtrOutputWithContext(ctx context.Context) QueryListPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QueryListOutput).ToQueryListPtrOutputWithContext(ctx)
}

// QueryListPtrInput is an input type that accepts QueryListArgs, QueryListPtr and QueryListPtrOutput values.
// You can construct a concrete instance of `QueryListPtrInput` via:
//
//	        QueryListArgs{...}
//
//	or:
//
//	        nil
type QueryListPtrInput interface {
	pulumi.Input

	ToQueryListPtrOutput() QueryListPtrOutput
	ToQueryListPtrOutputWithContext(context.Context) QueryListPtrOutput
}

type queryListPtrType QueryListArgs

func QueryListPtr(v *QueryListArgs) QueryListPtrInput {
	return (*queryListPtrType)(v)
}

func (*queryListPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**QueryList)(nil)).Elem()
}

func (i *queryListPtrType) ToQueryListPtrOutput() QueryListPtrOutput {
	return i.ToQueryListPtrOutputWithContext(context.Background())
}

func (i *queryListPtrType) ToQueryListPtrOutputWithContext(ctx context.Context) QueryListPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QueryListPtrOutput)
}

// A list of queries to run on a cluster.
type QueryListOutput struct{ *pulumi.OutputState }

func (QueryListOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*QueryList)(nil)).Elem()
}

func (o QueryListOutput) ToQueryListOutput() QueryListOutput {
	return o
}

func (o QueryListOutput) ToQueryListOutputWithContext(ctx context.Context) QueryListOutput {
	return o
}

func (o QueryListOutput) ToQueryListPtrOutput() QueryListPtrOutput {
	return o.ToQueryListPtrOutputWithContext(context.Background())
}

func (o QueryListOutput) ToQueryListPtrOutputWithContext(ctx context.Context) QueryListPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v QueryList) *QueryList {
		return &v
	}).(QueryListPtrOutput)
}

// The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
func (o QueryListOutput) Queries() pulumi.StringArrayOutput {
	return o.ApplyT(func(v QueryList) []string { return v.Queries }).(pulumi.StringArrayOutput)
}

type QueryListPtrOutput struct{ *pulumi.OutputState }

func (QueryListPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**QueryList)(nil)).Elem()
}

func (o QueryListPtrOutput) ToQueryListPtrOutput() QueryListPtrOutput {
	return o
}

func (o QueryListPtrOutput) ToQueryListPtrOutputWithContext(ctx context.Context) QueryListPtrOutput {
	return o
}

func (o QueryListPtrOutput) Elem() QueryListOutput {
	return o.ApplyT(func(v *QueryList) QueryList {
		if v != nil {
			return *v
		}
		var ret QueryList
		return ret
	}).(QueryListOutput)
}

// The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
func (o QueryListPtrOutput) Queries() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *QueryList) []string {
		if v == nil {
			return nil
		}
		return v.Queries
	}).(pulumi.StringArrayOutput)
}

// A list of queries to run on a cluster.
type QueryListResponse struct {
	// The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
	Queries []string `pulumi:"queries"`
}

// A list of queries to run on a cluster.
type QueryListResponseOutput struct{ *pulumi.OutputState }

func (QueryListResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*QueryListResponse)(nil)).Elem()
}

func (o QueryListResponseOutput) ToQueryListResponseOutput() QueryListResponseOutput {
	return o
}

func (o QueryListResponseOutput) ToQueryListResponseOutputWithContext(ctx context.Context) QueryListResponseOutput {
	return o
}

// The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
func (o QueryListResponseOutput) Queries() pulumi.StringArrayOutput {
	return o.ApplyT(func(v QueryListResponse) []string { return v.Queries }).(pulumi.StringArrayOutput)
}

// Validation based on regular expressions.
type RegexValidation struct {
	// RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
	Regexes []string `pulumi:"regexes"`
}

// RegexValidationInput is an input type that accepts RegexValidationArgs and RegexValidationOutput values.
// You can construct a concrete instance of `RegexValidationInput` via:
//
//	RegexValidationArgs{...}
type RegexValidationInput interface {
	pulumi.Input

	ToRegexValidationOutput() RegexValidationOutput
	ToRegexValidationOutputWithContext(context.Context) RegexValidationOutput
}

// Validation based on regular expressions.
type RegexValidationArgs struct {
	// RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
	Regexes pulumi.StringArrayInput `pulumi:"regexes"`
}

func (RegexValidationArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*RegexValidation)(nil)).Elem()
}

func (i RegexValidationArgs) ToRegexValidationOutput() RegexValidationOutput {
	return i.ToRegexValidationOutputWithContext(context.Background())
}

func (i RegexValidationArgs) ToRegexValidationOutputWithContext(ctx context.Context) RegexValidationOutput {
	return pulumi.ToOutputWithContext(ctx, i).(RegexValidationOutput)
}

func (i RegexValidationArgs) ToRegexValidationPtrOutput() RegexValidationPtrOutput {
	return i.ToRegexValidationPtrOutputWithContext(context.Background())
}

func (i RegexValidationArgs) ToRegexValidationPtrOutputWithContext(ctx context.Context) RegexValidationPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(RegexValidationOutput).ToRegexValidationPtrOutputWithContext(ctx)
}

// RegexValidationPtrInput is an input type that accepts RegexValidationArgs, RegexValidationPtr and RegexValidationPtrOutput values.
// You can construct a concrete instance of `RegexValidationPtrInput` via:
//
//	        RegexValidationArgs{...}
//
//	or:
//
//	        nil
type RegexValidationPtrInput interface {
	pulumi.Input

	ToRegexValidationPtrOutput() RegexValidationPtrOutput
	ToRegexValidationPtrOutputWithContext(context.Context) RegexValidationPtrOutput
}

type regexValidationPtrType RegexValidationArgs

func RegexValidationPtr(v *RegexValidationArgs) RegexValidationPtrInput {
	return (*regexValidationPtrType)(v)
}

func (*regexValidationPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**RegexValidation)(nil)).Elem()
}

func (i *regexValidationPtrType) ToRegexValidationPtrOutput() RegexValidationPtrOutput {
	return i.ToRegexValidationPtrOutputWithContext(context.Background())
}

func (i *regexValidationPtrType) ToRegexValidationPtrOutputWithContext(ctx context.Context) RegexValidationPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(RegexValidationPtrOutput)
}

// Validation based on regular expressions.
type RegexValidationOutput struct{ *pulumi.OutputState }

func (RegexValidationOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*RegexValidation)(nil)).Elem()
}

func (o RegexValidationOutput) ToRegexValidationOutput() RegexValidationOutput {
	return o
}

func (o RegexValidationOutput) ToRegexValidationOutputWithContext(ctx context.Context) RegexValidationOutput {
	return o
}

func (o RegexValidationOutput) ToRegexValidationPtrOutput() RegexValidationPtrOutput {
	return o.ToRegexValidationPtrOutputWithContext(context.Background())
}

func (o RegexValidationOutput) ToRegexValidationPtrOutputWithContext(ctx context.Context) RegexValidationPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v RegexValidation) *RegexValidation {
		return &v
	}).(RegexValidationPtrOutput)
}

// RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
func (o RegexValidationOutput) Regexes() pulumi.StringArrayOutput {
	return o.ApplyT(func(v RegexValidation) []string { return v.Regexes }).(pulumi.StringArrayOutput)
}

type RegexValidationPtrOutput struct{ *pulumi.OutputState }

func (RegexValidationPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**RegexValidation)(nil)).Elem()
}

func (o RegexValidationPtrOutput) ToRegexValidationPtrOutput() RegexValidationPtrOutput {
	return o
}

func (o RegexValidationPtrOutput) ToRegexValidationPtrOutputWithContext(ctx context.Context) RegexValidationPtrOutput {
	return o
}

func (o RegexValidationPtrOutput) Elem() RegexValidationOutput {
	return o.ApplyT(func(v *RegexValidation) RegexValidation {
		if v != nil {
			return *v
		}
		var ret RegexValidation
		return ret
	}).(RegexValidationOutput)
}

// RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
func (o RegexValidationPtrOutput) Regexes() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *RegexValidation) []string {
		if v == nil {
			return nil
		}
		return v.Regexes
	}).(pulumi.StringArrayOutput)
}

// Validation based on regular expressions.
type RegexValidationResponse struct {
	// RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
	Regexes []string `pulumi:"regexes"`
}

// Validation based on regular expressions.
type RegexValidationResponseOutput struct{ *pulumi.OutputState }

func (RegexValidationResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*RegexValidationResponse)(nil)).Elem()
}

func (o RegexValidationResponseOutput) ToRegexValidationResponseOutput() RegexValidationResponseOutput {
	return o
}

func (o RegexValidationResponseOutput) ToRegexValidationResponseOutputWithContext(ctx context.Context) RegexValidationResponseOutput {
	return o
}

// RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
func (o RegexValidationResponseOutput) Regexes() pulumi.StringArrayOutput {
	return o.ApplyT(func(v RegexValidationResponse) []string { return v.Regexes }).(pulumi.StringArrayOutput)
}

// Reservation Affinity for consuming Zonal reservation.
type ReservationAffinity struct {
	// Optional. Type of reservation to consume
	ConsumeReservationType *ReservationAffinityConsumeReservationType `pulumi:"consumeReservationType"`
	// Optional. Corresponds to the label key of reservation resource.
	Key *string `pulumi:"key"`
	// Optional. Corresponds to the label values of reservation resource.
	Values []string `pulumi:"values"`
}

// ReservationAffinityInput is an input type that accepts ReservationAffinityArgs and ReservationAffinityOutput values.
// You can construct a concrete instance of `ReservationAffinityInput` via:
//
//	ReservationAffinityArgs{...}
type ReservationAffinityInput interface {
	pulumi.Input

	ToReservationAffinityOutput() ReservationAffinityOutput
	ToReservationAffinityOutputWithContext(context.Context) ReservationAffinityOutput
}

// Reservation Affinity for consuming Zonal reservation.
type ReservationAffinityArgs struct {
	// Optional. Type of reservation to consume
	ConsumeReservationType ReservationAffinityConsumeReservationTypePtrInput `pulumi:"consumeReservationType"`
	// Optional. Corresponds to the label key of reservation resource.
	Key pulumi.StringPtrInput `pulumi:"key"`
	// Optional. Corresponds to the label values of reservation resource.
	Values pulumi.StringArrayInput `pulumi:"values"`
}

func (ReservationAffinityArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ReservationAffinity)(nil)).Elem()
}

func (i ReservationAffinityArgs) ToReservationAffinityOutput() ReservationAffinityOutput {
	return i.ToReservationAffinityOutputWithContext(context.Background())
}

func (i ReservationAffinityArgs) ToReservationAffinityOutputWithContext(ctx context.Context) ReservationAffinityOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ReservationAffinityOutput)
}

func (i ReservationAffinityArgs) ToReservationAffinityPtrOutput() ReservationAffinityPtrOutput {
	return i.ToReservationAffinityPtrOutputWithContext(context.Background())
}

func (i ReservationAffinityArgs) ToReservationAffinityPtrOutputWithContext(ctx context.Context) ReservationAffinityPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ReservationAffinityOutput).ToReservationAffinityPtrOutputWithContext(ctx)
}

// ReservationAffinityPtrInput is an input type that accepts ReservationAffinityArgs, ReservationAffinityPtr and ReservationAffinityPtrOutput values.
// You can construct a concrete instance of `ReservationAffinityPtrInput` via:
//
//	        ReservationAffinityArgs{...}
//
//	or:
//
//	        nil
type ReservationAffinityPtrInput interface {
	pulumi.Input

	ToReservationAffinityPtrOutput() ReservationAffinityPtrOutput
	ToReservationAffinityPtrOutputWithContext(context.Context) ReservationAffinityPtrOutput
}

type reservationAffinityPtrType ReservationAffinityArgs

func ReservationAffinityPtr(v *ReservationAffinityArgs) ReservationAffinityPtrInput {
	return (*reservationAffinityPtrType)(v)
}

func (*reservationAffinityPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ReservationAffinity)(nil)).Elem()
}

func (i *reservationAffinityPtrType) ToReservationAffinityPtrOutput() ReservationAffinityPtrOutput {
	return i.ToReservationAffinityPtrOutputWithContext(context.Background())
}

func (i *reservationAffinityPtrType) ToReservationAffinityPtrOutputWithContext(ctx context.Context) ReservationAffinityPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ReservationAffinityPtrOutput)
}

// Reservation Affinity for consuming Zonal reservation.
type ReservationAffinityOutput struct{ *pulumi.OutputState }

func (ReservationAffinityOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ReservationAffinity)(nil)).Elem()
}

func (o ReservationAffinityOutput) ToReservationAffinityOutput() ReservationAffinityOutput {
	return o
}

func (o ReservationAffinityOutput) ToReservationAffinityOutputWithContext(ctx context.Context) ReservationAffinityOutput {
	return o
}

func (o ReservationAffinityOutput) ToReservationAffinityPtrOutput() ReservationAffinityPtrOutput {
	return o.ToReservationAffinityPtrOutputWithContext(context.Background())
}

func (o ReservationAffinityOutput) ToReservationAffinityPtrOutputWithContext(ctx context.Context) ReservationAffinityPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ReservationAffinity) *ReservationAffinity {
		return &v
	}).(ReservationAffinityPtrOutput)
}

// Optional. Type of reservation to consume
func (o ReservationAffinityOutput) ConsumeReservationType() ReservationAffinityConsumeReservationTypePtrOutput {
	return o.ApplyT(func(v ReservationAffinity) *ReservationAffinityConsumeReservationType {
		return v.ConsumeReservationType
	}).(ReservationAffinityConsumeReservationTypePtrOutput)
}

// Optional. Corresponds to the label key of reservation resource.
func (o ReservationAffinityOutput) Key() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ReservationAffinity) *string { return v.Key }).(pulumi.StringPtrOutput)
}

// Optional. Corresponds to the label values of reservation resource.
func (o ReservationAffinityOutput) Values() pulumi.StringArrayOutput {
	return o.ApplyT(func(v ReservationAffinity) []string { return v.Values }).(pulumi.StringArrayOutput)
}

type ReservationAffinityPtrOutput struct{ *pulumi.OutputState }

func (ReservationAffinityPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ReservationAffinity)(nil)).Elem()
}

func (o ReservationAffinityPtrOutput) ToReservationAffinityPtrOutput() ReservationAffinityPtrOutput {
	return o
}

func (o ReservationAffinityPtrOutput) ToReservationAffinityPtrOutputWithContext(ctx context.Context) ReservationAffinityPtrOutput {
	return o
}

func (o ReservationAffinityPtrOutput) Elem() ReservationAffinityOutput {
	return o.ApplyT(func(v *ReservationAffinity) ReservationAffinity {
		if v != nil {
			return *v
		}
		var ret ReservationAffinity
		return ret
	}).(ReservationAffinityOutput)
}

// Optional. Type of reservation to consume
func (o ReservationAffinityPtrOutput) ConsumeReservationType() ReservationAffinityConsumeReservationTypePtrOutput {
	return o.ApplyT(func(v *ReservationAffinity) *ReservationAffinityConsumeReservationType {
		if v == nil {
			return nil
		}
		return v.ConsumeReservationType
	}).(ReservationAffinityConsumeReservationTypePtrOutput)
}

// Optional. Corresponds to the label key of reservation resource.
func (o ReservationAffinityPtrOutput) Key() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ReservationAffinity) *string {
		if v == nil {
			return nil
		}
		return v.Key
	}).(pulumi.StringPtrOutput)
}

// Optional. Corresponds to the label values of reservation resource.
func (o ReservationAffinityPtrOutput) Values() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *ReservationAffinity) []string {
		if v == nil {
			return nil
		}
		return v.Values
	}).(pulumi.StringArrayOutput)
}

// Reservation Affinity for consuming Zonal reservation.
type ReservationAffinityResponse struct {
	// Optional. Type of reservation to consume
	ConsumeReservationType string `pulumi:"consumeReservationType"`
	// Optional. Corresponds to the label key of reservation resource.
	Key string `pulumi:"key"`
	// Optional. Corresponds to the label values of reservation resource.
	Values []string `pulumi:"values"`
}

// Reservation Affinity for consuming Zonal reservation.
type ReservationAffinityResponseOutput struct{ *pulumi.OutputState }

func (ReservationAffinityResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ReservationAffinityResponse)(nil)).Elem()
}

func (o ReservationAffinityResponseOutput) ToReservationAffinityResponseOutput() ReservationAffinityResponseOutput {
	return o
}

func (o ReservationAffinityResponseOutput) ToReservationAffinityResponseOutputWithContext(ctx context.Context) ReservationAffinityResponseOutput {
	return o
}

// Optional. Type of reservation to consume
func (o ReservationAffinityResponseOutput) ConsumeReservationType() pulumi.StringOutput {
	return o.ApplyT(func(v ReservationAffinityResponse) string { return v.ConsumeReservationType }).(pulumi.StringOutput)
}

// Optional. Corresponds to the label key of reservation resource.
func (o ReservationAffinityResponseOutput) Key() pulumi.StringOutput {
	return o.ApplyT(func(v ReservationAffinityResponse) string { return v.Key }).(pulumi.StringOutput)
}

// Optional. Corresponds to the label values of reservation resource.
func (o ReservationAffinityResponseOutput) Values() pulumi.StringArrayOutput {
	return o.ApplyT(func(v ReservationAffinityResponse) []string { return v.Values }).(pulumi.StringArrayOutput)
}

// Runtime configuration for a workload.
type RuntimeConfig struct {
	// Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
	ContainerImage *string `pulumi:"containerImage"`
	// Optional. A mapping of property names to values, which are used to configure workload execution.
	Properties map[string]string `pulumi:"properties"`
	// Optional. Version of the batch runtime.
	Version *string `pulumi:"version"`
}

// RuntimeConfigInput is an input type that accepts RuntimeConfigArgs and RuntimeConfigOutput values.
// You can construct a concrete instance of `RuntimeConfigInput` via:
//
//	RuntimeConfigArgs{...}
type RuntimeConfigInput interface {
	pulumi.Input

	ToRuntimeConfigOutput() RuntimeConfigOutput
	ToRuntimeConfigOutputWithContext(context.Context) RuntimeConfigOutput
}

// Runtime configuration for a workload.
type RuntimeConfigArgs struct {
	// Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
	ContainerImage pulumi.StringPtrInput `pulumi:"containerImage"`
	// Optional. A mapping of property names to values, which are used to configure workload execution.
	Properties pulumi.StringMapInput `pulumi:"properties"`
	// Optional. Version of the batch runtime.
	Version pulumi.StringPtrInput `pulumi:"version"`
}

func (RuntimeConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*RuntimeConfig)(nil)).Elem()
}

func (i RuntimeConfigArgs) ToRuntimeConfigOutput() RuntimeConfigOutput {
	return i.ToRuntimeConfigOutputWithContext(context.Background())
}

func (i RuntimeConfigArgs) ToRuntimeConfigOutputWithContext(ctx context.Context) RuntimeConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(RuntimeConfigOutput)
}

func (i RuntimeConfigArgs) ToRuntimeConfigPtrOutput() RuntimeConfigPtrOutput {
	return i.ToRuntimeConfigPtrOutputWithContext(context.Background())
}

func (i RuntimeConfigArgs) ToRuntimeConfigPtrOutputWithContext(ctx context.Context) RuntimeConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(RuntimeConfigOutput).ToRuntimeConfigPtrOutputWithContext(ctx)
}

// RuntimeConfigPtrInput is an input type that accepts RuntimeConfigArgs, RuntimeConfigPtr and RuntimeConfigPtrOutput values.
// You can construct a concrete instance of `RuntimeConfigPtrInput` via:
//
//	        RuntimeConfigArgs{...}
//
//	or:
//
//	        nil
type RuntimeConfigPtrInput interface {
	pulumi.Input

	ToRuntimeConfigPtrOutput() RuntimeConfigPtrOutput
	ToRuntimeConfigPtrOutputWithContext(context.Context) RuntimeConfigPtrOutput
}

type runtimeConfigPtrType RuntimeConfigArgs

func RuntimeConfigPtr(v *RuntimeConfigArgs) RuntimeConfigPtrInput {
	return (*runtimeConfigPtrType)(v)
}

func (*runtimeConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**RuntimeConfig)(nil)).Elem()
}

func (i *runtimeConfigPtrType) ToRuntimeConfigPtrOutput() RuntimeConfigPtrOutput {
	return i.ToRuntimeConfigPtrOutputWithContext(context.Background())
}

func (i *runtimeConfigPtrType) ToRuntimeConfigPtrOutputWithContext(ctx context.Context) RuntimeConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(RuntimeConfigPtrOutput)
}

// Runtime configuration for a workload.
type RuntimeConfigOutput struct{ *pulumi.OutputState }

func (RuntimeConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*RuntimeConfig)(nil)).Elem()
}

func (o RuntimeConfigOutput) ToRuntimeConfigOutput() RuntimeConfigOutput {
	return o
}

func (o RuntimeConfigOutput) ToRuntimeConfigOutputWithContext(ctx context.Context) RuntimeConfigOutput {
	return o
}

func (o RuntimeConfigOutput) ToRuntimeConfigPtrOutput() RuntimeConfigPtrOutput {
	return o.ToRuntimeConfigPtrOutputWithContext(context.Background())
}

func (o RuntimeConfigOutput) ToRuntimeConfigPtrOutputWithContext(ctx context.Context) RuntimeConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v RuntimeConfig) *RuntimeConfig {
		return &v
	}).(RuntimeConfigPtrOutput)
}

// Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
func (o RuntimeConfigOutput) ContainerImage() pulumi.StringPtrOutput {
	return o.ApplyT(func(v RuntimeConfig) *string { return v.ContainerImage }).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, which are used to configure workload execution.
func (o RuntimeConfigOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v RuntimeConfig) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// Optional. Version of the batch runtime.
func (o RuntimeConfigOutput) Version() pulumi.StringPtrOutput {
	return o.ApplyT(func(v RuntimeConfig) *string { return v.Version }).(pulumi.StringPtrOutput)
}

type RuntimeConfigPtrOutput struct{ *pulumi.OutputState }

func (RuntimeConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**RuntimeConfig)(nil)).Elem()
}

func (o RuntimeConfigPtrOutput) ToRuntimeConfigPtrOutput() RuntimeConfigPtrOutput {
	return o
}

func (o RuntimeConfigPtrOutput) ToRuntimeConfigPtrOutputWithContext(ctx context.Context) RuntimeConfigPtrOutput {
	return o
}

func (o RuntimeConfigPtrOutput) Elem() RuntimeConfigOutput {
	return o.ApplyT(func(v *RuntimeConfig) RuntimeConfig {
		if v != nil {
			return *v
		}
		var ret RuntimeConfig
		return ret
	}).(RuntimeConfigOutput)
}

// Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
func (o RuntimeConfigPtrOutput) ContainerImage() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *RuntimeConfig) *string {
		if v == nil {
			return nil
		}
		return v.ContainerImage
	}).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, which are used to configure workload execution.
func (o RuntimeConfigPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *RuntimeConfig) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// Optional. Version of the batch runtime.
func (o RuntimeConfigPtrOutput) Version() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *RuntimeConfig) *string {
		if v == nil {
			return nil
		}
		return v.Version
	}).(pulumi.StringPtrOutput)
}

// Runtime configuration for a workload.
type RuntimeConfigResponse struct {
	// Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
	ContainerImage string `pulumi:"containerImage"`
	// Optional. A mapping of property names to values, which are used to configure workload execution.
	Properties map[string]string `pulumi:"properties"`
	// Optional. Version of the batch runtime.
	Version string `pulumi:"version"`
}

// Runtime configuration for a workload.
type RuntimeConfigResponseOutput struct{ *pulumi.OutputState }

func (RuntimeConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*RuntimeConfigResponse)(nil)).Elem()
}

func (o RuntimeConfigResponseOutput) ToRuntimeConfigResponseOutput() RuntimeConfigResponseOutput {
	return o
}

func (o RuntimeConfigResponseOutput) ToRuntimeConfigResponseOutputWithContext(ctx context.Context) RuntimeConfigResponseOutput {
	return o
}

// Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
func (o RuntimeConfigResponseOutput) ContainerImage() pulumi.StringOutput {
	return o.ApplyT(func(v RuntimeConfigResponse) string { return v.ContainerImage }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, which are used to configure workload execution.
func (o RuntimeConfigResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v RuntimeConfigResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// Optional. Version of the batch runtime.
func (o RuntimeConfigResponseOutput) Version() pulumi.StringOutput {
	return o.ApplyT(func(v RuntimeConfigResponse) string { return v.Version }).(pulumi.StringOutput)
}

// Runtime information about workload execution.
type RuntimeInfoResponse struct {
	// Approximate workload resource usage calculated after workload finishes (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
	ApproximateUsage UsageMetricsResponse `pulumi:"approximateUsage"`
	// Snapshot of current workload resource usage.
	CurrentUsage UsageSnapshotResponse `pulumi:"currentUsage"`
	// A URI pointing to the location of the diagnostics tarball.
	DiagnosticOutputUri string `pulumi:"diagnosticOutputUri"`
	// Map of remote access endpoints (such as web interfaces and APIs) to their URIs.
	Endpoints map[string]string `pulumi:"endpoints"`
	// A URI pointing to the location of the stdout and stderr of the workload.
	OutputUri string `pulumi:"outputUri"`
}

// Runtime information about workload execution.
type RuntimeInfoResponseOutput struct{ *pulumi.OutputState }

func (RuntimeInfoResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*RuntimeInfoResponse)(nil)).Elem()
}

func (o RuntimeInfoResponseOutput) ToRuntimeInfoResponseOutput() RuntimeInfoResponseOutput {
	return o
}

func (o RuntimeInfoResponseOutput) ToRuntimeInfoResponseOutputWithContext(ctx context.Context) RuntimeInfoResponseOutput {
	return o
}

// Approximate workload resource usage calculated after workload finishes (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
func (o RuntimeInfoResponseOutput) ApproximateUsage() UsageMetricsResponseOutput {
	return o.ApplyT(func(v RuntimeInfoResponse) UsageMetricsResponse { return v.ApproximateUsage }).(UsageMetricsResponseOutput)
}

// Snapshot of current workload resource usage.
func (o RuntimeInfoResponseOutput) CurrentUsage() UsageSnapshotResponseOutput {
	return o.ApplyT(func(v RuntimeInfoResponse) UsageSnapshotResponse { return v.CurrentUsage }).(UsageSnapshotResponseOutput)
}

// A URI pointing to the location of the diagnostics tarball.
func (o RuntimeInfoResponseOutput) DiagnosticOutputUri() pulumi.StringOutput {
	return o.ApplyT(func(v RuntimeInfoResponse) string { return v.DiagnosticOutputUri }).(pulumi.StringOutput)
}

// Map of remote access endpoints (such as web interfaces and APIs) to their URIs.
func (o RuntimeInfoResponseOutput) Endpoints() pulumi.StringMapOutput {
	return o.ApplyT(func(v RuntimeInfoResponse) map[string]string { return v.Endpoints }).(pulumi.StringMapOutput)
}

// A URI pointing to the location of the stdout and stderr of the workload.
func (o RuntimeInfoResponseOutput) OutputUri() pulumi.StringOutput {
	return o.ApplyT(func(v RuntimeInfoResponse) string { return v.OutputUri }).(pulumi.StringOutput)
}

// Security related configuration, including encryption, Kerberos, etc.
type SecurityConfig struct {
	// Optional. Identity related configuration, including service account based secure multi-tenancy user mappings.
	IdentityConfig *IdentityConfig `pulumi:"identityConfig"`
	// Optional. Kerberos related configuration.
	KerberosConfig *KerberosConfig `pulumi:"kerberosConfig"`
}

// SecurityConfigInput is an input type that accepts SecurityConfigArgs and SecurityConfigOutput values.
// You can construct a concrete instance of `SecurityConfigInput` via:
//
//	SecurityConfigArgs{...}
type SecurityConfigInput interface {
	pulumi.Input

	ToSecurityConfigOutput() SecurityConfigOutput
	ToSecurityConfigOutputWithContext(context.Context) SecurityConfigOutput
}

// Security related configuration, including encryption, Kerberos, etc.
type SecurityConfigArgs struct {
	// Optional. Identity related configuration, including service account based secure multi-tenancy user mappings.
	IdentityConfig IdentityConfigPtrInput `pulumi:"identityConfig"`
	// Optional. Kerberos related configuration.
	KerberosConfig KerberosConfigPtrInput `pulumi:"kerberosConfig"`
}

func (SecurityConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SecurityConfig)(nil)).Elem()
}

func (i SecurityConfigArgs) ToSecurityConfigOutput() SecurityConfigOutput {
	return i.ToSecurityConfigOutputWithContext(context.Background())
}

func (i SecurityConfigArgs) ToSecurityConfigOutputWithContext(ctx context.Context) SecurityConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SecurityConfigOutput)
}

func (i SecurityConfigArgs) ToSecurityConfigPtrOutput() SecurityConfigPtrOutput {
	return i.ToSecurityConfigPtrOutputWithContext(context.Background())
}

func (i SecurityConfigArgs) ToSecurityConfigPtrOutputWithContext(ctx context.Context) SecurityConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SecurityConfigOutput).ToSecurityConfigPtrOutputWithContext(ctx)
}

// SecurityConfigPtrInput is an input type that accepts SecurityConfigArgs, SecurityConfigPtr and SecurityConfigPtrOutput values.
// You can construct a concrete instance of `SecurityConfigPtrInput` via:
//
//	        SecurityConfigArgs{...}
//
//	or:
//
//	        nil
type SecurityConfigPtrInput interface {
	pulumi.Input

	ToSecurityConfigPtrOutput() SecurityConfigPtrOutput
	ToSecurityConfigPtrOutputWithContext(context.Context) SecurityConfigPtrOutput
}

type securityConfigPtrType SecurityConfigArgs

func SecurityConfigPtr(v *SecurityConfigArgs) SecurityConfigPtrInput {
	return (*securityConfigPtrType)(v)
}

func (*securityConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SecurityConfig)(nil)).Elem()
}

func (i *securityConfigPtrType) ToSecurityConfigPtrOutput() SecurityConfigPtrOutput {
	return i.ToSecurityConfigPtrOutputWithContext(context.Background())
}

func (i *securityConfigPtrType) ToSecurityConfigPtrOutputWithContext(ctx context.Context) SecurityConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SecurityConfigPtrOutput)
}

// Security related configuration, including encryption, Kerberos, etc.
type SecurityConfigOutput struct{ *pulumi.OutputState }

func (SecurityConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SecurityConfig)(nil)).Elem()
}

func (o SecurityConfigOutput) ToSecurityConfigOutput() SecurityConfigOutput {
	return o
}

func (o SecurityConfigOutput) ToSecurityConfigOutputWithContext(ctx context.Context) SecurityConfigOutput {
	return o
}

func (o SecurityConfigOutput) ToSecurityConfigPtrOutput() SecurityConfigPtrOutput {
	return o.ToSecurityConfigPtrOutputWithContext(context.Background())
}

func (o SecurityConfigOutput) ToSecurityConfigPtrOutputWithContext(ctx context.Context) SecurityConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SecurityConfig) *SecurityConfig {
		return &v
	}).(SecurityConfigPtrOutput)
}

// Optional. Identity related configuration, including service account based secure multi-tenancy user mappings.
func (o SecurityConfigOutput) IdentityConfig() IdentityConfigPtrOutput {
	return o.ApplyT(func(v SecurityConfig) *IdentityConfig { return v.IdentityConfig }).(IdentityConfigPtrOutput)
}

// Optional. Kerberos related configuration.
func (o SecurityConfigOutput) KerberosConfig() KerberosConfigPtrOutput {
	return o.ApplyT(func(v SecurityConfig) *KerberosConfig { return v.KerberosConfig }).(KerberosConfigPtrOutput)
}

type SecurityConfigPtrOutput struct{ *pulumi.OutputState }

func (SecurityConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SecurityConfig)(nil)).Elem()
}

func (o SecurityConfigPtrOutput) ToSecurityConfigPtrOutput() SecurityConfigPtrOutput {
	return o
}

func (o SecurityConfigPtrOutput) ToSecurityConfigPtrOutputWithContext(ctx context.Context) SecurityConfigPtrOutput {
	return o
}

func (o SecurityConfigPtrOutput) Elem() SecurityConfigOutput {
	return o.ApplyT(func(v *SecurityConfig) SecurityConfig {
		if v != nil {
			return *v
		}
		var ret SecurityConfig
		return ret
	}).(SecurityConfigOutput)
}

// Optional. Identity related configuration, including service account based secure multi-tenancy user mappings.
func (o SecurityConfigPtrOutput) IdentityConfig() IdentityConfigPtrOutput {
	return o.ApplyT(func(v *SecurityConfig) *IdentityConfig {
		if v == nil {
			return nil
		}
		return v.IdentityConfig
	}).(IdentityConfigPtrOutput)
}

// Optional. Kerberos related configuration.
func (o SecurityConfigPtrOutput) KerberosConfig() KerberosConfigPtrOutput {
	return o.ApplyT(func(v *SecurityConfig) *KerberosConfig {
		if v == nil {
			return nil
		}
		return v.KerberosConfig
	}).(KerberosConfigPtrOutput)
}

// Security related configuration, including encryption, Kerberos, etc.
type SecurityConfigResponse struct {
	// Optional. Identity related configuration, including service account based secure multi-tenancy user mappings.
	IdentityConfig IdentityConfigResponse `pulumi:"identityConfig"`
	// Optional. Kerberos related configuration.
	KerberosConfig KerberosConfigResponse `pulumi:"kerberosConfig"`
}

// Security related configuration, including encryption, Kerberos, etc.
type SecurityConfigResponseOutput struct{ *pulumi.OutputState }

func (SecurityConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SecurityConfigResponse)(nil)).Elem()
}

func (o SecurityConfigResponseOutput) ToSecurityConfigResponseOutput() SecurityConfigResponseOutput {
	return o
}

func (o SecurityConfigResponseOutput) ToSecurityConfigResponseOutputWithContext(ctx context.Context) SecurityConfigResponseOutput {
	return o
}

// Optional. Identity related configuration, including service account based secure multi-tenancy user mappings.
func (o SecurityConfigResponseOutput) IdentityConfig() IdentityConfigResponseOutput {
	return o.ApplyT(func(v SecurityConfigResponse) IdentityConfigResponse { return v.IdentityConfig }).(IdentityConfigResponseOutput)
}

// Optional. Kerberos related configuration.
func (o SecurityConfigResponseOutput) KerberosConfig() KerberosConfigResponseOutput {
	return o.ApplyT(func(v SecurityConfigResponse) KerberosConfigResponse { return v.KerberosConfig }).(KerberosConfigResponseOutput)
}

// Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
type ShieldedInstanceConfig struct {
	// Optional. Defines whether instances have integrity monitoring enabled.
	EnableIntegrityMonitoring *bool `pulumi:"enableIntegrityMonitoring"`
	// Optional. Defines whether instances have Secure Boot enabled.
	EnableSecureBoot *bool `pulumi:"enableSecureBoot"`
	// Optional. Defines whether instances have the vTPM enabled.
	EnableVtpm *bool `pulumi:"enableVtpm"`
}

// ShieldedInstanceConfigInput is an input type that accepts ShieldedInstanceConfigArgs and ShieldedInstanceConfigOutput values.
// You can construct a concrete instance of `ShieldedInstanceConfigInput` via:
//
//	ShieldedInstanceConfigArgs{...}
type ShieldedInstanceConfigInput interface {
	pulumi.Input

	ToShieldedInstanceConfigOutput() ShieldedInstanceConfigOutput
	ToShieldedInstanceConfigOutputWithContext(context.Context) ShieldedInstanceConfigOutput
}

// Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
type ShieldedInstanceConfigArgs struct {
	// Optional. Defines whether instances have integrity monitoring enabled.
	EnableIntegrityMonitoring pulumi.BoolPtrInput `pulumi:"enableIntegrityMonitoring"`
	// Optional. Defines whether instances have Secure Boot enabled.
	EnableSecureBoot pulumi.BoolPtrInput `pulumi:"enableSecureBoot"`
	// Optional. Defines whether instances have the vTPM enabled.
	EnableVtpm pulumi.BoolPtrInput `pulumi:"enableVtpm"`
}

func (ShieldedInstanceConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ShieldedInstanceConfig)(nil)).Elem()
}

func (i ShieldedInstanceConfigArgs) ToShieldedInstanceConfigOutput() ShieldedInstanceConfigOutput {
	return i.ToShieldedInstanceConfigOutputWithContext(context.Background())
}

func (i ShieldedInstanceConfigArgs) ToShieldedInstanceConfigOutputWithContext(ctx context.Context) ShieldedInstanceConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ShieldedInstanceConfigOutput)
}

func (i ShieldedInstanceConfigArgs) ToShieldedInstanceConfigPtrOutput() ShieldedInstanceConfigPtrOutput {
	return i.ToShieldedInstanceConfigPtrOutputWithContext(context.Background())
}

func (i ShieldedInstanceConfigArgs) ToShieldedInstanceConfigPtrOutputWithContext(ctx context.Context) ShieldedInstanceConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ShieldedInstanceConfigOutput).ToShieldedInstanceConfigPtrOutputWithContext(ctx)
}

// ShieldedInstanceConfigPtrInput is an input type that accepts ShieldedInstanceConfigArgs, ShieldedInstanceConfigPtr and ShieldedInstanceConfigPtrOutput values.
// You can construct a concrete instance of `ShieldedInstanceConfigPtrInput` via:
//
//	        ShieldedInstanceConfigArgs{...}
//
//	or:
//
//	        nil
type ShieldedInstanceConfigPtrInput interface {
	pulumi.Input

	ToShieldedInstanceConfigPtrOutput() ShieldedInstanceConfigPtrOutput
	ToShieldedInstanceConfigPtrOutputWithContext(context.Context) ShieldedInstanceConfigPtrOutput
}

type shieldedInstanceConfigPtrType ShieldedInstanceConfigArgs

func ShieldedInstanceConfigPtr(v *ShieldedInstanceConfigArgs) ShieldedInstanceConfigPtrInput {
	return (*shieldedInstanceConfigPtrType)(v)
}

func (*shieldedInstanceConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ShieldedInstanceConfig)(nil)).Elem()
}

func (i *shieldedInstanceConfigPtrType) ToShieldedInstanceConfigPtrOutput() ShieldedInstanceConfigPtrOutput {
	return i.ToShieldedInstanceConfigPtrOutputWithContext(context.Background())
}

func (i *shieldedInstanceConfigPtrType) ToShieldedInstanceConfigPtrOutputWithContext(ctx context.Context) ShieldedInstanceConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ShieldedInstanceConfigPtrOutput)
}

// Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
type ShieldedInstanceConfigOutput struct{ *pulumi.OutputState }

func (ShieldedInstanceConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ShieldedInstanceConfig)(nil)).Elem()
}

func (o ShieldedInstanceConfigOutput) ToShieldedInstanceConfigOutput() ShieldedInstanceConfigOutput {
	return o
}

func (o ShieldedInstanceConfigOutput) ToShieldedInstanceConfigOutputWithContext(ctx context.Context) ShieldedInstanceConfigOutput {
	return o
}

func (o ShieldedInstanceConfigOutput) ToShieldedInstanceConfigPtrOutput() ShieldedInstanceConfigPtrOutput {
	return o.ToShieldedInstanceConfigPtrOutputWithContext(context.Background())
}

func (o ShieldedInstanceConfigOutput) ToShieldedInstanceConfigPtrOutputWithContext(ctx context.Context) ShieldedInstanceConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ShieldedInstanceConfig) *ShieldedInstanceConfig {
		return &v
	}).(ShieldedInstanceConfigPtrOutput)
}

// Optional. Defines whether instances have integrity monitoring enabled.
func (o ShieldedInstanceConfigOutput) EnableIntegrityMonitoring() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v ShieldedInstanceConfig) *bool { return v.EnableIntegrityMonitoring }).(pulumi.BoolPtrOutput)
}

// Optional. Defines whether instances have Secure Boot enabled.
func (o ShieldedInstanceConfigOutput) EnableSecureBoot() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v ShieldedInstanceConfig) *bool { return v.EnableSecureBoot }).(pulumi.BoolPtrOutput)
}

// Optional. Defines whether instances have the vTPM enabled.
func (o ShieldedInstanceConfigOutput) EnableVtpm() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v ShieldedInstanceConfig) *bool { return v.EnableVtpm }).(pulumi.BoolPtrOutput)
}

type ShieldedInstanceConfigPtrOutput struct{ *pulumi.OutputState }

func (ShieldedInstanceConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ShieldedInstanceConfig)(nil)).Elem()
}

func (o ShieldedInstanceConfigPtrOutput) ToShieldedInstanceConfigPtrOutput() ShieldedInstanceConfigPtrOutput {
	return o
}

func (o ShieldedInstanceConfigPtrOutput) ToShieldedInstanceConfigPtrOutputWithContext(ctx context.Context) ShieldedInstanceConfigPtrOutput {
	return o
}

func (o ShieldedInstanceConfigPtrOutput) Elem() ShieldedInstanceConfigOutput {
	return o.ApplyT(func(v *ShieldedInstanceConfig) ShieldedInstanceConfig {
		if v != nil {
			return *v
		}
		var ret ShieldedInstanceConfig
		return ret
	}).(ShieldedInstanceConfigOutput)
}

// Optional. Defines whether instances have integrity monitoring enabled.
func (o ShieldedInstanceConfigPtrOutput) EnableIntegrityMonitoring() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *ShieldedInstanceConfig) *bool {
		if v == nil {
			return nil
		}
		return v.EnableIntegrityMonitoring
	}).(pulumi.BoolPtrOutput)
}

// Optional. Defines whether instances have Secure Boot enabled.
func (o ShieldedInstanceConfigPtrOutput) EnableSecureBoot() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *ShieldedInstanceConfig) *bool {
		if v == nil {
			return nil
		}
		return v.EnableSecureBoot
	}).(pulumi.BoolPtrOutput)
}

// Optional. Defines whether instances have the vTPM enabled.
func (o ShieldedInstanceConfigPtrOutput) EnableVtpm() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *ShieldedInstanceConfig) *bool {
		if v == nil {
			return nil
		}
		return v.EnableVtpm
	}).(pulumi.BoolPtrOutput)
}

// Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
type ShieldedInstanceConfigResponse struct {
	// Optional. Defines whether instances have integrity monitoring enabled.
	EnableIntegrityMonitoring bool `pulumi:"enableIntegrityMonitoring"`
	// Optional. Defines whether instances have Secure Boot enabled.
	EnableSecureBoot bool `pulumi:"enableSecureBoot"`
	// Optional. Defines whether instances have the vTPM enabled.
	EnableVtpm bool `pulumi:"enableVtpm"`
}

// Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
type ShieldedInstanceConfigResponseOutput struct{ *pulumi.OutputState }

func (ShieldedInstanceConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ShieldedInstanceConfigResponse)(nil)).Elem()
}

func (o ShieldedInstanceConfigResponseOutput) ToShieldedInstanceConfigResponseOutput() ShieldedInstanceConfigResponseOutput {
	return o
}

func (o ShieldedInstanceConfigResponseOutput) ToShieldedInstanceConfigResponseOutputWithContext(ctx context.Context) ShieldedInstanceConfigResponseOutput {
	return o
}

// Optional. Defines whether instances have integrity monitoring enabled.
func (o ShieldedInstanceConfigResponseOutput) EnableIntegrityMonitoring() pulumi.BoolOutput {
	return o.ApplyT(func(v ShieldedInstanceConfigResponse) bool { return v.EnableIntegrityMonitoring }).(pulumi.BoolOutput)
}

// Optional. Defines whether instances have Secure Boot enabled.
func (o ShieldedInstanceConfigResponseOutput) EnableSecureBoot() pulumi.BoolOutput {
	return o.ApplyT(func(v ShieldedInstanceConfigResponse) bool { return v.EnableSecureBoot }).(pulumi.BoolOutput)
}

// Optional. Defines whether instances have the vTPM enabled.
func (o ShieldedInstanceConfigResponseOutput) EnableVtpm() pulumi.BoolOutput {
	return o.ApplyT(func(v ShieldedInstanceConfigResponse) bool { return v.EnableVtpm }).(pulumi.BoolOutput)
}

// Specifies the selection and config of software inside the cluster.
type SoftwareConfig struct {
	// Optional. The version of software inside the cluster. It must be one of the supported Dataproc Versions (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
	ImageVersion *string `pulumi:"imageVersion"`
	// Optional. The set of components to activate on the cluster.
	OptionalComponents []SoftwareConfigOptionalComponentsItem `pulumi:"optionalComponents"`
	// Optional. The properties to set on daemon config files.Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. The following are supported prefixes and their mappings: capacity-scheduler: capacity-scheduler.xml core: core-site.xml distcp: distcp-default.xml hdfs: hdfs-site.xml hive: hive-site.xml mapred: mapred-site.xml pig: pig.properties spark: spark-defaults.conf yarn: yarn-site.xmlFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
	Properties map[string]string `pulumi:"properties"`
}

// SoftwareConfigInput is an input type that accepts SoftwareConfigArgs and SoftwareConfigOutput values.
// You can construct a concrete instance of `SoftwareConfigInput` via:
//
//	SoftwareConfigArgs{...}
type SoftwareConfigInput interface {
	pulumi.Input

	ToSoftwareConfigOutput() SoftwareConfigOutput
	ToSoftwareConfigOutputWithContext(context.Context) SoftwareConfigOutput
}

// Specifies the selection and config of software inside the cluster.
type SoftwareConfigArgs struct {
	// Optional. The version of software inside the cluster. It must be one of the supported Dataproc Versions (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
	ImageVersion pulumi.StringPtrInput `pulumi:"imageVersion"`
	// Optional. The set of components to activate on the cluster.
	OptionalComponents SoftwareConfigOptionalComponentsItemArrayInput `pulumi:"optionalComponents"`
	// Optional. The properties to set on daemon config files.Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. The following are supported prefixes and their mappings: capacity-scheduler: capacity-scheduler.xml core: core-site.xml distcp: distcp-default.xml hdfs: hdfs-site.xml hive: hive-site.xml mapred: mapred-site.xml pig: pig.properties spark: spark-defaults.conf yarn: yarn-site.xmlFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
	Properties pulumi.StringMapInput `pulumi:"properties"`
}

func (SoftwareConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SoftwareConfig)(nil)).Elem()
}

func (i SoftwareConfigArgs) ToSoftwareConfigOutput() SoftwareConfigOutput {
	return i.ToSoftwareConfigOutputWithContext(context.Background())
}

func (i SoftwareConfigArgs) ToSoftwareConfigOutputWithContext(ctx context.Context) SoftwareConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SoftwareConfigOutput)
}

func (i SoftwareConfigArgs) ToSoftwareConfigPtrOutput() SoftwareConfigPtrOutput {
	return i.ToSoftwareConfigPtrOutputWithContext(context.Background())
}

func (i SoftwareConfigArgs) ToSoftwareConfigPtrOutputWithContext(ctx context.Context) SoftwareConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SoftwareConfigOutput).ToSoftwareConfigPtrOutputWithContext(ctx)
}

// SoftwareConfigPtrInput is an input type that accepts SoftwareConfigArgs, SoftwareConfigPtr and SoftwareConfigPtrOutput values.
// You can construct a concrete instance of `SoftwareConfigPtrInput` via:
//
//	        SoftwareConfigArgs{...}
//
//	or:
//
//	        nil
type SoftwareConfigPtrInput interface {
	pulumi.Input

	ToSoftwareConfigPtrOutput() SoftwareConfigPtrOutput
	ToSoftwareConfigPtrOutputWithContext(context.Context) SoftwareConfigPtrOutput
}

type softwareConfigPtrType SoftwareConfigArgs

func SoftwareConfigPtr(v *SoftwareConfigArgs) SoftwareConfigPtrInput {
	return (*softwareConfigPtrType)(v)
}

func (*softwareConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SoftwareConfig)(nil)).Elem()
}

func (i *softwareConfigPtrType) ToSoftwareConfigPtrOutput() SoftwareConfigPtrOutput {
	return i.ToSoftwareConfigPtrOutputWithContext(context.Background())
}

func (i *softwareConfigPtrType) ToSoftwareConfigPtrOutputWithContext(ctx context.Context) SoftwareConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SoftwareConfigPtrOutput)
}

// Specifies the selection and config of software inside the cluster.
type SoftwareConfigOutput struct{ *pulumi.OutputState }

func (SoftwareConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SoftwareConfig)(nil)).Elem()
}

func (o SoftwareConfigOutput) ToSoftwareConfigOutput() SoftwareConfigOutput {
	return o
}

func (o SoftwareConfigOutput) ToSoftwareConfigOutputWithContext(ctx context.Context) SoftwareConfigOutput {
	return o
}

func (o SoftwareConfigOutput) ToSoftwareConfigPtrOutput() SoftwareConfigPtrOutput {
	return o.ToSoftwareConfigPtrOutputWithContext(context.Background())
}

func (o SoftwareConfigOutput) ToSoftwareConfigPtrOutputWithContext(ctx context.Context) SoftwareConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SoftwareConfig) *SoftwareConfig {
		return &v
	}).(SoftwareConfigPtrOutput)
}

// Optional. The version of software inside the cluster. It must be one of the supported Dataproc Versions (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
func (o SoftwareConfigOutput) ImageVersion() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SoftwareConfig) *string { return v.ImageVersion }).(pulumi.StringPtrOutput)
}

// Optional. The set of components to activate on the cluster.
func (o SoftwareConfigOutput) OptionalComponents() SoftwareConfigOptionalComponentsItemArrayOutput {
	return o.ApplyT(func(v SoftwareConfig) []SoftwareConfigOptionalComponentsItem { return v.OptionalComponents }).(SoftwareConfigOptionalComponentsItemArrayOutput)
}

// Optional. The properties to set on daemon config files.Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. The following are supported prefixes and their mappings: capacity-scheduler: capacity-scheduler.xml core: core-site.xml distcp: distcp-default.xml hdfs: hdfs-site.xml hive: hive-site.xml mapred: mapred-site.xml pig: pig.properties spark: spark-defaults.conf yarn: yarn-site.xmlFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
func (o SoftwareConfigOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SoftwareConfig) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

type SoftwareConfigPtrOutput struct{ *pulumi.OutputState }

func (SoftwareConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SoftwareConfig)(nil)).Elem()
}

func (o SoftwareConfigPtrOutput) ToSoftwareConfigPtrOutput() SoftwareConfigPtrOutput {
	return o
}

func (o SoftwareConfigPtrOutput) ToSoftwareConfigPtrOutputWithContext(ctx context.Context) SoftwareConfigPtrOutput {
	return o
}

func (o SoftwareConfigPtrOutput) Elem() SoftwareConfigOutput {
	return o.ApplyT(func(v *SoftwareConfig) SoftwareConfig {
		if v != nil {
			return *v
		}
		var ret SoftwareConfig
		return ret
	}).(SoftwareConfigOutput)
}

// Optional. The version of software inside the cluster. It must be one of the supported Dataproc Versions (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
func (o SoftwareConfigPtrOutput) ImageVersion() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SoftwareConfig) *string {
		if v == nil {
			return nil
		}
		return v.ImageVersion
	}).(pulumi.StringPtrOutput)
}

// Optional. The set of components to activate on the cluster.
func (o SoftwareConfigPtrOutput) OptionalComponents() SoftwareConfigOptionalComponentsItemArrayOutput {
	return o.ApplyT(func(v *SoftwareConfig) []SoftwareConfigOptionalComponentsItem {
		if v == nil {
			return nil
		}
		return v.OptionalComponents
	}).(SoftwareConfigOptionalComponentsItemArrayOutput)
}

// Optional. The properties to set on daemon config files.Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. The following are supported prefixes and their mappings: capacity-scheduler: capacity-scheduler.xml core: core-site.xml distcp: distcp-default.xml hdfs: hdfs-site.xml hive: hive-site.xml mapred: mapred-site.xml pig: pig.properties spark: spark-defaults.conf yarn: yarn-site.xmlFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
func (o SoftwareConfigPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SoftwareConfig) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// Specifies the selection and config of software inside the cluster.
type SoftwareConfigResponse struct {
	// Optional. The version of software inside the cluster. It must be one of the supported Dataproc Versions (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
	ImageVersion string `pulumi:"imageVersion"`
	// Optional. The set of components to activate on the cluster.
	OptionalComponents []string `pulumi:"optionalComponents"`
	// Optional. The properties to set on daemon config files.Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. The following are supported prefixes and their mappings: capacity-scheduler: capacity-scheduler.xml core: core-site.xml distcp: distcp-default.xml hdfs: hdfs-site.xml hive: hive-site.xml mapred: mapred-site.xml pig: pig.properties spark: spark-defaults.conf yarn: yarn-site.xmlFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
	Properties map[string]string `pulumi:"properties"`
}

// Specifies the selection and config of software inside the cluster.
type SoftwareConfigResponseOutput struct{ *pulumi.OutputState }

func (SoftwareConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SoftwareConfigResponse)(nil)).Elem()
}

func (o SoftwareConfigResponseOutput) ToSoftwareConfigResponseOutput() SoftwareConfigResponseOutput {
	return o
}

func (o SoftwareConfigResponseOutput) ToSoftwareConfigResponseOutputWithContext(ctx context.Context) SoftwareConfigResponseOutput {
	return o
}

// Optional. The version of software inside the cluster. It must be one of the supported Dataproc Versions (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
func (o SoftwareConfigResponseOutput) ImageVersion() pulumi.StringOutput {
	return o.ApplyT(func(v SoftwareConfigResponse) string { return v.ImageVersion }).(pulumi.StringOutput)
}

// Optional. The set of components to activate on the cluster.
func (o SoftwareConfigResponseOutput) OptionalComponents() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SoftwareConfigResponse) []string { return v.OptionalComponents }).(pulumi.StringArrayOutput)
}

// Optional. The properties to set on daemon config files.Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. The following are supported prefixes and their mappings: capacity-scheduler: capacity-scheduler.xml core: core-site.xml distcp: distcp-default.xml hdfs: hdfs-site.xml hive: hive-site.xml mapred: mapred-site.xml pig: pig.properties spark: spark-defaults.conf yarn: yarn-site.xmlFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
func (o SoftwareConfigResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SoftwareConfigResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// A configuration for running an Apache Spark (https://spark.apache.org/) batch workload.
type SparkBatch struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
	MainClass *string `pulumi:"mainClass"`
	// Optional. The HCFS URI of the jar file that contains the main class.
	MainJarFileUri *string `pulumi:"mainJarFileUri"`
}

// SparkBatchInput is an input type that accepts SparkBatchArgs and SparkBatchOutput values.
// You can construct a concrete instance of `SparkBatchInput` via:
//
//	SparkBatchArgs{...}
type SparkBatchInput interface {
	pulumi.Input

	ToSparkBatchOutput() SparkBatchOutput
	ToSparkBatchOutputWithContext(context.Context) SparkBatchOutput
}

// A configuration for running an Apache Spark (https://spark.apache.org/) batch workload.
type SparkBatchArgs struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
	MainClass pulumi.StringPtrInput `pulumi:"mainClass"`
	// Optional. The HCFS URI of the jar file that contains the main class.
	MainJarFileUri pulumi.StringPtrInput `pulumi:"mainJarFileUri"`
}

func (SparkBatchArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkBatch)(nil)).Elem()
}

func (i SparkBatchArgs) ToSparkBatchOutput() SparkBatchOutput {
	return i.ToSparkBatchOutputWithContext(context.Background())
}

func (i SparkBatchArgs) ToSparkBatchOutputWithContext(ctx context.Context) SparkBatchOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkBatchOutput)
}

func (i SparkBatchArgs) ToSparkBatchPtrOutput() SparkBatchPtrOutput {
	return i.ToSparkBatchPtrOutputWithContext(context.Background())
}

func (i SparkBatchArgs) ToSparkBatchPtrOutputWithContext(ctx context.Context) SparkBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkBatchOutput).ToSparkBatchPtrOutputWithContext(ctx)
}

// SparkBatchPtrInput is an input type that accepts SparkBatchArgs, SparkBatchPtr and SparkBatchPtrOutput values.
// You can construct a concrete instance of `SparkBatchPtrInput` via:
//
//	        SparkBatchArgs{...}
//
//	or:
//
//	        nil
type SparkBatchPtrInput interface {
	pulumi.Input

	ToSparkBatchPtrOutput() SparkBatchPtrOutput
	ToSparkBatchPtrOutputWithContext(context.Context) SparkBatchPtrOutput
}

type sparkBatchPtrType SparkBatchArgs

func SparkBatchPtr(v *SparkBatchArgs) SparkBatchPtrInput {
	return (*sparkBatchPtrType)(v)
}

func (*sparkBatchPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkBatch)(nil)).Elem()
}

func (i *sparkBatchPtrType) ToSparkBatchPtrOutput() SparkBatchPtrOutput {
	return i.ToSparkBatchPtrOutputWithContext(context.Background())
}

func (i *sparkBatchPtrType) ToSparkBatchPtrOutputWithContext(ctx context.Context) SparkBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkBatchPtrOutput)
}

// A configuration for running an Apache Spark (https://spark.apache.org/) batch workload.
type SparkBatchOutput struct{ *pulumi.OutputState }

func (SparkBatchOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkBatch)(nil)).Elem()
}

func (o SparkBatchOutput) ToSparkBatchOutput() SparkBatchOutput {
	return o
}

func (o SparkBatchOutput) ToSparkBatchOutputWithContext(ctx context.Context) SparkBatchOutput {
	return o
}

func (o SparkBatchOutput) ToSparkBatchPtrOutput() SparkBatchPtrOutput {
	return o.ToSparkBatchPtrOutputWithContext(context.Background())
}

func (o SparkBatchOutput) ToSparkBatchPtrOutputWithContext(ctx context.Context) SparkBatchPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkBatch) *SparkBatch {
		return &v
	}).(SparkBatchPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkBatchOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatch) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o SparkBatchOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatch) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o SparkBatchOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatch) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
func (o SparkBatchOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatch) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
func (o SparkBatchOutput) MainClass() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SparkBatch) *string { return v.MainClass }).(pulumi.StringPtrOutput)
}

// Optional. The HCFS URI of the jar file that contains the main class.
func (o SparkBatchOutput) MainJarFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SparkBatch) *string { return v.MainJarFileUri }).(pulumi.StringPtrOutput)
}

type SparkBatchPtrOutput struct{ *pulumi.OutputState }

func (SparkBatchPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkBatch)(nil)).Elem()
}

func (o SparkBatchPtrOutput) ToSparkBatchPtrOutput() SparkBatchPtrOutput {
	return o
}

func (o SparkBatchPtrOutput) ToSparkBatchPtrOutputWithContext(ctx context.Context) SparkBatchPtrOutput {
	return o
}

func (o SparkBatchPtrOutput) Elem() SparkBatchOutput {
	return o.ApplyT(func(v *SparkBatch) SparkBatch {
		if v != nil {
			return *v
		}
		var ret SparkBatch
		return ret
	}).(SparkBatchOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkBatchPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o SparkBatchPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o SparkBatchPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
func (o SparkBatchPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
func (o SparkBatchPtrOutput) MainClass() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkBatch) *string {
		if v == nil {
			return nil
		}
		return v.MainClass
	}).(pulumi.StringPtrOutput)
}

// Optional. The HCFS URI of the jar file that contains the main class.
func (o SparkBatchPtrOutput) MainJarFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkBatch) *string {
		if v == nil {
			return nil
		}
		return v.MainJarFileUri
	}).(pulumi.StringPtrOutput)
}

// A configuration for running an Apache Spark (https://spark.apache.org/) batch workload.
type SparkBatchResponse struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
	MainClass string `pulumi:"mainClass"`
	// Optional. The HCFS URI of the jar file that contains the main class.
	MainJarFileUri string `pulumi:"mainJarFileUri"`
}

// A configuration for running an Apache Spark (https://spark.apache.org/) batch workload.
type SparkBatchResponseOutput struct{ *pulumi.OutputState }

func (SparkBatchResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkBatchResponse)(nil)).Elem()
}

func (o SparkBatchResponseOutput) ToSparkBatchResponseOutput() SparkBatchResponseOutput {
	return o
}

func (o SparkBatchResponseOutput) ToSparkBatchResponseOutputWithContext(ctx context.Context) SparkBatchResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkBatchResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatchResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o SparkBatchResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatchResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o SparkBatchResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatchResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
func (o SparkBatchResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatchResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
func (o SparkBatchResponseOutput) MainClass() pulumi.StringOutput {
	return o.ApplyT(func(v SparkBatchResponse) string { return v.MainClass }).(pulumi.StringOutput)
}

// Optional. The HCFS URI of the jar file that contains the main class.
func (o SparkBatchResponseOutput) MainJarFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkBatchResponse) string { return v.MainJarFileUri }).(pulumi.StringOutput)
}

// Spark History Server configuration for the workload.
type SparkHistoryServerConfig struct {
	// Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
	DataprocCluster *string `pulumi:"dataprocCluster"`
}

// SparkHistoryServerConfigInput is an input type that accepts SparkHistoryServerConfigArgs and SparkHistoryServerConfigOutput values.
// You can construct a concrete instance of `SparkHistoryServerConfigInput` via:
//
//	SparkHistoryServerConfigArgs{...}
type SparkHistoryServerConfigInput interface {
	pulumi.Input

	ToSparkHistoryServerConfigOutput() SparkHistoryServerConfigOutput
	ToSparkHistoryServerConfigOutputWithContext(context.Context) SparkHistoryServerConfigOutput
}

// Spark History Server configuration for the workload.
type SparkHistoryServerConfigArgs struct {
	// Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
	DataprocCluster pulumi.StringPtrInput `pulumi:"dataprocCluster"`
}

func (SparkHistoryServerConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkHistoryServerConfig)(nil)).Elem()
}

func (i SparkHistoryServerConfigArgs) ToSparkHistoryServerConfigOutput() SparkHistoryServerConfigOutput {
	return i.ToSparkHistoryServerConfigOutputWithContext(context.Background())
}

func (i SparkHistoryServerConfigArgs) ToSparkHistoryServerConfigOutputWithContext(ctx context.Context) SparkHistoryServerConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkHistoryServerConfigOutput)
}

func (i SparkHistoryServerConfigArgs) ToSparkHistoryServerConfigPtrOutput() SparkHistoryServerConfigPtrOutput {
	return i.ToSparkHistoryServerConfigPtrOutputWithContext(context.Background())
}

func (i SparkHistoryServerConfigArgs) ToSparkHistoryServerConfigPtrOutputWithContext(ctx context.Context) SparkHistoryServerConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkHistoryServerConfigOutput).ToSparkHistoryServerConfigPtrOutputWithContext(ctx)
}

// SparkHistoryServerConfigPtrInput is an input type that accepts SparkHistoryServerConfigArgs, SparkHistoryServerConfigPtr and SparkHistoryServerConfigPtrOutput values.
// You can construct a concrete instance of `SparkHistoryServerConfigPtrInput` via:
//
//	        SparkHistoryServerConfigArgs{...}
//
//	or:
//
//	        nil
type SparkHistoryServerConfigPtrInput interface {
	pulumi.Input

	ToSparkHistoryServerConfigPtrOutput() SparkHistoryServerConfigPtrOutput
	ToSparkHistoryServerConfigPtrOutputWithContext(context.Context) SparkHistoryServerConfigPtrOutput
}

type sparkHistoryServerConfigPtrType SparkHistoryServerConfigArgs

func SparkHistoryServerConfigPtr(v *SparkHistoryServerConfigArgs) SparkHistoryServerConfigPtrInput {
	return (*sparkHistoryServerConfigPtrType)(v)
}

func (*sparkHistoryServerConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkHistoryServerConfig)(nil)).Elem()
}

func (i *sparkHistoryServerConfigPtrType) ToSparkHistoryServerConfigPtrOutput() SparkHistoryServerConfigPtrOutput {
	return i.ToSparkHistoryServerConfigPtrOutputWithContext(context.Background())
}

func (i *sparkHistoryServerConfigPtrType) ToSparkHistoryServerConfigPtrOutputWithContext(ctx context.Context) SparkHistoryServerConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkHistoryServerConfigPtrOutput)
}

// Spark History Server configuration for the workload.
type SparkHistoryServerConfigOutput struct{ *pulumi.OutputState }

func (SparkHistoryServerConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkHistoryServerConfig)(nil)).Elem()
}

func (o SparkHistoryServerConfigOutput) ToSparkHistoryServerConfigOutput() SparkHistoryServerConfigOutput {
	return o
}

func (o SparkHistoryServerConfigOutput) ToSparkHistoryServerConfigOutputWithContext(ctx context.Context) SparkHistoryServerConfigOutput {
	return o
}

func (o SparkHistoryServerConfigOutput) ToSparkHistoryServerConfigPtrOutput() SparkHistoryServerConfigPtrOutput {
	return o.ToSparkHistoryServerConfigPtrOutputWithContext(context.Background())
}

func (o SparkHistoryServerConfigOutput) ToSparkHistoryServerConfigPtrOutputWithContext(ctx context.Context) SparkHistoryServerConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkHistoryServerConfig) *SparkHistoryServerConfig {
		return &v
	}).(SparkHistoryServerConfigPtrOutput)
}

// Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
func (o SparkHistoryServerConfigOutput) DataprocCluster() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SparkHistoryServerConfig) *string { return v.DataprocCluster }).(pulumi.StringPtrOutput)
}

type SparkHistoryServerConfigPtrOutput struct{ *pulumi.OutputState }

func (SparkHistoryServerConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkHistoryServerConfig)(nil)).Elem()
}

func (o SparkHistoryServerConfigPtrOutput) ToSparkHistoryServerConfigPtrOutput() SparkHistoryServerConfigPtrOutput {
	return o
}

func (o SparkHistoryServerConfigPtrOutput) ToSparkHistoryServerConfigPtrOutputWithContext(ctx context.Context) SparkHistoryServerConfigPtrOutput {
	return o
}

func (o SparkHistoryServerConfigPtrOutput) Elem() SparkHistoryServerConfigOutput {
	return o.ApplyT(func(v *SparkHistoryServerConfig) SparkHistoryServerConfig {
		if v != nil {
			return *v
		}
		var ret SparkHistoryServerConfig
		return ret
	}).(SparkHistoryServerConfigOutput)
}

// Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
func (o SparkHistoryServerConfigPtrOutput) DataprocCluster() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkHistoryServerConfig) *string {
		if v == nil {
			return nil
		}
		return v.DataprocCluster
	}).(pulumi.StringPtrOutput)
}

// Spark History Server configuration for the workload.
type SparkHistoryServerConfigResponse struct {
	// Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
	DataprocCluster string `pulumi:"dataprocCluster"`
}

// Spark History Server configuration for the workload.
type SparkHistoryServerConfigResponseOutput struct{ *pulumi.OutputState }

func (SparkHistoryServerConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkHistoryServerConfigResponse)(nil)).Elem()
}

func (o SparkHistoryServerConfigResponseOutput) ToSparkHistoryServerConfigResponseOutput() SparkHistoryServerConfigResponseOutput {
	return o
}

func (o SparkHistoryServerConfigResponseOutput) ToSparkHistoryServerConfigResponseOutputWithContext(ctx context.Context) SparkHistoryServerConfigResponseOutput {
	return o
}

// Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
func (o SparkHistoryServerConfigResponseOutput) DataprocCluster() pulumi.StringOutput {
	return o.ApplyT(func(v SparkHistoryServerConfigResponse) string { return v.DataprocCluster }).(pulumi.StringOutput)
}

// A Dataproc job for running Apache Spark (https://spark.apache.org/) applications on YARN.
type SparkJob struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
	MainClass *string `pulumi:"mainClass"`
	// The HCFS URI of the jar file that contains the main class.
	MainJarFileUri *string `pulumi:"mainJarFileUri"`
	// Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties map[string]string `pulumi:"properties"`
}

// SparkJobInput is an input type that accepts SparkJobArgs and SparkJobOutput values.
// You can construct a concrete instance of `SparkJobInput` via:
//
//	SparkJobArgs{...}
type SparkJobInput interface {
	pulumi.Input

	ToSparkJobOutput() SparkJobOutput
	ToSparkJobOutputWithContext(context.Context) SparkJobOutput
}

// A Dataproc job for running Apache Spark (https://spark.apache.org/) applications on YARN.
type SparkJobArgs struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
	MainClass pulumi.StringPtrInput `pulumi:"mainClass"`
	// The HCFS URI of the jar file that contains the main class.
	MainJarFileUri pulumi.StringPtrInput `pulumi:"mainJarFileUri"`
	// Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties pulumi.StringMapInput `pulumi:"properties"`
}

func (SparkJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJob)(nil)).Elem()
}

func (i SparkJobArgs) ToSparkJobOutput() SparkJobOutput {
	return i.ToSparkJobOutputWithContext(context.Background())
}

func (i SparkJobArgs) ToSparkJobOutputWithContext(ctx context.Context) SparkJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobOutput)
}

func (i SparkJobArgs) ToSparkJobPtrOutput() SparkJobPtrOutput {
	return i.ToSparkJobPtrOutputWithContext(context.Background())
}

func (i SparkJobArgs) ToSparkJobPtrOutputWithContext(ctx context.Context) SparkJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobOutput).ToSparkJobPtrOutputWithContext(ctx)
}

// SparkJobPtrInput is an input type that accepts SparkJobArgs, SparkJobPtr and SparkJobPtrOutput values.
// You can construct a concrete instance of `SparkJobPtrInput` via:
//
//	        SparkJobArgs{...}
//
//	or:
//
//	        nil
type SparkJobPtrInput interface {
	pulumi.Input

	ToSparkJobPtrOutput() SparkJobPtrOutput
	ToSparkJobPtrOutputWithContext(context.Context) SparkJobPtrOutput
}

type sparkJobPtrType SparkJobArgs

func SparkJobPtr(v *SparkJobArgs) SparkJobPtrInput {
	return (*sparkJobPtrType)(v)
}

func (*sparkJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkJob)(nil)).Elem()
}

func (i *sparkJobPtrType) ToSparkJobPtrOutput() SparkJobPtrOutput {
	return i.ToSparkJobPtrOutputWithContext(context.Background())
}

func (i *sparkJobPtrType) ToSparkJobPtrOutputWithContext(ctx context.Context) SparkJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobPtrOutput)
}

// A Dataproc job for running Apache Spark (https://spark.apache.org/) applications on YARN.
type SparkJobOutput struct{ *pulumi.OutputState }

func (SparkJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJob)(nil)).Elem()
}

func (o SparkJobOutput) ToSparkJobOutput() SparkJobOutput {
	return o
}

func (o SparkJobOutput) ToSparkJobOutputWithContext(ctx context.Context) SparkJobOutput {
	return o
}

func (o SparkJobOutput) ToSparkJobPtrOutput() SparkJobPtrOutput {
	return o.ToSparkJobPtrOutputWithContext(context.Background())
}

func (o SparkJobOutput) ToSparkJobPtrOutputWithContext(ctx context.Context) SparkJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkJob) *SparkJob {
		return &v
	}).(SparkJobPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkJobOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJob) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o SparkJobOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJob) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o SparkJobOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJob) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
func (o SparkJobOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJob) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v SparkJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
func (o SparkJobOutput) MainClass() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SparkJob) *string { return v.MainClass }).(pulumi.StringPtrOutput)
}

// The HCFS URI of the jar file that contains the main class.
func (o SparkJobOutput) MainJarFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SparkJob) *string { return v.MainJarFileUri }).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o SparkJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

type SparkJobPtrOutput struct{ *pulumi.OutputState }

func (SparkJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkJob)(nil)).Elem()
}

func (o SparkJobPtrOutput) ToSparkJobPtrOutput() SparkJobPtrOutput {
	return o
}

func (o SparkJobPtrOutput) ToSparkJobPtrOutputWithContext(ctx context.Context) SparkJobPtrOutput {
	return o
}

func (o SparkJobPtrOutput) Elem() SparkJobOutput {
	return o.ApplyT(func(v *SparkJob) SparkJob {
		if v != nil {
			return *v
		}
		var ret SparkJob
		return ret
	}).(SparkJobOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkJobPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkJob) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o SparkJobPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkJob) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o SparkJobPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkJob) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
func (o SparkJobPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkJob) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *SparkJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
func (o SparkJobPtrOutput) MainClass() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkJob) *string {
		if v == nil {
			return nil
		}
		return v.MainClass
	}).(pulumi.StringPtrOutput)
}

// The HCFS URI of the jar file that contains the main class.
func (o SparkJobPtrOutput) MainJarFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkJob) *string {
		if v == nil {
			return nil
		}
		return v.MainJarFileUri
	}).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o SparkJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SparkJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Spark (https://spark.apache.org/) applications on YARN.
type SparkJobResponse struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
	MainClass string `pulumi:"mainClass"`
	// The HCFS URI of the jar file that contains the main class.
	MainJarFileUri string `pulumi:"mainJarFileUri"`
	// Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties map[string]string `pulumi:"properties"`
}

// A Dataproc job for running Apache Spark (https://spark.apache.org/) applications on YARN.
type SparkJobResponseOutput struct{ *pulumi.OutputState }

func (SparkJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJobResponse)(nil)).Elem()
}

func (o SparkJobResponseOutput) ToSparkJobResponseOutput() SparkJobResponseOutput {
	return o
}

func (o SparkJobResponseOutput) ToSparkJobResponseOutputWithContext(ctx context.Context) SparkJobResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkJobResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJobResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o SparkJobResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJobResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o SparkJobResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJobResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
func (o SparkJobResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJobResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v SparkJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
func (o SparkJobResponseOutput) MainClass() pulumi.StringOutput {
	return o.ApplyT(func(v SparkJobResponse) string { return v.MainClass }).(pulumi.StringOutput)
}

// The HCFS URI of the jar file that contains the main class.
func (o SparkJobResponseOutput) MainJarFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkJobResponse) string { return v.MainJarFileUri }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o SparkJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload.
type SparkRBatch struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris []string `pulumi:"fileUris"`
	// The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
	MainRFileUri string `pulumi:"mainRFileUri"`
}

// SparkRBatchInput is an input type that accepts SparkRBatchArgs and SparkRBatchOutput values.
// You can construct a concrete instance of `SparkRBatchInput` via:
//
//	SparkRBatchArgs{...}
type SparkRBatchInput interface {
	pulumi.Input

	ToSparkRBatchOutput() SparkRBatchOutput
	ToSparkRBatchOutputWithContext(context.Context) SparkRBatchOutput
}

// A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload.
type SparkRBatchArgs struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
	MainRFileUri pulumi.StringInput `pulumi:"mainRFileUri"`
}

func (SparkRBatchArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkRBatch)(nil)).Elem()
}

func (i SparkRBatchArgs) ToSparkRBatchOutput() SparkRBatchOutput {
	return i.ToSparkRBatchOutputWithContext(context.Background())
}

func (i SparkRBatchArgs) ToSparkRBatchOutputWithContext(ctx context.Context) SparkRBatchOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkRBatchOutput)
}

func (i SparkRBatchArgs) ToSparkRBatchPtrOutput() SparkRBatchPtrOutput {
	return i.ToSparkRBatchPtrOutputWithContext(context.Background())
}

func (i SparkRBatchArgs) ToSparkRBatchPtrOutputWithContext(ctx context.Context) SparkRBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkRBatchOutput).ToSparkRBatchPtrOutputWithContext(ctx)
}

// SparkRBatchPtrInput is an input type that accepts SparkRBatchArgs, SparkRBatchPtr and SparkRBatchPtrOutput values.
// You can construct a concrete instance of `SparkRBatchPtrInput` via:
//
//	        SparkRBatchArgs{...}
//
//	or:
//
//	        nil
type SparkRBatchPtrInput interface {
	pulumi.Input

	ToSparkRBatchPtrOutput() SparkRBatchPtrOutput
	ToSparkRBatchPtrOutputWithContext(context.Context) SparkRBatchPtrOutput
}

type sparkRBatchPtrType SparkRBatchArgs

func SparkRBatchPtr(v *SparkRBatchArgs) SparkRBatchPtrInput {
	return (*sparkRBatchPtrType)(v)
}

func (*sparkRBatchPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkRBatch)(nil)).Elem()
}

func (i *sparkRBatchPtrType) ToSparkRBatchPtrOutput() SparkRBatchPtrOutput {
	return i.ToSparkRBatchPtrOutputWithContext(context.Background())
}

func (i *sparkRBatchPtrType) ToSparkRBatchPtrOutputWithContext(ctx context.Context) SparkRBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkRBatchPtrOutput)
}

// A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload.
type SparkRBatchOutput struct{ *pulumi.OutputState }

func (SparkRBatchOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkRBatch)(nil)).Elem()
}

func (o SparkRBatchOutput) ToSparkRBatchOutput() SparkRBatchOutput {
	return o
}

func (o SparkRBatchOutput) ToSparkRBatchOutputWithContext(ctx context.Context) SparkRBatchOutput {
	return o
}

func (o SparkRBatchOutput) ToSparkRBatchPtrOutput() SparkRBatchPtrOutput {
	return o.ToSparkRBatchPtrOutputWithContext(context.Background())
}

func (o SparkRBatchOutput) ToSparkRBatchPtrOutputWithContext(ctx context.Context) SparkRBatchPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkRBatch) *SparkRBatch {
		return &v
	}).(SparkRBatchPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkRBatchOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRBatch) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o SparkRBatchOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRBatch) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o SparkRBatchOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRBatch) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
func (o SparkRBatchOutput) MainRFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkRBatch) string { return v.MainRFileUri }).(pulumi.StringOutput)
}

type SparkRBatchPtrOutput struct{ *pulumi.OutputState }

func (SparkRBatchPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkRBatch)(nil)).Elem()
}

func (o SparkRBatchPtrOutput) ToSparkRBatchPtrOutput() SparkRBatchPtrOutput {
	return o
}

func (o SparkRBatchPtrOutput) ToSparkRBatchPtrOutputWithContext(ctx context.Context) SparkRBatchPtrOutput {
	return o
}

func (o SparkRBatchPtrOutput) Elem() SparkRBatchOutput {
	return o.ApplyT(func(v *SparkRBatch) SparkRBatch {
		if v != nil {
			return *v
		}
		var ret SparkRBatch
		return ret
	}).(SparkRBatchOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkRBatchPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkRBatch) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o SparkRBatchPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkRBatch) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o SparkRBatchPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkRBatch) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
func (o SparkRBatchPtrOutput) MainRFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkRBatch) *string {
		if v == nil {
			return nil
		}
		return &v.MainRFileUri
	}).(pulumi.StringPtrOutput)
}

// A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload.
type SparkRBatchResponse struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris []string `pulumi:"fileUris"`
	// The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
	MainRFileUri string `pulumi:"mainRFileUri"`
}

// A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload.
type SparkRBatchResponseOutput struct{ *pulumi.OutputState }

func (SparkRBatchResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkRBatchResponse)(nil)).Elem()
}

func (o SparkRBatchResponseOutput) ToSparkRBatchResponseOutput() SparkRBatchResponseOutput {
	return o
}

func (o SparkRBatchResponseOutput) ToSparkRBatchResponseOutputWithContext(ctx context.Context) SparkRBatchResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkRBatchResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRBatchResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o SparkRBatchResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRBatchResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o SparkRBatchResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRBatchResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
func (o SparkRBatchResponseOutput) MainRFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkRBatchResponse) string { return v.MainRFileUri }).(pulumi.StringOutput)
}

// A Dataproc job for running Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) applications on YARN.
type SparkRJob struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// The HCFS URI of the main R file to use as the driver. Must be a .R file.
	MainRFileUri string `pulumi:"mainRFileUri"`
	// Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties map[string]string `pulumi:"properties"`
}

// SparkRJobInput is an input type that accepts SparkRJobArgs and SparkRJobOutput values.
// You can construct a concrete instance of `SparkRJobInput` via:
//
//	SparkRJobArgs{...}
type SparkRJobInput interface {
	pulumi.Input

	ToSparkRJobOutput() SparkRJobOutput
	ToSparkRJobOutputWithContext(context.Context) SparkRJobOutput
}

// A Dataproc job for running Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) applications on YARN.
type SparkRJobArgs struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// The HCFS URI of the main R file to use as the driver. Must be a .R file.
	MainRFileUri pulumi.StringInput `pulumi:"mainRFileUri"`
	// Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties pulumi.StringMapInput `pulumi:"properties"`
}

func (SparkRJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkRJob)(nil)).Elem()
}

func (i SparkRJobArgs) ToSparkRJobOutput() SparkRJobOutput {
	return i.ToSparkRJobOutputWithContext(context.Background())
}

func (i SparkRJobArgs) ToSparkRJobOutputWithContext(ctx context.Context) SparkRJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkRJobOutput)
}

func (i SparkRJobArgs) ToSparkRJobPtrOutput() SparkRJobPtrOutput {
	return i.ToSparkRJobPtrOutputWithContext(context.Background())
}

func (i SparkRJobArgs) ToSparkRJobPtrOutputWithContext(ctx context.Context) SparkRJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkRJobOutput).ToSparkRJobPtrOutputWithContext(ctx)
}

// SparkRJobPtrInput is an input type that accepts SparkRJobArgs, SparkRJobPtr and SparkRJobPtrOutput values.
// You can construct a concrete instance of `SparkRJobPtrInput` via:
//
//	        SparkRJobArgs{...}
//
//	or:
//
//	        nil
type SparkRJobPtrInput interface {
	pulumi.Input

	ToSparkRJobPtrOutput() SparkRJobPtrOutput
	ToSparkRJobPtrOutputWithContext(context.Context) SparkRJobPtrOutput
}

type sparkRJobPtrType SparkRJobArgs

func SparkRJobPtr(v *SparkRJobArgs) SparkRJobPtrInput {
	return (*sparkRJobPtrType)(v)
}

func (*sparkRJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkRJob)(nil)).Elem()
}

func (i *sparkRJobPtrType) ToSparkRJobPtrOutput() SparkRJobPtrOutput {
	return i.ToSparkRJobPtrOutputWithContext(context.Background())
}

func (i *sparkRJobPtrType) ToSparkRJobPtrOutputWithContext(ctx context.Context) SparkRJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkRJobPtrOutput)
}

// A Dataproc job for running Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) applications on YARN.
type SparkRJobOutput struct{ *pulumi.OutputState }

func (SparkRJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkRJob)(nil)).Elem()
}

func (o SparkRJobOutput) ToSparkRJobOutput() SparkRJobOutput {
	return o
}

func (o SparkRJobOutput) ToSparkRJobOutputWithContext(ctx context.Context) SparkRJobOutput {
	return o
}

func (o SparkRJobOutput) ToSparkRJobPtrOutput() SparkRJobPtrOutput {
	return o.ToSparkRJobPtrOutputWithContext(context.Background())
}

func (o SparkRJobOutput) ToSparkRJobPtrOutputWithContext(ctx context.Context) SparkRJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkRJob) *SparkRJob {
		return &v
	}).(SparkRJobPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkRJobOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRJob) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o SparkRJobOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRJob) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o SparkRJobOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRJob) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkRJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v SparkRJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// The HCFS URI of the main R file to use as the driver. Must be a .R file.
func (o SparkRJobOutput) MainRFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkRJob) string { return v.MainRFileUri }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o SparkRJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkRJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

type SparkRJobPtrOutput struct{ *pulumi.OutputState }

func (SparkRJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkRJob)(nil)).Elem()
}

func (o SparkRJobPtrOutput) ToSparkRJobPtrOutput() SparkRJobPtrOutput {
	return o
}

func (o SparkRJobPtrOutput) ToSparkRJobPtrOutputWithContext(ctx context.Context) SparkRJobPtrOutput {
	return o
}

func (o SparkRJobPtrOutput) Elem() SparkRJobOutput {
	return o.ApplyT(func(v *SparkRJob) SparkRJob {
		if v != nil {
			return *v
		}
		var ret SparkRJob
		return ret
	}).(SparkRJobOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkRJobPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkRJob) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o SparkRJobPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkRJob) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o SparkRJobPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkRJob) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkRJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *SparkRJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// The HCFS URI of the main R file to use as the driver. Must be a .R file.
func (o SparkRJobPtrOutput) MainRFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkRJob) *string {
		if v == nil {
			return nil
		}
		return &v.MainRFileUri
	}).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o SparkRJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SparkRJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) applications on YARN.
type SparkRJobResponse struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// The HCFS URI of the main R file to use as the driver. Must be a .R file.
	MainRFileUri string `pulumi:"mainRFileUri"`
	// Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties map[string]string `pulumi:"properties"`
}

// A Dataproc job for running Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) applications on YARN.
type SparkRJobResponseOutput struct{ *pulumi.OutputState }

func (SparkRJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkRJobResponse)(nil)).Elem()
}

func (o SparkRJobResponseOutput) ToSparkRJobResponseOutput() SparkRJobResponseOutput {
	return o
}

func (o SparkRJobResponseOutput) ToSparkRJobResponseOutputWithContext(ctx context.Context) SparkRJobResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkRJobResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRJobResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o SparkRJobResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRJobResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o SparkRJobResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRJobResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkRJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v SparkRJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// The HCFS URI of the main R file to use as the driver. Must be a .R file.
func (o SparkRJobResponseOutput) MainRFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkRJobResponse) string { return v.MainRFileUri }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o SparkRJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkRJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload.
type SparkSqlBatch struct {
	// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris []string `pulumi:"jarFileUris"`
	// The HCFS URI of the script that contains Spark SQL queries to execute.
	QueryFileUri string `pulumi:"queryFileUri"`
	// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	QueryVariables map[string]string `pulumi:"queryVariables"`
}

// SparkSqlBatchInput is an input type that accepts SparkSqlBatchArgs and SparkSqlBatchOutput values.
// You can construct a concrete instance of `SparkSqlBatchInput` via:
//
//	SparkSqlBatchArgs{...}
type SparkSqlBatchInput interface {
	pulumi.Input

	ToSparkSqlBatchOutput() SparkSqlBatchOutput
	ToSparkSqlBatchOutputWithContext(context.Context) SparkSqlBatchOutput
}

// A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload.
type SparkSqlBatchArgs struct {
	// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// The HCFS URI of the script that contains Spark SQL queries to execute.
	QueryFileUri pulumi.StringInput `pulumi:"queryFileUri"`
	// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	QueryVariables pulumi.StringMapInput `pulumi:"queryVariables"`
}

func (SparkSqlBatchArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkSqlBatch)(nil)).Elem()
}

func (i SparkSqlBatchArgs) ToSparkSqlBatchOutput() SparkSqlBatchOutput {
	return i.ToSparkSqlBatchOutputWithContext(context.Background())
}

func (i SparkSqlBatchArgs) ToSparkSqlBatchOutputWithContext(ctx context.Context) SparkSqlBatchOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkSqlBatchOutput)
}

func (i SparkSqlBatchArgs) ToSparkSqlBatchPtrOutput() SparkSqlBatchPtrOutput {
	return i.ToSparkSqlBatchPtrOutputWithContext(context.Background())
}

func (i SparkSqlBatchArgs) ToSparkSqlBatchPtrOutputWithContext(ctx context.Context) SparkSqlBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkSqlBatchOutput).ToSparkSqlBatchPtrOutputWithContext(ctx)
}

// SparkSqlBatchPtrInput is an input type that accepts SparkSqlBatchArgs, SparkSqlBatchPtr and SparkSqlBatchPtrOutput values.
// You can construct a concrete instance of `SparkSqlBatchPtrInput` via:
//
//	        SparkSqlBatchArgs{...}
//
//	or:
//
//	        nil
type SparkSqlBatchPtrInput interface {
	pulumi.Input

	ToSparkSqlBatchPtrOutput() SparkSqlBatchPtrOutput
	ToSparkSqlBatchPtrOutputWithContext(context.Context) SparkSqlBatchPtrOutput
}

type sparkSqlBatchPtrType SparkSqlBatchArgs

func SparkSqlBatchPtr(v *SparkSqlBatchArgs) SparkSqlBatchPtrInput {
	return (*sparkSqlBatchPtrType)(v)
}

func (*sparkSqlBatchPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkSqlBatch)(nil)).Elem()
}

func (i *sparkSqlBatchPtrType) ToSparkSqlBatchPtrOutput() SparkSqlBatchPtrOutput {
	return i.ToSparkSqlBatchPtrOutputWithContext(context.Background())
}

func (i *sparkSqlBatchPtrType) ToSparkSqlBatchPtrOutputWithContext(ctx context.Context) SparkSqlBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkSqlBatchPtrOutput)
}

// A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload.
type SparkSqlBatchOutput struct{ *pulumi.OutputState }

func (SparkSqlBatchOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkSqlBatch)(nil)).Elem()
}

func (o SparkSqlBatchOutput) ToSparkSqlBatchOutput() SparkSqlBatchOutput {
	return o
}

func (o SparkSqlBatchOutput) ToSparkSqlBatchOutputWithContext(ctx context.Context) SparkSqlBatchOutput {
	return o
}

func (o SparkSqlBatchOutput) ToSparkSqlBatchPtrOutput() SparkSqlBatchPtrOutput {
	return o.ToSparkSqlBatchPtrOutputWithContext(context.Background())
}

func (o SparkSqlBatchOutput) ToSparkSqlBatchPtrOutputWithContext(ctx context.Context) SparkSqlBatchPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkSqlBatch) *SparkSqlBatch {
		return &v
	}).(SparkSqlBatchPtrOutput)
}

// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
func (o SparkSqlBatchOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkSqlBatch) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// The HCFS URI of the script that contains Spark SQL queries to execute.
func (o SparkSqlBatchOutput) QueryFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkSqlBatch) string { return v.QueryFileUri }).(pulumi.StringOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
func (o SparkSqlBatchOutput) QueryVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkSqlBatch) map[string]string { return v.QueryVariables }).(pulumi.StringMapOutput)
}

type SparkSqlBatchPtrOutput struct{ *pulumi.OutputState }

func (SparkSqlBatchPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkSqlBatch)(nil)).Elem()
}

func (o SparkSqlBatchPtrOutput) ToSparkSqlBatchPtrOutput() SparkSqlBatchPtrOutput {
	return o
}

func (o SparkSqlBatchPtrOutput) ToSparkSqlBatchPtrOutputWithContext(ctx context.Context) SparkSqlBatchPtrOutput {
	return o
}

func (o SparkSqlBatchPtrOutput) Elem() SparkSqlBatchOutput {
	return o.ApplyT(func(v *SparkSqlBatch) SparkSqlBatch {
		if v != nil {
			return *v
		}
		var ret SparkSqlBatch
		return ret
	}).(SparkSqlBatchOutput)
}

// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
func (o SparkSqlBatchPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkSqlBatch) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// The HCFS URI of the script that contains Spark SQL queries to execute.
func (o SparkSqlBatchPtrOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkSqlBatch) *string {
		if v == nil {
			return nil
		}
		return &v.QueryFileUri
	}).(pulumi.StringPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
func (o SparkSqlBatchPtrOutput) QueryVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SparkSqlBatch) map[string]string {
		if v == nil {
			return nil
		}
		return v.QueryVariables
	}).(pulumi.StringMapOutput)
}

// A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload.
type SparkSqlBatchResponse struct {
	// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris []string `pulumi:"jarFileUris"`
	// The HCFS URI of the script that contains Spark SQL queries to execute.
	QueryFileUri string `pulumi:"queryFileUri"`
	// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	QueryVariables map[string]string `pulumi:"queryVariables"`
}

// A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload.
type SparkSqlBatchResponseOutput struct{ *pulumi.OutputState }

func (SparkSqlBatchResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkSqlBatchResponse)(nil)).Elem()
}

func (o SparkSqlBatchResponseOutput) ToSparkSqlBatchResponseOutput() SparkSqlBatchResponseOutput {
	return o
}

func (o SparkSqlBatchResponseOutput) ToSparkSqlBatchResponseOutputWithContext(ctx context.Context) SparkSqlBatchResponseOutput {
	return o
}

// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
func (o SparkSqlBatchResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkSqlBatchResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// The HCFS URI of the script that contains Spark SQL queries to execute.
func (o SparkSqlBatchResponseOutput) QueryFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkSqlBatchResponse) string { return v.QueryFileUri }).(pulumi.StringOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
func (o SparkSqlBatchResponseOutput) QueryVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkSqlBatchResponse) map[string]string { return v.QueryVariables }).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Spark SQL (https://spark.apache.org/sql/) queries.
type SparkSqlJob struct {
	// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri *string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList *QueryList `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	ScriptVariables map[string]string `pulumi:"scriptVariables"`
}

// SparkSqlJobInput is an input type that accepts SparkSqlJobArgs and SparkSqlJobOutput values.
// You can construct a concrete instance of `SparkSqlJobInput` via:
//
//	SparkSqlJobArgs{...}
type SparkSqlJobInput interface {
	pulumi.Input

	ToSparkSqlJobOutput() SparkSqlJobOutput
	ToSparkSqlJobOutputWithContext(context.Context) SparkSqlJobOutput
}

// A Dataproc job for running Apache Spark SQL (https://spark.apache.org/sql/) queries.
type SparkSqlJobArgs struct {
	// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
	Properties pulumi.StringMapInput `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri pulumi.StringPtrInput `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListPtrInput `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	ScriptVariables pulumi.StringMapInput `pulumi:"scriptVariables"`
}

func (SparkSqlJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkSqlJob)(nil)).Elem()
}

func (i SparkSqlJobArgs) ToSparkSqlJobOutput() SparkSqlJobOutput {
	return i.ToSparkSqlJobOutputWithContext(context.Background())
}

func (i SparkSqlJobArgs) ToSparkSqlJobOutputWithContext(ctx context.Context) SparkSqlJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkSqlJobOutput)
}

func (i SparkSqlJobArgs) ToSparkSqlJobPtrOutput() SparkSqlJobPtrOutput {
	return i.ToSparkSqlJobPtrOutputWithContext(context.Background())
}

func (i SparkSqlJobArgs) ToSparkSqlJobPtrOutputWithContext(ctx context.Context) SparkSqlJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkSqlJobOutput).ToSparkSqlJobPtrOutputWithContext(ctx)
}

// SparkSqlJobPtrInput is an input type that accepts SparkSqlJobArgs, SparkSqlJobPtr and SparkSqlJobPtrOutput values.
// You can construct a concrete instance of `SparkSqlJobPtrInput` via:
//
//	        SparkSqlJobArgs{...}
//
//	or:
//
//	        nil
type SparkSqlJobPtrInput interface {
	pulumi.Input

	ToSparkSqlJobPtrOutput() SparkSqlJobPtrOutput
	ToSparkSqlJobPtrOutputWithContext(context.Context) SparkSqlJobPtrOutput
}

type sparkSqlJobPtrType SparkSqlJobArgs

func SparkSqlJobPtr(v *SparkSqlJobArgs) SparkSqlJobPtrInput {
	return (*sparkSqlJobPtrType)(v)
}

func (*sparkSqlJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkSqlJob)(nil)).Elem()
}

func (i *sparkSqlJobPtrType) ToSparkSqlJobPtrOutput() SparkSqlJobPtrOutput {
	return i.ToSparkSqlJobPtrOutputWithContext(context.Background())
}

func (i *sparkSqlJobPtrType) ToSparkSqlJobPtrOutputWithContext(ctx context.Context) SparkSqlJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkSqlJobPtrOutput)
}

// A Dataproc job for running Apache Spark SQL (https://spark.apache.org/sql/) queries.
type SparkSqlJobOutput struct{ *pulumi.OutputState }

func (SparkSqlJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkSqlJob)(nil)).Elem()
}

func (o SparkSqlJobOutput) ToSparkSqlJobOutput() SparkSqlJobOutput {
	return o
}

func (o SparkSqlJobOutput) ToSparkSqlJobOutputWithContext(ctx context.Context) SparkSqlJobOutput {
	return o
}

func (o SparkSqlJobOutput) ToSparkSqlJobPtrOutput() SparkSqlJobPtrOutput {
	return o.ToSparkSqlJobPtrOutputWithContext(context.Background())
}

func (o SparkSqlJobOutput) ToSparkSqlJobPtrOutputWithContext(ctx context.Context) SparkSqlJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkSqlJob) *SparkSqlJob {
		return &v
	}).(SparkSqlJobPtrOutput)
}

// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
func (o SparkSqlJobOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkSqlJob) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkSqlJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v SparkSqlJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
func (o SparkSqlJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkSqlJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o SparkSqlJobOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SparkSqlJob) *string { return v.QueryFileUri }).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o SparkSqlJobOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v SparkSqlJob) *QueryList { return v.QueryList }).(QueryListPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
func (o SparkSqlJobOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkSqlJob) map[string]string { return v.ScriptVariables }).(pulumi.StringMapOutput)
}

type SparkSqlJobPtrOutput struct{ *pulumi.OutputState }

func (SparkSqlJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkSqlJob)(nil)).Elem()
}

func (o SparkSqlJobPtrOutput) ToSparkSqlJobPtrOutput() SparkSqlJobPtrOutput {
	return o
}

func (o SparkSqlJobPtrOutput) ToSparkSqlJobPtrOutputWithContext(ctx context.Context) SparkSqlJobPtrOutput {
	return o
}

func (o SparkSqlJobPtrOutput) Elem() SparkSqlJobOutput {
	return o.ApplyT(func(v *SparkSqlJob) SparkSqlJob {
		if v != nil {
			return *v
		}
		var ret SparkSqlJob
		return ret
	}).(SparkSqlJobOutput)
}

// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
func (o SparkSqlJobPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkSqlJob) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkSqlJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *SparkSqlJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
func (o SparkSqlJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SparkSqlJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o SparkSqlJobPtrOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkSqlJob) *string {
		if v == nil {
			return nil
		}
		return v.QueryFileUri
	}).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o SparkSqlJobPtrOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v *SparkSqlJob) *QueryList {
		if v == nil {
			return nil
		}
		return v.QueryList
	}).(QueryListPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
func (o SparkSqlJobPtrOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SparkSqlJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.ScriptVariables
	}).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Spark SQL (https://spark.apache.org/sql/) queries.
type SparkSqlJobResponse struct {
	// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListResponse `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	ScriptVariables map[string]string `pulumi:"scriptVariables"`
}

// A Dataproc job for running Apache Spark SQL (https://spark.apache.org/sql/) queries.
type SparkSqlJobResponseOutput struct{ *pulumi.OutputState }

func (SparkSqlJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkSqlJobResponse)(nil)).Elem()
}

func (o SparkSqlJobResponseOutput) ToSparkSqlJobResponseOutput() SparkSqlJobResponseOutput {
	return o
}

func (o SparkSqlJobResponseOutput) ToSparkSqlJobResponseOutputWithContext(ctx context.Context) SparkSqlJobResponseOutput {
	return o
}

// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
func (o SparkSqlJobResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkSqlJobResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkSqlJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v SparkSqlJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
func (o SparkSqlJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkSqlJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o SparkSqlJobResponseOutput) QueryFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkSqlJobResponse) string { return v.QueryFileUri }).(pulumi.StringOutput)
}

// A list of queries.
func (o SparkSqlJobResponseOutput) QueryList() QueryListResponseOutput {
	return o.ApplyT(func(v SparkSqlJobResponse) QueryListResponse { return v.QueryList }).(QueryListResponseOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
func (o SparkSqlJobResponseOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkSqlJobResponse) map[string]string { return v.ScriptVariables }).(pulumi.StringMapOutput)
}

// Basic autoscaling configurations for Spark Standalone.
type SparkStandaloneAutoscalingConfig struct {
	// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decommissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
	GracefulDecommissionTimeout string `pulumi:"gracefulDecommissionTimeout"`
	// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
	ScaleDownFactor float64 `pulumi:"scaleDownFactor"`
	// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleDownMinWorkerFraction *float64 `pulumi:"scaleDownMinWorkerFraction"`
	// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
	ScaleUpFactor float64 `pulumi:"scaleUpFactor"`
	// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleUpMinWorkerFraction *float64 `pulumi:"scaleUpMinWorkerFraction"`
}

// SparkStandaloneAutoscalingConfigInput is an input type that accepts SparkStandaloneAutoscalingConfigArgs and SparkStandaloneAutoscalingConfigOutput values.
// You can construct a concrete instance of `SparkStandaloneAutoscalingConfigInput` via:
//
//	SparkStandaloneAutoscalingConfigArgs{...}
type SparkStandaloneAutoscalingConfigInput interface {
	pulumi.Input

	ToSparkStandaloneAutoscalingConfigOutput() SparkStandaloneAutoscalingConfigOutput
	ToSparkStandaloneAutoscalingConfigOutputWithContext(context.Context) SparkStandaloneAutoscalingConfigOutput
}

// Basic autoscaling configurations for Spark Standalone.
type SparkStandaloneAutoscalingConfigArgs struct {
	// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decommissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
	GracefulDecommissionTimeout pulumi.StringInput `pulumi:"gracefulDecommissionTimeout"`
	// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
	ScaleDownFactor pulumi.Float64Input `pulumi:"scaleDownFactor"`
	// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleDownMinWorkerFraction pulumi.Float64PtrInput `pulumi:"scaleDownMinWorkerFraction"`
	// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
	ScaleUpFactor pulumi.Float64Input `pulumi:"scaleUpFactor"`
	// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleUpMinWorkerFraction pulumi.Float64PtrInput `pulumi:"scaleUpMinWorkerFraction"`
}

func (SparkStandaloneAutoscalingConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkStandaloneAutoscalingConfig)(nil)).Elem()
}

func (i SparkStandaloneAutoscalingConfigArgs) ToSparkStandaloneAutoscalingConfigOutput() SparkStandaloneAutoscalingConfigOutput {
	return i.ToSparkStandaloneAutoscalingConfigOutputWithContext(context.Background())
}

func (i SparkStandaloneAutoscalingConfigArgs) ToSparkStandaloneAutoscalingConfigOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkStandaloneAutoscalingConfigOutput)
}

func (i SparkStandaloneAutoscalingConfigArgs) ToSparkStandaloneAutoscalingConfigPtrOutput() SparkStandaloneAutoscalingConfigPtrOutput {
	return i.ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i SparkStandaloneAutoscalingConfigArgs) ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkStandaloneAutoscalingConfigOutput).ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(ctx)
}

// SparkStandaloneAutoscalingConfigPtrInput is an input type that accepts SparkStandaloneAutoscalingConfigArgs, SparkStandaloneAutoscalingConfigPtr and SparkStandaloneAutoscalingConfigPtrOutput values.
// You can construct a concrete instance of `SparkStandaloneAutoscalingConfigPtrInput` via:
//
//	        SparkStandaloneAutoscalingConfigArgs{...}
//
//	or:
//
//	        nil
type SparkStandaloneAutoscalingConfigPtrInput interface {
	pulumi.Input

	ToSparkStandaloneAutoscalingConfigPtrOutput() SparkStandaloneAutoscalingConfigPtrOutput
	ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(context.Context) SparkStandaloneAutoscalingConfigPtrOutput
}

type sparkStandaloneAutoscalingConfigPtrType SparkStandaloneAutoscalingConfigArgs

func SparkStandaloneAutoscalingConfigPtr(v *SparkStandaloneAutoscalingConfigArgs) SparkStandaloneAutoscalingConfigPtrInput {
	return (*sparkStandaloneAutoscalingConfigPtrType)(v)
}

func (*sparkStandaloneAutoscalingConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkStandaloneAutoscalingConfig)(nil)).Elem()
}

func (i *sparkStandaloneAutoscalingConfigPtrType) ToSparkStandaloneAutoscalingConfigPtrOutput() SparkStandaloneAutoscalingConfigPtrOutput {
	return i.ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i *sparkStandaloneAutoscalingConfigPtrType) ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkStandaloneAutoscalingConfigPtrOutput)
}

// Basic autoscaling configurations for Spark Standalone.
type SparkStandaloneAutoscalingConfigOutput struct{ *pulumi.OutputState }

func (SparkStandaloneAutoscalingConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkStandaloneAutoscalingConfig)(nil)).Elem()
}

func (o SparkStandaloneAutoscalingConfigOutput) ToSparkStandaloneAutoscalingConfigOutput() SparkStandaloneAutoscalingConfigOutput {
	return o
}

func (o SparkStandaloneAutoscalingConfigOutput) ToSparkStandaloneAutoscalingConfigOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigOutput {
	return o
}

func (o SparkStandaloneAutoscalingConfigOutput) ToSparkStandaloneAutoscalingConfigPtrOutput() SparkStandaloneAutoscalingConfigPtrOutput {
	return o.ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (o SparkStandaloneAutoscalingConfigOutput) ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkStandaloneAutoscalingConfig) *SparkStandaloneAutoscalingConfig {
		return &v
	}).(SparkStandaloneAutoscalingConfigPtrOutput)
}

// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decommissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
func (o SparkStandaloneAutoscalingConfigOutput) GracefulDecommissionTimeout() pulumi.StringOutput {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfig) string { return v.GracefulDecommissionTimeout }).(pulumi.StringOutput)
}

// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
func (o SparkStandaloneAutoscalingConfigOutput) ScaleDownFactor() pulumi.Float64Output {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfig) float64 { return v.ScaleDownFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o SparkStandaloneAutoscalingConfigOutput) ScaleDownMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfig) *float64 { return v.ScaleDownMinWorkerFraction }).(pulumi.Float64PtrOutput)
}

// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
func (o SparkStandaloneAutoscalingConfigOutput) ScaleUpFactor() pulumi.Float64Output {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfig) float64 { return v.ScaleUpFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o SparkStandaloneAutoscalingConfigOutput) ScaleUpMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfig) *float64 { return v.ScaleUpMinWorkerFraction }).(pulumi.Float64PtrOutput)
}

type SparkStandaloneAutoscalingConfigPtrOutput struct{ *pulumi.OutputState }

func (SparkStandaloneAutoscalingConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkStandaloneAutoscalingConfig)(nil)).Elem()
}

func (o SparkStandaloneAutoscalingConfigPtrOutput) ToSparkStandaloneAutoscalingConfigPtrOutput() SparkStandaloneAutoscalingConfigPtrOutput {
	return o
}

func (o SparkStandaloneAutoscalingConfigPtrOutput) ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigPtrOutput {
	return o
}

func (o SparkStandaloneAutoscalingConfigPtrOutput) Elem() SparkStandaloneAutoscalingConfigOutput {
	return o.ApplyT(func(v *SparkStandaloneAutoscalingConfig) SparkStandaloneAutoscalingConfig {
		if v != nil {
			return *v
		}
		var ret SparkStandaloneAutoscalingConfig
		return ret
	}).(SparkStandaloneAutoscalingConfigOutput)
}

// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decommissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
func (o SparkStandaloneAutoscalingConfigPtrOutput) GracefulDecommissionTimeout() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkStandaloneAutoscalingConfig) *string {
		if v == nil {
			return nil
		}
		return &v.GracefulDecommissionTimeout
	}).(pulumi.StringPtrOutput)
}

// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
func (o SparkStandaloneAutoscalingConfigPtrOutput) ScaleDownFactor() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *SparkStandaloneAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return &v.ScaleDownFactor
	}).(pulumi.Float64PtrOutput)
}

// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o SparkStandaloneAutoscalingConfigPtrOutput) ScaleDownMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *SparkStandaloneAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return v.ScaleDownMinWorkerFraction
	}).(pulumi.Float64PtrOutput)
}

// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
func (o SparkStandaloneAutoscalingConfigPtrOutput) ScaleUpFactor() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *SparkStandaloneAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return &v.ScaleUpFactor
	}).(pulumi.Float64PtrOutput)
}

// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o SparkStandaloneAutoscalingConfigPtrOutput) ScaleUpMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *SparkStandaloneAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return v.ScaleUpMinWorkerFraction
	}).(pulumi.Float64PtrOutput)
}

// Basic autoscaling configurations for Spark Standalone.
type SparkStandaloneAutoscalingConfigResponse struct {
	// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decommissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
	GracefulDecommissionTimeout string `pulumi:"gracefulDecommissionTimeout"`
	// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
	ScaleDownFactor float64 `pulumi:"scaleDownFactor"`
	// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleDownMinWorkerFraction float64 `pulumi:"scaleDownMinWorkerFraction"`
	// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
	ScaleUpFactor float64 `pulumi:"scaleUpFactor"`
	// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleUpMinWorkerFraction float64 `pulumi:"scaleUpMinWorkerFraction"`
}

// Basic autoscaling configurations for Spark Standalone.
type SparkStandaloneAutoscalingConfigResponseOutput struct{ *pulumi.OutputState }

func (SparkStandaloneAutoscalingConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkStandaloneAutoscalingConfigResponse)(nil)).Elem()
}

func (o SparkStandaloneAutoscalingConfigResponseOutput) ToSparkStandaloneAutoscalingConfigResponseOutput() SparkStandaloneAutoscalingConfigResponseOutput {
	return o
}

func (o SparkStandaloneAutoscalingConfigResponseOutput) ToSparkStandaloneAutoscalingConfigResponseOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigResponseOutput {
	return o
}

// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decommissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
func (o SparkStandaloneAutoscalingConfigResponseOutput) GracefulDecommissionTimeout() pulumi.StringOutput {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfigResponse) string { return v.GracefulDecommissionTimeout }).(pulumi.StringOutput)
}

// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
func (o SparkStandaloneAutoscalingConfigResponseOutput) ScaleDownFactor() pulumi.Float64Output {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfigResponse) float64 { return v.ScaleDownFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o SparkStandaloneAutoscalingConfigResponseOutput) ScaleDownMinWorkerFraction() pulumi.Float64Output {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfigResponse) float64 { return v.ScaleDownMinWorkerFraction }).(pulumi.Float64Output)
}

// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
func (o SparkStandaloneAutoscalingConfigResponseOutput) ScaleUpFactor() pulumi.Float64Output {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfigResponse) float64 { return v.ScaleUpFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o SparkStandaloneAutoscalingConfigResponseOutput) ScaleUpMinWorkerFraction() pulumi.Float64Output {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfigResponse) float64 { return v.ScaleUpMinWorkerFraction }).(pulumi.Float64Output)
}

// Historical state information.
type StateHistoryResponse struct {
	// The state of the batch at this point in history.
	State string `pulumi:"state"`
	// Details about the state at this point in history.
	StateMessage string `pulumi:"stateMessage"`
	// The time when the batch entered the historical state.
	StateStartTime string `pulumi:"stateStartTime"`
}

// Historical state information.
type StateHistoryResponseOutput struct{ *pulumi.OutputState }

func (StateHistoryResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*StateHistoryResponse)(nil)).Elem()
}

func (o StateHistoryResponseOutput) ToStateHistoryResponseOutput() StateHistoryResponseOutput {
	return o
}

func (o StateHistoryResponseOutput) ToStateHistoryResponseOutputWithContext(ctx context.Context) StateHistoryResponseOutput {
	return o
}

// The state of the batch at this point in history.
func (o StateHistoryResponseOutput) State() pulumi.StringOutput {
	return o.ApplyT(func(v StateHistoryResponse) string { return v.State }).(pulumi.StringOutput)
}

// Details about the state at this point in history.
func (o StateHistoryResponseOutput) StateMessage() pulumi.StringOutput {
	return o.ApplyT(func(v StateHistoryResponse) string { return v.StateMessage }).(pulumi.StringOutput)
}

// The time when the batch entered the historical state.
func (o StateHistoryResponseOutput) StateStartTime() pulumi.StringOutput {
	return o.ApplyT(func(v StateHistoryResponse) string { return v.StateStartTime }).(pulumi.StringOutput)
}

type StateHistoryResponseArrayOutput struct{ *pulumi.OutputState }

func (StateHistoryResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]StateHistoryResponse)(nil)).Elem()
}

func (o StateHistoryResponseArrayOutput) ToStateHistoryResponseArrayOutput() StateHistoryResponseArrayOutput {
	return o
}

func (o StateHistoryResponseArrayOutput) ToStateHistoryResponseArrayOutputWithContext(ctx context.Context) StateHistoryResponseArrayOutput {
	return o
}

func (o StateHistoryResponseArrayOutput) Index(i pulumi.IntInput) StateHistoryResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) StateHistoryResponse {
		return vs[0].([]StateHistoryResponse)[vs[1].(int)]
	}).(StateHistoryResponseOutput)
}

// A configurable parameter that replaces one or more fields in the template. Parameterizable fields: - Labels - File uris - Job properties - Job arguments - Script variables - Main class (in HadoopJob and SparkJob) - Zone (in ClusterSelector)
type TemplateParameter struct {
	// Optional. Brief description of the parameter. Must not exceed 1024 characters.
	Description *string `pulumi:"description"`
	// Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths.A field path is similar in syntax to a google.protobuf.FieldMask. For example, a field path that references the zone field of a workflow template's cluster selector would be specified as placement.clusterSelector.zone.Also, field paths can reference fields using the following syntax: Values in maps can be referenced by key: labels'key' placement.clusterSelector.clusterLabels'key' placement.managedCluster.labels'key' placement.clusterSelector.clusterLabels'key' jobs'step-id'.labels'key' Jobs in the jobs list can be referenced by step-id: jobs'step-id'.hadoopJob.mainJarFileUri jobs'step-id'.hiveJob.queryFileUri jobs'step-id'.pySparkJob.mainPythonFileUri jobs'step-id'.hadoopJob.jarFileUris0 jobs'step-id'.hadoopJob.archiveUris0 jobs'step-id'.hadoopJob.fileUris0 jobs'step-id'.pySparkJob.pythonFileUris0 Items in repeated fields can be referenced by a zero-based index: jobs'step-id'.sparkJob.args0 Other examples: jobs'step-id'.hadoopJob.properties'key' jobs'step-id'.hadoopJob.args0 jobs'step-id'.hiveJob.scriptVariables'key' jobs'step-id'.hadoopJob.mainJarFileUri placement.clusterSelector.zoneIt may not be possible to parameterize maps and repeated fields in their entirety since only individual map values and individual items in repeated fields can be referenced. For example, the following field paths are invalid: placement.clusterSelector.clusterLabels jobs'step-id'.sparkJob.args
	Fields []string `pulumi:"fields"`
	// Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
	Name string `pulumi:"name"`
	// Optional. Validation rules to be applied to this parameter's value.
	Validation *ParameterValidation `pulumi:"validation"`
}

// TemplateParameterInput is an input type that accepts TemplateParameterArgs and TemplateParameterOutput values.
// You can construct a concrete instance of `TemplateParameterInput` via:
//
//	TemplateParameterArgs{...}
type TemplateParameterInput interface {
	pulumi.Input

	ToTemplateParameterOutput() TemplateParameterOutput
	ToTemplateParameterOutputWithContext(context.Context) TemplateParameterOutput
}

// A configurable parameter that replaces one or more fields in the template. Parameterizable fields: - Labels - File uris - Job properties - Job arguments - Script variables - Main class (in HadoopJob and SparkJob) - Zone (in ClusterSelector)
type TemplateParameterArgs struct {
	// Optional. Brief description of the parameter. Must not exceed 1024 characters.
	Description pulumi.StringPtrInput `pulumi:"description"`
	// Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths.A field path is similar in syntax to a google.protobuf.FieldMask. For example, a field path that references the zone field of a workflow template's cluster selector would be specified as placement.clusterSelector.zone.Also, field paths can reference fields using the following syntax: Values in maps can be referenced by key: labels'key' placement.clusterSelector.clusterLabels'key' placement.managedCluster.labels'key' placement.clusterSelector.clusterLabels'key' jobs'step-id'.labels'key' Jobs in the jobs list can be referenced by step-id: jobs'step-id'.hadoopJob.mainJarFileUri jobs'step-id'.hiveJob.queryFileUri jobs'step-id'.pySparkJob.mainPythonFileUri jobs'step-id'.hadoopJob.jarFileUris0 jobs'step-id'.hadoopJob.archiveUris0 jobs'step-id'.hadoopJob.fileUris0 jobs'step-id'.pySparkJob.pythonFileUris0 Items in repeated fields can be referenced by a zero-based index: jobs'step-id'.sparkJob.args0 Other examples: jobs'step-id'.hadoopJob.properties'key' jobs'step-id'.hadoopJob.args0 jobs'step-id'.hiveJob.scriptVariables'key' jobs'step-id'.hadoopJob.mainJarFileUri placement.clusterSelector.zoneIt may not be possible to parameterize maps and repeated fields in their entirety since only individual map values and individual items in repeated fields can be referenced. For example, the following field paths are invalid: placement.clusterSelector.clusterLabels jobs'step-id'.sparkJob.args
	Fields pulumi.StringArrayInput `pulumi:"fields"`
	// Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
	Name pulumi.StringInput `pulumi:"name"`
	// Optional. Validation rules to be applied to this parameter's value.
	Validation ParameterValidationPtrInput `pulumi:"validation"`
}

func (TemplateParameterArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*TemplateParameter)(nil)).Elem()
}

func (i TemplateParameterArgs) ToTemplateParameterOutput() TemplateParameterOutput {
	return i.ToTemplateParameterOutputWithContext(context.Background())
}

func (i TemplateParameterArgs) ToTemplateParameterOutputWithContext(ctx context.Context) TemplateParameterOutput {
	return pulumi.ToOutputWithContext(ctx, i).(TemplateParameterOutput)
}

// TemplateParameterArrayInput is an input type that accepts TemplateParameterArray and TemplateParameterArrayOutput values.
// You can construct a concrete instance of `TemplateParameterArrayInput` via:
//
//	TemplateParameterArray{ TemplateParameterArgs{...} }
type TemplateParameterArrayInput interface {
	pulumi.Input

	ToTemplateParameterArrayOutput() TemplateParameterArrayOutput
	ToTemplateParameterArrayOutputWithContext(context.Context) TemplateParameterArrayOutput
}

type TemplateParameterArray []TemplateParameterInput

func (TemplateParameterArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]TemplateParameter)(nil)).Elem()
}

func (i TemplateParameterArray) ToTemplateParameterArrayOutput() TemplateParameterArrayOutput {
	return i.ToTemplateParameterArrayOutputWithContext(context.Background())
}

func (i TemplateParameterArray) ToTemplateParameterArrayOutputWithContext(ctx context.Context) TemplateParameterArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(TemplateParameterArrayOutput)
}

// A configurable parameter that replaces one or more fields in the template. Parameterizable fields: - Labels - File uris - Job properties - Job arguments - Script variables - Main class (in HadoopJob and SparkJob) - Zone (in ClusterSelector)
type TemplateParameterOutput struct{ *pulumi.OutputState }

func (TemplateParameterOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*TemplateParameter)(nil)).Elem()
}

func (o TemplateParameterOutput) ToTemplateParameterOutput() TemplateParameterOutput {
	return o
}

func (o TemplateParameterOutput) ToTemplateParameterOutputWithContext(ctx context.Context) TemplateParameterOutput {
	return o
}

// Optional. Brief description of the parameter. Must not exceed 1024 characters.
func (o TemplateParameterOutput) Description() pulumi.StringPtrOutput {
	return o.ApplyT(func(v TemplateParameter) *string { return v.Description }).(pulumi.StringPtrOutput)
}

// Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths.A field path is similar in syntax to a google.protobuf.FieldMask. For example, a field path that references the zone field of a workflow template's cluster selector would be specified as placement.clusterSelector.zone.Also, field paths can reference fields using the following syntax: Values in maps can be referenced by key: labels'key' placement.clusterSelector.clusterLabels'key' placement.managedCluster.labels'key' placement.clusterSelector.clusterLabels'key' jobs'step-id'.labels'key' Jobs in the jobs list can be referenced by step-id: jobs'step-id'.hadoopJob.mainJarFileUri jobs'step-id'.hiveJob.queryFileUri jobs'step-id'.pySparkJob.mainPythonFileUri jobs'step-id'.hadoopJob.jarFileUris0 jobs'step-id'.hadoopJob.archiveUris0 jobs'step-id'.hadoopJob.fileUris0 jobs'step-id'.pySparkJob.pythonFileUris0 Items in repeated fields can be referenced by a zero-based index: jobs'step-id'.sparkJob.args0 Other examples: jobs'step-id'.hadoopJob.properties'key' jobs'step-id'.hadoopJob.args0 jobs'step-id'.hiveJob.scriptVariables'key' jobs'step-id'.hadoopJob.mainJarFileUri placement.clusterSelector.zoneIt may not be possible to parameterize maps and repeated fields in their entirety since only individual map values and individual items in repeated fields can be referenced. For example, the following field paths are invalid: placement.clusterSelector.clusterLabels jobs'step-id'.sparkJob.args
func (o TemplateParameterOutput) Fields() pulumi.StringArrayOutput {
	return o.ApplyT(func(v TemplateParameter) []string { return v.Fields }).(pulumi.StringArrayOutput)
}

// Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
func (o TemplateParameterOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v TemplateParameter) string { return v.Name }).(pulumi.StringOutput)
}

// Optional. Validation rules to be applied to this parameter's value.
func (o TemplateParameterOutput) Validation() ParameterValidationPtrOutput {
	return o.ApplyT(func(v TemplateParameter) *ParameterValidation { return v.Validation }).(ParameterValidationPtrOutput)
}

type TemplateParameterArrayOutput struct{ *pulumi.OutputState }

func (TemplateParameterArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]TemplateParameter)(nil)).Elem()
}

func (o TemplateParameterArrayOutput) ToTemplateParameterArrayOutput() TemplateParameterArrayOutput {
	return o
}

func (o TemplateParameterArrayOutput) ToTemplateParameterArrayOutputWithContext(ctx context.Context) TemplateParameterArrayOutput {
	return o
}

func (o TemplateParameterArrayOutput) Index(i pulumi.IntInput) TemplateParameterOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) TemplateParameter {
		return vs[0].([]TemplateParameter)[vs[1].(int)]
	}).(TemplateParameterOutput)
}

// A configurable parameter that replaces one or more fields in the template. Parameterizable fields: - Labels - File uris - Job properties - Job arguments - Script variables - Main class (in HadoopJob and SparkJob) - Zone (in ClusterSelector)
type TemplateParameterResponse struct {
	// Optional. Brief description of the parameter. Must not exceed 1024 characters.
	Description string `pulumi:"description"`
	// Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths.A field path is similar in syntax to a google.protobuf.FieldMask. For example, a field path that references the zone field of a workflow template's cluster selector would be specified as placement.clusterSelector.zone.Also, field paths can reference fields using the following syntax: Values in maps can be referenced by key: labels'key' placement.clusterSelector.clusterLabels'key' placement.managedCluster.labels'key' placement.clusterSelector.clusterLabels'key' jobs'step-id'.labels'key' Jobs in the jobs list can be referenced by step-id: jobs'step-id'.hadoopJob.mainJarFileUri jobs'step-id'.hiveJob.queryFileUri jobs'step-id'.pySparkJob.mainPythonFileUri jobs'step-id'.hadoopJob.jarFileUris0 jobs'step-id'.hadoopJob.archiveUris0 jobs'step-id'.hadoopJob.fileUris0 jobs'step-id'.pySparkJob.pythonFileUris0 Items in repeated fields can be referenced by a zero-based index: jobs'step-id'.sparkJob.args0 Other examples: jobs'step-id'.hadoopJob.properties'key' jobs'step-id'.hadoopJob.args0 jobs'step-id'.hiveJob.scriptVariables'key' jobs'step-id'.hadoopJob.mainJarFileUri placement.clusterSelector.zoneIt may not be possible to parameterize maps and repeated fields in their entirety since only individual map values and individual items in repeated fields can be referenced. For example, the following field paths are invalid: placement.clusterSelector.clusterLabels jobs'step-id'.sparkJob.args
	Fields []string `pulumi:"fields"`
	// Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
	Name string `pulumi:"name"`
	// Optional. Validation rules to be applied to this parameter's value.
	Validation ParameterValidationResponse `pulumi:"validation"`
}

// A configurable parameter that replaces one or more fields in the template. Parameterizable fields: - Labels - File uris - Job properties - Job arguments - Script variables - Main class (in HadoopJob and SparkJob) - Zone (in ClusterSelector)
type TemplateParameterResponseOutput struct{ *pulumi.OutputState }

func (TemplateParameterResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*TemplateParameterResponse)(nil)).Elem()
}

func (o TemplateParameterResponseOutput) ToTemplateParameterResponseOutput() TemplateParameterResponseOutput {
	return o
}

func (o TemplateParameterResponseOutput) ToTemplateParameterResponseOutputWithContext(ctx context.Context) TemplateParameterResponseOutput {
	return o
}

// Optional. Brief description of the parameter. Must not exceed 1024 characters.
func (o TemplateParameterResponseOutput) Description() pulumi.StringOutput {
	return o.ApplyT(func(v TemplateParameterResponse) string { return v.Description }).(pulumi.StringOutput)
}

// Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths.A field path is similar in syntax to a google.protobuf.FieldMask. For example, a field path that references the zone field of a workflow template's cluster selector would be specified as placement.clusterSelector.zone.Also, field paths can reference fields using the following syntax: Values in maps can be referenced by key: labels'key' placement.clusterSelector.clusterLabels'key' placement.managedCluster.labels'key' placement.clusterSelector.clusterLabels'key' jobs'step-id'.labels'key' Jobs in the jobs list can be referenced by step-id: jobs'step-id'.hadoopJob.mainJarFileUri jobs'step-id'.hiveJob.queryFileUri jobs'step-id'.pySparkJob.mainPythonFileUri jobs'step-id'.hadoopJob.jarFileUris0 jobs'step-id'.hadoopJob.archiveUris0 jobs'step-id'.hadoopJob.fileUris0 jobs'step-id'.pySparkJob.pythonFileUris0 Items in repeated fields can be referenced by a zero-based index: jobs'step-id'.sparkJob.args0 Other examples: jobs'step-id'.hadoopJob.properties'key' jobs'step-id'.hadoopJob.args0 jobs'step-id'.hiveJob.scriptVariables'key' jobs'step-id'.hadoopJob.mainJarFileUri placement.clusterSelector.zoneIt may not be possible to parameterize maps and repeated fields in their entirety since only individual map values and individual items in repeated fields can be referenced. For example, the following field paths are invalid: placement.clusterSelector.clusterLabels jobs'step-id'.sparkJob.args
func (o TemplateParameterResponseOutput) Fields() pulumi.StringArrayOutput {
	return o.ApplyT(func(v TemplateParameterResponse) []string { return v.Fields }).(pulumi.StringArrayOutput)
}

// Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
func (o TemplateParameterResponseOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v TemplateParameterResponse) string { return v.Name }).(pulumi.StringOutput)
}

// Optional. Validation rules to be applied to this parameter's value.
func (o TemplateParameterResponseOutput) Validation() ParameterValidationResponseOutput {
	return o.ApplyT(func(v TemplateParameterResponse) ParameterValidationResponse { return v.Validation }).(ParameterValidationResponseOutput)
}

type TemplateParameterResponseArrayOutput struct{ *pulumi.OutputState }

func (TemplateParameterResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]TemplateParameterResponse)(nil)).Elem()
}

func (o TemplateParameterResponseArrayOutput) ToTemplateParameterResponseArrayOutput() TemplateParameterResponseArrayOutput {
	return o
}

func (o TemplateParameterResponseArrayOutput) ToTemplateParameterResponseArrayOutputWithContext(ctx context.Context) TemplateParameterResponseArrayOutput {
	return o
}

func (o TemplateParameterResponseArrayOutput) Index(i pulumi.IntInput) TemplateParameterResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) TemplateParameterResponse {
		return vs[0].([]TemplateParameterResponse)[vs[1].(int)]
	}).(TemplateParameterResponseOutput)
}

// A Dataproc job for running Trino (https://trino.io/) queries. IMPORTANT: The Dataproc Trino Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/trino) must be enabled when the cluster is created to submit a Trino job to the cluster.
type TrinoJob struct {
	// Optional. Trino client tags to attach to this query
	ClientTags []string `pulumi:"clientTags"`
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure *bool `pulumi:"continueOnFailure"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// Optional. The format in which query output will be displayed. See the Trino documentation for supported output formats
	OutputFormat *string `pulumi:"outputFormat"`
	// Optional. A mapping of property names to values. Used to set Trino session properties (https://trino.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Trino CLI
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri *string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList *QueryList `pulumi:"queryList"`
}

// TrinoJobInput is an input type that accepts TrinoJobArgs and TrinoJobOutput values.
// You can construct a concrete instance of `TrinoJobInput` via:
//
//	TrinoJobArgs{...}
type TrinoJobInput interface {
	pulumi.Input

	ToTrinoJobOutput() TrinoJobOutput
	ToTrinoJobOutputWithContext(context.Context) TrinoJobOutput
}

// A Dataproc job for running Trino (https://trino.io/) queries. IMPORTANT: The Dataproc Trino Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/trino) must be enabled when the cluster is created to submit a Trino job to the cluster.
type TrinoJobArgs struct {
	// Optional. Trino client tags to attach to this query
	ClientTags pulumi.StringArrayInput `pulumi:"clientTags"`
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure pulumi.BoolPtrInput `pulumi:"continueOnFailure"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// Optional. The format in which query output will be displayed. See the Trino documentation for supported output formats
	OutputFormat pulumi.StringPtrInput `pulumi:"outputFormat"`
	// Optional. A mapping of property names to values. Used to set Trino session properties (https://trino.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Trino CLI
	Properties pulumi.StringMapInput `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri pulumi.StringPtrInput `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListPtrInput `pulumi:"queryList"`
}

func (TrinoJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*TrinoJob)(nil)).Elem()
}

func (i TrinoJobArgs) ToTrinoJobOutput() TrinoJobOutput {
	return i.ToTrinoJobOutputWithContext(context.Background())
}

func (i TrinoJobArgs) ToTrinoJobOutputWithContext(ctx context.Context) TrinoJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(TrinoJobOutput)
}

func (i TrinoJobArgs) ToTrinoJobPtrOutput() TrinoJobPtrOutput {
	return i.ToTrinoJobPtrOutputWithContext(context.Background())
}

func (i TrinoJobArgs) ToTrinoJobPtrOutputWithContext(ctx context.Context) TrinoJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(TrinoJobOutput).ToTrinoJobPtrOutputWithContext(ctx)
}

// TrinoJobPtrInput is an input type that accepts TrinoJobArgs, TrinoJobPtr and TrinoJobPtrOutput values.
// You can construct a concrete instance of `TrinoJobPtrInput` via:
//
//	        TrinoJobArgs{...}
//
//	or:
//
//	        nil
type TrinoJobPtrInput interface {
	pulumi.Input

	ToTrinoJobPtrOutput() TrinoJobPtrOutput
	ToTrinoJobPtrOutputWithContext(context.Context) TrinoJobPtrOutput
}

type trinoJobPtrType TrinoJobArgs

func TrinoJobPtr(v *TrinoJobArgs) TrinoJobPtrInput {
	return (*trinoJobPtrType)(v)
}

func (*trinoJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**TrinoJob)(nil)).Elem()
}

func (i *trinoJobPtrType) ToTrinoJobPtrOutput() TrinoJobPtrOutput {
	return i.ToTrinoJobPtrOutputWithContext(context.Background())
}

func (i *trinoJobPtrType) ToTrinoJobPtrOutputWithContext(ctx context.Context) TrinoJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(TrinoJobPtrOutput)
}

// A Dataproc job for running Trino (https://trino.io/) queries. IMPORTANT: The Dataproc Trino Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/trino) must be enabled when the cluster is created to submit a Trino job to the cluster.
type TrinoJobOutput struct{ *pulumi.OutputState }

func (TrinoJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*TrinoJob)(nil)).Elem()
}

func (o TrinoJobOutput) ToTrinoJobOutput() TrinoJobOutput {
	return o
}

func (o TrinoJobOutput) ToTrinoJobOutputWithContext(ctx context.Context) TrinoJobOutput {
	return o
}

func (o TrinoJobOutput) ToTrinoJobPtrOutput() TrinoJobPtrOutput {
	return o.ToTrinoJobPtrOutputWithContext(context.Background())
}

func (o TrinoJobOutput) ToTrinoJobPtrOutputWithContext(ctx context.Context) TrinoJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v TrinoJob) *TrinoJob {
		return &v
	}).(TrinoJobPtrOutput)
}

// Optional. Trino client tags to attach to this query
func (o TrinoJobOutput) ClientTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v TrinoJob) []string { return v.ClientTags }).(pulumi.StringArrayOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o TrinoJobOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v TrinoJob) *bool { return v.ContinueOnFailure }).(pulumi.BoolPtrOutput)
}

// Optional. The runtime log config for job execution.
func (o TrinoJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v TrinoJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// Optional. The format in which query output will be displayed. See the Trino documentation for supported output formats
func (o TrinoJobOutput) OutputFormat() pulumi.StringPtrOutput {
	return o.ApplyT(func(v TrinoJob) *string { return v.OutputFormat }).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values. Used to set Trino session properties (https://trino.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Trino CLI
func (o TrinoJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v TrinoJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o TrinoJobOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v TrinoJob) *string { return v.QueryFileUri }).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o TrinoJobOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v TrinoJob) *QueryList { return v.QueryList }).(QueryListPtrOutput)
}

type TrinoJobPtrOutput struct{ *pulumi.OutputState }

func (TrinoJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**TrinoJob)(nil)).Elem()
}

func (o TrinoJobPtrOutput) ToTrinoJobPtrOutput() TrinoJobPtrOutput {
	return o
}

func (o TrinoJobPtrOutput) ToTrinoJobPtrOutputWithContext(ctx context.Context) TrinoJobPtrOutput {
	return o
}

func (o TrinoJobPtrOutput) Elem() TrinoJobOutput {
	return o.ApplyT(func(v *TrinoJob) TrinoJob {
		if v != nil {
			return *v
		}
		var ret TrinoJob
		return ret
	}).(TrinoJobOutput)
}

// Optional. Trino client tags to attach to this query
func (o TrinoJobPtrOutput) ClientTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *TrinoJob) []string {
		if v == nil {
			return nil
		}
		return v.ClientTags
	}).(pulumi.StringArrayOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o TrinoJobPtrOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *TrinoJob) *bool {
		if v == nil {
			return nil
		}
		return v.ContinueOnFailure
	}).(pulumi.BoolPtrOutput)
}

// Optional. The runtime log config for job execution.
func (o TrinoJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *TrinoJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// Optional. The format in which query output will be displayed. See the Trino documentation for supported output formats
func (o TrinoJobPtrOutput) OutputFormat() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *TrinoJob) *string {
		if v == nil {
			return nil
		}
		return v.OutputFormat
	}).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values. Used to set Trino session properties (https://trino.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Trino CLI
func (o TrinoJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *TrinoJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o TrinoJobPtrOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *TrinoJob) *string {
		if v == nil {
			return nil
		}
		return v.QueryFileUri
	}).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o TrinoJobPtrOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v *TrinoJob) *QueryList {
		if v == nil {
			return nil
		}
		return v.QueryList
	}).(QueryListPtrOutput)
}

// A Dataproc job for running Trino (https://trino.io/) queries. IMPORTANT: The Dataproc Trino Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/trino) must be enabled when the cluster is created to submit a Trino job to the cluster.
type TrinoJobResponse struct {
	// Optional. Trino client tags to attach to this query
	ClientTags []string `pulumi:"clientTags"`
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure bool `pulumi:"continueOnFailure"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// Optional. The format in which query output will be displayed. See the Trino documentation for supported output formats
	OutputFormat string `pulumi:"outputFormat"`
	// Optional. A mapping of property names to values. Used to set Trino session properties (https://trino.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Trino CLI
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListResponse `pulumi:"queryList"`
}

// A Dataproc job for running Trino (https://trino.io/) queries. IMPORTANT: The Dataproc Trino Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/trino) must be enabled when the cluster is created to submit a Trino job to the cluster.
type TrinoJobResponseOutput struct{ *pulumi.OutputState }

func (TrinoJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*TrinoJobResponse)(nil)).Elem()
}

func (o TrinoJobResponseOutput) ToTrinoJobResponseOutput() TrinoJobResponseOutput {
	return o
}

func (o TrinoJobResponseOutput) ToTrinoJobResponseOutputWithContext(ctx context.Context) TrinoJobResponseOutput {
	return o
}

// Optional. Trino client tags to attach to this query
func (o TrinoJobResponseOutput) ClientTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v TrinoJobResponse) []string { return v.ClientTags }).(pulumi.StringArrayOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o TrinoJobResponseOutput) ContinueOnFailure() pulumi.BoolOutput {
	return o.ApplyT(func(v TrinoJobResponse) bool { return v.ContinueOnFailure }).(pulumi.BoolOutput)
}

// Optional. The runtime log config for job execution.
func (o TrinoJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v TrinoJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// Optional. The format in which query output will be displayed. See the Trino documentation for supported output formats
func (o TrinoJobResponseOutput) OutputFormat() pulumi.StringOutput {
	return o.ApplyT(func(v TrinoJobResponse) string { return v.OutputFormat }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values. Used to set Trino session properties (https://trino.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Trino CLI
func (o TrinoJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v TrinoJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o TrinoJobResponseOutput) QueryFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v TrinoJobResponse) string { return v.QueryFileUri }).(pulumi.StringOutput)
}

// A list of queries.
func (o TrinoJobResponseOutput) QueryList() QueryListResponseOutput {
	return o.ApplyT(func(v TrinoJobResponse) QueryListResponse { return v.QueryList }).(QueryListResponseOutput)
}

// Usage metrics represent approximate total resources consumed by a workload.
type UsageMetricsResponse struct {
	// Optional. DCU (Dataproc Compute Units) usage in (milliDCU x seconds) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
	MilliDcuSeconds string `pulumi:"milliDcuSeconds"`
	// Optional. Shuffle storage usage in (GB x seconds) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
	ShuffleStorageGbSeconds string `pulumi:"shuffleStorageGbSeconds"`
}

// Usage metrics represent approximate total resources consumed by a workload.
type UsageMetricsResponseOutput struct{ *pulumi.OutputState }

func (UsageMetricsResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*UsageMetricsResponse)(nil)).Elem()
}

func (o UsageMetricsResponseOutput) ToUsageMetricsResponseOutput() UsageMetricsResponseOutput {
	return o
}

func (o UsageMetricsResponseOutput) ToUsageMetricsResponseOutputWithContext(ctx context.Context) UsageMetricsResponseOutput {
	return o
}

// Optional. DCU (Dataproc Compute Units) usage in (milliDCU x seconds) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
func (o UsageMetricsResponseOutput) MilliDcuSeconds() pulumi.StringOutput {
	return o.ApplyT(func(v UsageMetricsResponse) string { return v.MilliDcuSeconds }).(pulumi.StringOutput)
}

// Optional. Shuffle storage usage in (GB x seconds) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
func (o UsageMetricsResponseOutput) ShuffleStorageGbSeconds() pulumi.StringOutput {
	return o.ApplyT(func(v UsageMetricsResponse) string { return v.ShuffleStorageGbSeconds }).(pulumi.StringOutput)
}

// The usage snaphot represents the resources consumed by a workload at a specified time.
type UsageSnapshotResponse struct {
	// Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
	MilliDcu string `pulumi:"milliDcu"`
	// Optional. Shuffle Storage in gigabytes (GB). (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing))
	ShuffleStorageGb string `pulumi:"shuffleStorageGb"`
	// Optional. The timestamp of the usage snapshot.
	SnapshotTime string `pulumi:"snapshotTime"`
}

// The usage snaphot represents the resources consumed by a workload at a specified time.
type UsageSnapshotResponseOutput struct{ *pulumi.OutputState }

func (UsageSnapshotResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*UsageSnapshotResponse)(nil)).Elem()
}

func (o UsageSnapshotResponseOutput) ToUsageSnapshotResponseOutput() UsageSnapshotResponseOutput {
	return o
}

func (o UsageSnapshotResponseOutput) ToUsageSnapshotResponseOutputWithContext(ctx context.Context) UsageSnapshotResponseOutput {
	return o
}

// Optional. Milli (one-thousandth) Dataproc Compute Units (DCUs) (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing)).
func (o UsageSnapshotResponseOutput) MilliDcu() pulumi.StringOutput {
	return o.ApplyT(func(v UsageSnapshotResponse) string { return v.MilliDcu }).(pulumi.StringOutput)
}

// Optional. Shuffle Storage in gigabytes (GB). (see Dataproc Serverless pricing (https://cloud.google.com/dataproc-serverless/pricing))
func (o UsageSnapshotResponseOutput) ShuffleStorageGb() pulumi.StringOutput {
	return o.ApplyT(func(v UsageSnapshotResponse) string { return v.ShuffleStorageGb }).(pulumi.StringOutput)
}

// Optional. The timestamp of the usage snapshot.
func (o UsageSnapshotResponseOutput) SnapshotTime() pulumi.StringOutput {
	return o.ApplyT(func(v UsageSnapshotResponse) string { return v.SnapshotTime }).(pulumi.StringOutput)
}

// Validation based on a list of allowed values.
type ValueValidation struct {
	// List of allowed values for the parameter.
	Values []string `pulumi:"values"`
}

// ValueValidationInput is an input type that accepts ValueValidationArgs and ValueValidationOutput values.
// You can construct a concrete instance of `ValueValidationInput` via:
//
//	ValueValidationArgs{...}
type ValueValidationInput interface {
	pulumi.Input

	ToValueValidationOutput() ValueValidationOutput
	ToValueValidationOutputWithContext(context.Context) ValueValidationOutput
}

// Validation based on a list of allowed values.
type ValueValidationArgs struct {
	// List of allowed values for the parameter.
	Values pulumi.StringArrayInput `pulumi:"values"`
}

func (ValueValidationArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ValueValidation)(nil)).Elem()
}

func (i ValueValidationArgs) ToValueValidationOutput() ValueValidationOutput {
	return i.ToValueValidationOutputWithContext(context.Background())
}

func (i ValueValidationArgs) ToValueValidationOutputWithContext(ctx context.Context) ValueValidationOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ValueValidationOutput)
}

func (i ValueValidationArgs) ToValueValidationPtrOutput() ValueValidationPtrOutput {
	return i.ToValueValidationPtrOutputWithContext(context.Background())
}

func (i ValueValidationArgs) ToValueValidationPtrOutputWithContext(ctx context.Context) ValueValidationPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ValueValidationOutput).ToValueValidationPtrOutputWithContext(ctx)
}

// ValueValidationPtrInput is an input type that accepts ValueValidationArgs, ValueValidationPtr and ValueValidationPtrOutput values.
// You can construct a concrete instance of `ValueValidationPtrInput` via:
//
//	        ValueValidationArgs{...}
//
//	or:
//
//	        nil
type ValueValidationPtrInput interface {
	pulumi.Input

	ToValueValidationPtrOutput() ValueValidationPtrOutput
	ToValueValidationPtrOutputWithContext(context.Context) ValueValidationPtrOutput
}

type valueValidationPtrType ValueValidationArgs

func ValueValidationPtr(v *ValueValidationArgs) ValueValidationPtrInput {
	return (*valueValidationPtrType)(v)
}

func (*valueValidationPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ValueValidation)(nil)).Elem()
}

func (i *valueValidationPtrType) ToValueValidationPtrOutput() ValueValidationPtrOutput {
	return i.ToValueValidationPtrOutputWithContext(context.Background())
}

func (i *valueValidationPtrType) ToValueValidationPtrOutputWithContext(ctx context.Context) ValueValidationPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ValueValidationPtrOutput)
}

// Validation based on a list of allowed values.
type ValueValidationOutput struct{ *pulumi.OutputState }

func (ValueValidationOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ValueValidation)(nil)).Elem()
}

func (o ValueValidationOutput) ToValueValidationOutput() ValueValidationOutput {
	return o
}

func (o ValueValidationOutput) ToValueValidationOutputWithContext(ctx context.Context) ValueValidationOutput {
	return o
}

func (o ValueValidationOutput) ToValueValidationPtrOutput() ValueValidationPtrOutput {
	return o.ToValueValidationPtrOutputWithContext(context.Background())
}

func (o ValueValidationOutput) ToValueValidationPtrOutputWithContext(ctx context.Context) ValueValidationPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ValueValidation) *ValueValidation {
		return &v
	}).(ValueValidationPtrOutput)
}

// List of allowed values for the parameter.
func (o ValueValidationOutput) Values() pulumi.StringArrayOutput {
	return o.ApplyT(func(v ValueValidation) []string { return v.Values }).(pulumi.StringArrayOutput)
}

type ValueValidationPtrOutput struct{ *pulumi.OutputState }

func (ValueValidationPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ValueValidation)(nil)).Elem()
}

func (o ValueValidationPtrOutput) ToValueValidationPtrOutput() ValueValidationPtrOutput {
	return o
}

func (o ValueValidationPtrOutput) ToValueValidationPtrOutputWithContext(ctx context.Context) ValueValidationPtrOutput {
	return o
}

func (o ValueValidationPtrOutput) Elem() ValueValidationOutput {
	return o.ApplyT(func(v *ValueValidation) ValueValidation {
		if v != nil {
			return *v
		}
		var ret ValueValidation
		return ret
	}).(ValueValidationOutput)
}

// List of allowed values for the parameter.
func (o ValueValidationPtrOutput) Values() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *ValueValidation) []string {
		if v == nil {
			return nil
		}
		return v.Values
	}).(pulumi.StringArrayOutput)
}

// Validation based on a list of allowed values.
type ValueValidationResponse struct {
	// List of allowed values for the parameter.
	Values []string `pulumi:"values"`
}

// Validation based on a list of allowed values.
type ValueValidationResponseOutput struct{ *pulumi.OutputState }

func (ValueValidationResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ValueValidationResponse)(nil)).Elem()
}

func (o ValueValidationResponseOutput) ToValueValidationResponseOutput() ValueValidationResponseOutput {
	return o
}

func (o ValueValidationResponseOutput) ToValueValidationResponseOutputWithContext(ctx context.Context) ValueValidationResponseOutput {
	return o
}

// List of allowed values for the parameter.
func (o ValueValidationResponseOutput) Values() pulumi.StringArrayOutput {
	return o.ApplyT(func(v ValueValidationResponse) []string { return v.Values }).(pulumi.StringArrayOutput)
}

// The Dataproc cluster config for a cluster that does not directly control the underlying compute resources, such as a Dataproc-on-GKE cluster (https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-overview).
type VirtualClusterConfig struct {
	// Optional. Configuration of auxiliary services used by this cluster.
	AuxiliaryServicesConfig *AuxiliaryServicesConfig `pulumi:"auxiliaryServicesConfig"`
	// The configuration for running the Dataproc cluster on Kubernetes.
	KubernetesClusterConfig KubernetesClusterConfig `pulumi:"kubernetesClusterConfig"`
	// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	StagingBucket *string `pulumi:"stagingBucket"`
}

// VirtualClusterConfigInput is an input type that accepts VirtualClusterConfigArgs and VirtualClusterConfigOutput values.
// You can construct a concrete instance of `VirtualClusterConfigInput` via:
//
//	VirtualClusterConfigArgs{...}
type VirtualClusterConfigInput interface {
	pulumi.Input

	ToVirtualClusterConfigOutput() VirtualClusterConfigOutput
	ToVirtualClusterConfigOutputWithContext(context.Context) VirtualClusterConfigOutput
}

// The Dataproc cluster config for a cluster that does not directly control the underlying compute resources, such as a Dataproc-on-GKE cluster (https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-overview).
type VirtualClusterConfigArgs struct {
	// Optional. Configuration of auxiliary services used by this cluster.
	AuxiliaryServicesConfig AuxiliaryServicesConfigPtrInput `pulumi:"auxiliaryServicesConfig"`
	// The configuration for running the Dataproc cluster on Kubernetes.
	KubernetesClusterConfig KubernetesClusterConfigInput `pulumi:"kubernetesClusterConfig"`
	// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	StagingBucket pulumi.StringPtrInput `pulumi:"stagingBucket"`
}

func (VirtualClusterConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*VirtualClusterConfig)(nil)).Elem()
}

func (i VirtualClusterConfigArgs) ToVirtualClusterConfigOutput() VirtualClusterConfigOutput {
	return i.ToVirtualClusterConfigOutputWithContext(context.Background())
}

func (i VirtualClusterConfigArgs) ToVirtualClusterConfigOutputWithContext(ctx context.Context) VirtualClusterConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(VirtualClusterConfigOutput)
}

func (i VirtualClusterConfigArgs) ToVirtualClusterConfigPtrOutput() VirtualClusterConfigPtrOutput {
	return i.ToVirtualClusterConfigPtrOutputWithContext(context.Background())
}

func (i VirtualClusterConfigArgs) ToVirtualClusterConfigPtrOutputWithContext(ctx context.Context) VirtualClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(VirtualClusterConfigOutput).ToVirtualClusterConfigPtrOutputWithContext(ctx)
}

// VirtualClusterConfigPtrInput is an input type that accepts VirtualClusterConfigArgs, VirtualClusterConfigPtr and VirtualClusterConfigPtrOutput values.
// You can construct a concrete instance of `VirtualClusterConfigPtrInput` via:
//
//	        VirtualClusterConfigArgs{...}
//
//	or:
//
//	        nil
type VirtualClusterConfigPtrInput interface {
	pulumi.Input

	ToVirtualClusterConfigPtrOutput() VirtualClusterConfigPtrOutput
	ToVirtualClusterConfigPtrOutputWithContext(context.Context) VirtualClusterConfigPtrOutput
}

type virtualClusterConfigPtrType VirtualClusterConfigArgs

func VirtualClusterConfigPtr(v *VirtualClusterConfigArgs) VirtualClusterConfigPtrInput {
	return (*virtualClusterConfigPtrType)(v)
}

func (*virtualClusterConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**VirtualClusterConfig)(nil)).Elem()
}

func (i *virtualClusterConfigPtrType) ToVirtualClusterConfigPtrOutput() VirtualClusterConfigPtrOutput {
	return i.ToVirtualClusterConfigPtrOutputWithContext(context.Background())
}

func (i *virtualClusterConfigPtrType) ToVirtualClusterConfigPtrOutputWithContext(ctx context.Context) VirtualClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(VirtualClusterConfigPtrOutput)
}

// The Dataproc cluster config for a cluster that does not directly control the underlying compute resources, such as a Dataproc-on-GKE cluster (https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-overview).
type VirtualClusterConfigOutput struct{ *pulumi.OutputState }

func (VirtualClusterConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*VirtualClusterConfig)(nil)).Elem()
}

func (o VirtualClusterConfigOutput) ToVirtualClusterConfigOutput() VirtualClusterConfigOutput {
	return o
}

func (o VirtualClusterConfigOutput) ToVirtualClusterConfigOutputWithContext(ctx context.Context) VirtualClusterConfigOutput {
	return o
}

func (o VirtualClusterConfigOutput) ToVirtualClusterConfigPtrOutput() VirtualClusterConfigPtrOutput {
	return o.ToVirtualClusterConfigPtrOutputWithContext(context.Background())
}

func (o VirtualClusterConfigOutput) ToVirtualClusterConfigPtrOutputWithContext(ctx context.Context) VirtualClusterConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v VirtualClusterConfig) *VirtualClusterConfig {
		return &v
	}).(VirtualClusterConfigPtrOutput)
}

// Optional. Configuration of auxiliary services used by this cluster.
func (o VirtualClusterConfigOutput) AuxiliaryServicesConfig() AuxiliaryServicesConfigPtrOutput {
	return o.ApplyT(func(v VirtualClusterConfig) *AuxiliaryServicesConfig { return v.AuxiliaryServicesConfig }).(AuxiliaryServicesConfigPtrOutput)
}

// The configuration for running the Dataproc cluster on Kubernetes.
func (o VirtualClusterConfigOutput) KubernetesClusterConfig() KubernetesClusterConfigOutput {
	return o.ApplyT(func(v VirtualClusterConfig) KubernetesClusterConfig { return v.KubernetesClusterConfig }).(KubernetesClusterConfigOutput)
}

// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o VirtualClusterConfigOutput) StagingBucket() pulumi.StringPtrOutput {
	return o.ApplyT(func(v VirtualClusterConfig) *string { return v.StagingBucket }).(pulumi.StringPtrOutput)
}

type VirtualClusterConfigPtrOutput struct{ *pulumi.OutputState }

func (VirtualClusterConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**VirtualClusterConfig)(nil)).Elem()
}

func (o VirtualClusterConfigPtrOutput) ToVirtualClusterConfigPtrOutput() VirtualClusterConfigPtrOutput {
	return o
}

func (o VirtualClusterConfigPtrOutput) ToVirtualClusterConfigPtrOutputWithContext(ctx context.Context) VirtualClusterConfigPtrOutput {
	return o
}

func (o VirtualClusterConfigPtrOutput) Elem() VirtualClusterConfigOutput {
	return o.ApplyT(func(v *VirtualClusterConfig) VirtualClusterConfig {
		if v != nil {
			return *v
		}
		var ret VirtualClusterConfig
		return ret
	}).(VirtualClusterConfigOutput)
}

// Optional. Configuration of auxiliary services used by this cluster.
func (o VirtualClusterConfigPtrOutput) AuxiliaryServicesConfig() AuxiliaryServicesConfigPtrOutput {
	return o.ApplyT(func(v *VirtualClusterConfig) *AuxiliaryServicesConfig {
		if v == nil {
			return nil
		}
		return v.AuxiliaryServicesConfig
	}).(AuxiliaryServicesConfigPtrOutput)
}

// The configuration for running the Dataproc cluster on Kubernetes.
func (o VirtualClusterConfigPtrOutput) KubernetesClusterConfig() KubernetesClusterConfigPtrOutput {
	return o.ApplyT(func(v *VirtualClusterConfig) *KubernetesClusterConfig {
		if v == nil {
			return nil
		}
		return &v.KubernetesClusterConfig
	}).(KubernetesClusterConfigPtrOutput)
}

// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o VirtualClusterConfigPtrOutput) StagingBucket() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *VirtualClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.StagingBucket
	}).(pulumi.StringPtrOutput)
}

// The Dataproc cluster config for a cluster that does not directly control the underlying compute resources, such as a Dataproc-on-GKE cluster (https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-overview).
type VirtualClusterConfigResponse struct {
	// Optional. Configuration of auxiliary services used by this cluster.
	AuxiliaryServicesConfig AuxiliaryServicesConfigResponse `pulumi:"auxiliaryServicesConfig"`
	// The configuration for running the Dataproc cluster on Kubernetes.
	KubernetesClusterConfig KubernetesClusterConfigResponse `pulumi:"kubernetesClusterConfig"`
	// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	StagingBucket string `pulumi:"stagingBucket"`
}

// The Dataproc cluster config for a cluster that does not directly control the underlying compute resources, such as a Dataproc-on-GKE cluster (https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-overview).
type VirtualClusterConfigResponseOutput struct{ *pulumi.OutputState }

func (VirtualClusterConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*VirtualClusterConfigResponse)(nil)).Elem()
}

func (o VirtualClusterConfigResponseOutput) ToVirtualClusterConfigResponseOutput() VirtualClusterConfigResponseOutput {
	return o
}

func (o VirtualClusterConfigResponseOutput) ToVirtualClusterConfigResponseOutputWithContext(ctx context.Context) VirtualClusterConfigResponseOutput {
	return o
}

// Optional. Configuration of auxiliary services used by this cluster.
func (o VirtualClusterConfigResponseOutput) AuxiliaryServicesConfig() AuxiliaryServicesConfigResponseOutput {
	return o.ApplyT(func(v VirtualClusterConfigResponse) AuxiliaryServicesConfigResponse { return v.AuxiliaryServicesConfig }).(AuxiliaryServicesConfigResponseOutput)
}

// The configuration for running the Dataproc cluster on Kubernetes.
func (o VirtualClusterConfigResponseOutput) KubernetesClusterConfig() KubernetesClusterConfigResponseOutput {
	return o.ApplyT(func(v VirtualClusterConfigResponse) KubernetesClusterConfigResponse { return v.KubernetesClusterConfig }).(KubernetesClusterConfigResponseOutput)
}

// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o VirtualClusterConfigResponseOutput) StagingBucket() pulumi.StringOutput {
	return o.ApplyT(func(v VirtualClusterConfigResponse) string { return v.StagingBucket }).(pulumi.StringOutput)
}

// Specifies workflow execution target.Either managed_cluster or cluster_selector is required.
type WorkflowTemplatePlacement struct {
	// Optional. A selector that chooses target cluster for jobs based on metadata.The selector is evaluated at the time each job is submitted.
	ClusterSelector *ClusterSelector `pulumi:"clusterSelector"`
	// A cluster that is managed by the workflow.
	ManagedCluster *ManagedCluster `pulumi:"managedCluster"`
}

// WorkflowTemplatePlacementInput is an input type that accepts WorkflowTemplatePlacementArgs and WorkflowTemplatePlacementOutput values.
// You can construct a concrete instance of `WorkflowTemplatePlacementInput` via:
//
//	WorkflowTemplatePlacementArgs{...}
type WorkflowTemplatePlacementInput interface {
	pulumi.Input

	ToWorkflowTemplatePlacementOutput() WorkflowTemplatePlacementOutput
	ToWorkflowTemplatePlacementOutputWithContext(context.Context) WorkflowTemplatePlacementOutput
}

// Specifies workflow execution target.Either managed_cluster or cluster_selector is required.
type WorkflowTemplatePlacementArgs struct {
	// Optional. A selector that chooses target cluster for jobs based on metadata.The selector is evaluated at the time each job is submitted.
	ClusterSelector ClusterSelectorPtrInput `pulumi:"clusterSelector"`
	// A cluster that is managed by the workflow.
	ManagedCluster ManagedClusterPtrInput `pulumi:"managedCluster"`
}

func (WorkflowTemplatePlacementArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*WorkflowTemplatePlacement)(nil)).Elem()
}

func (i WorkflowTemplatePlacementArgs) ToWorkflowTemplatePlacementOutput() WorkflowTemplatePlacementOutput {
	return i.ToWorkflowTemplatePlacementOutputWithContext(context.Background())
}

func (i WorkflowTemplatePlacementArgs) ToWorkflowTemplatePlacementOutputWithContext(ctx context.Context) WorkflowTemplatePlacementOutput {
	return pulumi.ToOutputWithContext(ctx, i).(WorkflowTemplatePlacementOutput)
}

// Specifies workflow execution target.Either managed_cluster or cluster_selector is required.
type WorkflowTemplatePlacementOutput struct{ *pulumi.OutputState }

func (WorkflowTemplatePlacementOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*WorkflowTemplatePlacement)(nil)).Elem()
}

func (o WorkflowTemplatePlacementOutput) ToWorkflowTemplatePlacementOutput() WorkflowTemplatePlacementOutput {
	return o
}

func (o WorkflowTemplatePlacementOutput) ToWorkflowTemplatePlacementOutputWithContext(ctx context.Context) WorkflowTemplatePlacementOutput {
	return o
}

// Optional. A selector that chooses target cluster for jobs based on metadata.The selector is evaluated at the time each job is submitted.
func (o WorkflowTemplatePlacementOutput) ClusterSelector() ClusterSelectorPtrOutput {
	return o.ApplyT(func(v WorkflowTemplatePlacement) *ClusterSelector { return v.ClusterSelector }).(ClusterSelectorPtrOutput)
}

// A cluster that is managed by the workflow.
func (o WorkflowTemplatePlacementOutput) ManagedCluster() ManagedClusterPtrOutput {
	return o.ApplyT(func(v WorkflowTemplatePlacement) *ManagedCluster { return v.ManagedCluster }).(ManagedClusterPtrOutput)
}

// Specifies workflow execution target.Either managed_cluster or cluster_selector is required.
type WorkflowTemplatePlacementResponse struct {
	// Optional. A selector that chooses target cluster for jobs based on metadata.The selector is evaluated at the time each job is submitted.
	ClusterSelector ClusterSelectorResponse `pulumi:"clusterSelector"`
	// A cluster that is managed by the workflow.
	ManagedCluster ManagedClusterResponse `pulumi:"managedCluster"`
}

// Specifies workflow execution target.Either managed_cluster or cluster_selector is required.
type WorkflowTemplatePlacementResponseOutput struct{ *pulumi.OutputState }

func (WorkflowTemplatePlacementResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*WorkflowTemplatePlacementResponse)(nil)).Elem()
}

func (o WorkflowTemplatePlacementResponseOutput) ToWorkflowTemplatePlacementResponseOutput() WorkflowTemplatePlacementResponseOutput {
	return o
}

func (o WorkflowTemplatePlacementResponseOutput) ToWorkflowTemplatePlacementResponseOutputWithContext(ctx context.Context) WorkflowTemplatePlacementResponseOutput {
	return o
}

// Optional. A selector that chooses target cluster for jobs based on metadata.The selector is evaluated at the time each job is submitted.
func (o WorkflowTemplatePlacementResponseOutput) ClusterSelector() ClusterSelectorResponseOutput {
	return o.ApplyT(func(v WorkflowTemplatePlacementResponse) ClusterSelectorResponse { return v.ClusterSelector }).(ClusterSelectorResponseOutput)
}

// A cluster that is managed by the workflow.
func (o WorkflowTemplatePlacementResponseOutput) ManagedCluster() ManagedClusterResponseOutput {
	return o.ApplyT(func(v WorkflowTemplatePlacementResponse) ManagedClusterResponse { return v.ManagedCluster }).(ManagedClusterResponseOutput)
}

// A YARN application created by a job. Application information is a subset of org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Beta Feature: This report is available for testing purposes only. It may be changed before final release.
type YarnApplicationResponse struct {
	// The application name.
	Name string `pulumi:"name"`
	// The numerical progress of the application, from 1 to 100.
	Progress float64 `pulumi:"progress"`
	// The application state.
	State string `pulumi:"state"`
	// Optional. The HTTP URL of the ApplicationMaster, HistoryServer, or TimelineServer that provides application-specific information. The URL uses the internal hostname, and requires a proxy server for resolution and, possibly, access.
	TrackingUrl string `pulumi:"trackingUrl"`
}

// A YARN application created by a job. Application information is a subset of org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Beta Feature: This report is available for testing purposes only. It may be changed before final release.
type YarnApplicationResponseOutput struct{ *pulumi.OutputState }

func (YarnApplicationResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*YarnApplicationResponse)(nil)).Elem()
}

func (o YarnApplicationResponseOutput) ToYarnApplicationResponseOutput() YarnApplicationResponseOutput {
	return o
}

func (o YarnApplicationResponseOutput) ToYarnApplicationResponseOutputWithContext(ctx context.Context) YarnApplicationResponseOutput {
	return o
}

// The application name.
func (o YarnApplicationResponseOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v YarnApplicationResponse) string { return v.Name }).(pulumi.StringOutput)
}

// The numerical progress of the application, from 1 to 100.
func (o YarnApplicationResponseOutput) Progress() pulumi.Float64Output {
	return o.ApplyT(func(v YarnApplicationResponse) float64 { return v.Progress }).(pulumi.Float64Output)
}

// The application state.
func (o YarnApplicationResponseOutput) State() pulumi.StringOutput {
	return o.ApplyT(func(v YarnApplicationResponse) string { return v.State }).(pulumi.StringOutput)
}

// Optional. The HTTP URL of the ApplicationMaster, HistoryServer, or TimelineServer that provides application-specific information. The URL uses the internal hostname, and requires a proxy server for resolution and, possibly, access.
func (o YarnApplicationResponseOutput) TrackingUrl() pulumi.StringOutput {
	return o.ApplyT(func(v YarnApplicationResponse) string { return v.TrackingUrl }).(pulumi.StringOutput)
}

type YarnApplicationResponseArrayOutput struct{ *pulumi.OutputState }

func (YarnApplicationResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]YarnApplicationResponse)(nil)).Elem()
}

func (o YarnApplicationResponseArrayOutput) ToYarnApplicationResponseArrayOutput() YarnApplicationResponseArrayOutput {
	return o
}

func (o YarnApplicationResponseArrayOutput) ToYarnApplicationResponseArrayOutputWithContext(ctx context.Context) YarnApplicationResponseArrayOutput {
	return o
}

func (o YarnApplicationResponseArrayOutput) Index(i pulumi.IntInput) YarnApplicationResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) YarnApplicationResponse {
		return vs[0].([]YarnApplicationResponse)[vs[1].(int)]
	}).(YarnApplicationResponseOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*AcceleratorConfigInput)(nil)).Elem(), AcceleratorConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*AcceleratorConfigArrayInput)(nil)).Elem(), AcceleratorConfigArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*AutoscalingConfigInput)(nil)).Elem(), AutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*AutoscalingConfigPtrInput)(nil)).Elem(), AutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*AuxiliaryNodeGroupInput)(nil)).Elem(), AuxiliaryNodeGroupArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*AuxiliaryNodeGroupArrayInput)(nil)).Elem(), AuxiliaryNodeGroupArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*AuxiliaryServicesConfigInput)(nil)).Elem(), AuxiliaryServicesConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*AuxiliaryServicesConfigPtrInput)(nil)).Elem(), AuxiliaryServicesConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*BasicAutoscalingAlgorithmInput)(nil)).Elem(), BasicAutoscalingAlgorithmArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*BasicAutoscalingAlgorithmPtrInput)(nil)).Elem(), BasicAutoscalingAlgorithmArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*BasicYarnAutoscalingConfigInput)(nil)).Elem(), BasicYarnAutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*BasicYarnAutoscalingConfigPtrInput)(nil)).Elem(), BasicYarnAutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*BindingInput)(nil)).Elem(), BindingArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*BindingArrayInput)(nil)).Elem(), BindingArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*ClusterConfigInput)(nil)).Elem(), ClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ClusterConfigPtrInput)(nil)).Elem(), ClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ClusterSelectorInput)(nil)).Elem(), ClusterSelectorArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ClusterSelectorPtrInput)(nil)).Elem(), ClusterSelectorArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ConfidentialInstanceConfigInput)(nil)).Elem(), ConfidentialInstanceConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ConfidentialInstanceConfigPtrInput)(nil)).Elem(), ConfidentialInstanceConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*DataprocMetricConfigInput)(nil)).Elem(), DataprocMetricConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*DataprocMetricConfigPtrInput)(nil)).Elem(), DataprocMetricConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*DiskConfigInput)(nil)).Elem(), DiskConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*DiskConfigPtrInput)(nil)).Elem(), DiskConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*DriverSchedulingConfigInput)(nil)).Elem(), DriverSchedulingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*DriverSchedulingConfigPtrInput)(nil)).Elem(), DriverSchedulingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*EncryptionConfigInput)(nil)).Elem(), EncryptionConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*EncryptionConfigPtrInput)(nil)).Elem(), EncryptionConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*EndpointConfigInput)(nil)).Elem(), EndpointConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*EndpointConfigPtrInput)(nil)).Elem(), EndpointConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*EnvironmentConfigInput)(nil)).Elem(), EnvironmentConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*EnvironmentConfigPtrInput)(nil)).Elem(), EnvironmentConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ExecutionConfigInput)(nil)).Elem(), ExecutionConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ExecutionConfigPtrInput)(nil)).Elem(), ExecutionConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ExprInput)(nil)).Elem(), ExprArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ExprPtrInput)(nil)).Elem(), ExprArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GceClusterConfigInput)(nil)).Elem(), GceClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GceClusterConfigPtrInput)(nil)).Elem(), GceClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeClusterConfigInput)(nil)).Elem(), GkeClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeClusterConfigPtrInput)(nil)).Elem(), GkeClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeNodeConfigInput)(nil)).Elem(), GkeNodeConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeNodeConfigPtrInput)(nil)).Elem(), GkeNodeConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeNodePoolAcceleratorConfigInput)(nil)).Elem(), GkeNodePoolAcceleratorConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeNodePoolAcceleratorConfigArrayInput)(nil)).Elem(), GkeNodePoolAcceleratorConfigArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeNodePoolAutoscalingConfigInput)(nil)).Elem(), GkeNodePoolAutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeNodePoolAutoscalingConfigPtrInput)(nil)).Elem(), GkeNodePoolAutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeNodePoolConfigInput)(nil)).Elem(), GkeNodePoolConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeNodePoolConfigPtrInput)(nil)).Elem(), GkeNodePoolConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeNodePoolTargetInput)(nil)).Elem(), GkeNodePoolTargetArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeNodePoolTargetArrayInput)(nil)).Elem(), GkeNodePoolTargetArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*HadoopJobInput)(nil)).Elem(), HadoopJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*HadoopJobPtrInput)(nil)).Elem(), HadoopJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*HiveJobInput)(nil)).Elem(), HiveJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*HiveJobPtrInput)(nil)).Elem(), HiveJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*IdentityConfigInput)(nil)).Elem(), IdentityConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*IdentityConfigPtrInput)(nil)).Elem(), IdentityConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*InstanceGroupAutoscalingPolicyConfigInput)(nil)).Elem(), InstanceGroupAutoscalingPolicyConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*InstanceGroupAutoscalingPolicyConfigPtrInput)(nil)).Elem(), InstanceGroupAutoscalingPolicyConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*InstanceGroupConfigInput)(nil)).Elem(), InstanceGroupConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*InstanceGroupConfigPtrInput)(nil)).Elem(), InstanceGroupConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*JobPlacementInput)(nil)).Elem(), JobPlacementArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*JobReferenceInput)(nil)).Elem(), JobReferenceArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*JobReferencePtrInput)(nil)).Elem(), JobReferenceArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*JobSchedulingInput)(nil)).Elem(), JobSchedulingArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*JobSchedulingPtrInput)(nil)).Elem(), JobSchedulingArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*KerberosConfigInput)(nil)).Elem(), KerberosConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*KerberosConfigPtrInput)(nil)).Elem(), KerberosConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*KubernetesClusterConfigInput)(nil)).Elem(), KubernetesClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*KubernetesClusterConfigPtrInput)(nil)).Elem(), KubernetesClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*KubernetesSoftwareConfigInput)(nil)).Elem(), KubernetesSoftwareConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*KubernetesSoftwareConfigPtrInput)(nil)).Elem(), KubernetesSoftwareConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*LifecycleConfigInput)(nil)).Elem(), LifecycleConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*LifecycleConfigPtrInput)(nil)).Elem(), LifecycleConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*LoggingConfigInput)(nil)).Elem(), LoggingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*LoggingConfigPtrInput)(nil)).Elem(), LoggingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ManagedClusterInput)(nil)).Elem(), ManagedClusterArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ManagedClusterPtrInput)(nil)).Elem(), ManagedClusterArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*MetastoreConfigInput)(nil)).Elem(), MetastoreConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*MetastoreConfigPtrInput)(nil)).Elem(), MetastoreConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*MetricInput)(nil)).Elem(), MetricArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*MetricArrayInput)(nil)).Elem(), MetricArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*NamespacedGkeDeploymentTargetInput)(nil)).Elem(), NamespacedGkeDeploymentTargetArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*NamespacedGkeDeploymentTargetPtrInput)(nil)).Elem(), NamespacedGkeDeploymentTargetArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*NodeGroupTypeInput)(nil)).Elem(), NodeGroupTypeArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*NodeGroupAffinityInput)(nil)).Elem(), NodeGroupAffinityArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*NodeGroupAffinityPtrInput)(nil)).Elem(), NodeGroupAffinityArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*NodeInitializationActionInput)(nil)).Elem(), NodeInitializationActionArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*NodeInitializationActionArrayInput)(nil)).Elem(), NodeInitializationActionArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*OrderedJobInput)(nil)).Elem(), OrderedJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*OrderedJobArrayInput)(nil)).Elem(), OrderedJobArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*ParameterValidationInput)(nil)).Elem(), ParameterValidationArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ParameterValidationPtrInput)(nil)).Elem(), ParameterValidationArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PeripheralsConfigInput)(nil)).Elem(), PeripheralsConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PeripheralsConfigPtrInput)(nil)).Elem(), PeripheralsConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PigJobInput)(nil)).Elem(), PigJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PigJobPtrInput)(nil)).Elem(), PigJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PrestoJobInput)(nil)).Elem(), PrestoJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PrestoJobPtrInput)(nil)).Elem(), PrestoJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PySparkBatchInput)(nil)).Elem(), PySparkBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PySparkBatchPtrInput)(nil)).Elem(), PySparkBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PySparkJobInput)(nil)).Elem(), PySparkJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PySparkJobPtrInput)(nil)).Elem(), PySparkJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*QueryListInput)(nil)).Elem(), QueryListArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*QueryListPtrInput)(nil)).Elem(), QueryListArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*RegexValidationInput)(nil)).Elem(), RegexValidationArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*RegexValidationPtrInput)(nil)).Elem(), RegexValidationArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ReservationAffinityInput)(nil)).Elem(), ReservationAffinityArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ReservationAffinityPtrInput)(nil)).Elem(), ReservationAffinityArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*RuntimeConfigInput)(nil)).Elem(), RuntimeConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*RuntimeConfigPtrInput)(nil)).Elem(), RuntimeConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SecurityConfigInput)(nil)).Elem(), SecurityConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SecurityConfigPtrInput)(nil)).Elem(), SecurityConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ShieldedInstanceConfigInput)(nil)).Elem(), ShieldedInstanceConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ShieldedInstanceConfigPtrInput)(nil)).Elem(), ShieldedInstanceConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SoftwareConfigInput)(nil)).Elem(), SoftwareConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SoftwareConfigPtrInput)(nil)).Elem(), SoftwareConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkBatchInput)(nil)).Elem(), SparkBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkBatchPtrInput)(nil)).Elem(), SparkBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkHistoryServerConfigInput)(nil)).Elem(), SparkHistoryServerConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkHistoryServerConfigPtrInput)(nil)).Elem(), SparkHistoryServerConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkJobInput)(nil)).Elem(), SparkJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkJobPtrInput)(nil)).Elem(), SparkJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkRBatchInput)(nil)).Elem(), SparkRBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkRBatchPtrInput)(nil)).Elem(), SparkRBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkRJobInput)(nil)).Elem(), SparkRJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkRJobPtrInput)(nil)).Elem(), SparkRJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkSqlBatchInput)(nil)).Elem(), SparkSqlBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkSqlBatchPtrInput)(nil)).Elem(), SparkSqlBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkSqlJobInput)(nil)).Elem(), SparkSqlJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkSqlJobPtrInput)(nil)).Elem(), SparkSqlJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkStandaloneAutoscalingConfigInput)(nil)).Elem(), SparkStandaloneAutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkStandaloneAutoscalingConfigPtrInput)(nil)).Elem(), SparkStandaloneAutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*TemplateParameterInput)(nil)).Elem(), TemplateParameterArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*TemplateParameterArrayInput)(nil)).Elem(), TemplateParameterArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*TrinoJobInput)(nil)).Elem(), TrinoJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*TrinoJobPtrInput)(nil)).Elem(), TrinoJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ValueValidationInput)(nil)).Elem(), ValueValidationArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ValueValidationPtrInput)(nil)).Elem(), ValueValidationArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*VirtualClusterConfigInput)(nil)).Elem(), VirtualClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*VirtualClusterConfigPtrInput)(nil)).Elem(), VirtualClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*WorkflowTemplatePlacementInput)(nil)).Elem(), WorkflowTemplatePlacementArgs{})
	pulumi.RegisterOutputType(AcceleratorConfigOutput{})
	pulumi.RegisterOutputType(AcceleratorConfigArrayOutput{})
	pulumi.RegisterOutputType(AcceleratorConfigResponseOutput{})
	pulumi.RegisterOutputType(AcceleratorConfigResponseArrayOutput{})
	pulumi.RegisterOutputType(AutoscalingConfigOutput{})
	pulumi.RegisterOutputType(AutoscalingConfigPtrOutput{})
	pulumi.RegisterOutputType(AutoscalingConfigResponseOutput{})
	pulumi.RegisterOutputType(AuxiliaryNodeGroupOutput{})
	pulumi.RegisterOutputType(AuxiliaryNodeGroupArrayOutput{})
	pulumi.RegisterOutputType(AuxiliaryNodeGroupResponseOutput{})
	pulumi.RegisterOutputType(AuxiliaryNodeGroupResponseArrayOutput{})
	pulumi.RegisterOutputType(AuxiliaryServicesConfigOutput{})
	pulumi.RegisterOutputType(AuxiliaryServicesConfigPtrOutput{})
	pulumi.RegisterOutputType(AuxiliaryServicesConfigResponseOutput{})
	pulumi.RegisterOutputType(BasicAutoscalingAlgorithmOutput{})
	pulumi.RegisterOutputType(BasicAutoscalingAlgorithmPtrOutput{})
	pulumi.RegisterOutputType(BasicAutoscalingAlgorithmResponseOutput{})
	pulumi.RegisterOutputType(BasicYarnAutoscalingConfigOutput{})
	pulumi.RegisterOutputType(BasicYarnAutoscalingConfigPtrOutput{})
	pulumi.RegisterOutputType(BasicYarnAutoscalingConfigResponseOutput{})
	pulumi.RegisterOutputType(BindingOutput{})
	pulumi.RegisterOutputType(BindingArrayOutput{})
	pulumi.RegisterOutputType(BindingResponseOutput{})
	pulumi.RegisterOutputType(BindingResponseArrayOutput{})
	pulumi.RegisterOutputType(ClusterConfigOutput{})
	pulumi.RegisterOutputType(ClusterConfigPtrOutput{})
	pulumi.RegisterOutputType(ClusterConfigResponseOutput{})
	pulumi.RegisterOutputType(ClusterMetricsResponseOutput{})
	pulumi.RegisterOutputType(ClusterSelectorOutput{})
	pulumi.RegisterOutputType(ClusterSelectorPtrOutput{})
	pulumi.RegisterOutputType(ClusterSelectorResponseOutput{})
	pulumi.RegisterOutputType(ClusterStatusResponseOutput{})
	pulumi.RegisterOutputType(ClusterStatusResponseArrayOutput{})
	pulumi.RegisterOutputType(ConfidentialInstanceConfigOutput{})
	pulumi.RegisterOutputType(ConfidentialInstanceConfigPtrOutput{})
	pulumi.RegisterOutputType(ConfidentialInstanceConfigResponseOutput{})
	pulumi.RegisterOutputType(DataprocMetricConfigOutput{})
	pulumi.RegisterOutputType(DataprocMetricConfigPtrOutput{})
	pulumi.RegisterOutputType(DataprocMetricConfigResponseOutput{})
	pulumi.RegisterOutputType(DiskConfigOutput{})
	pulumi.RegisterOutputType(DiskConfigPtrOutput{})
	pulumi.RegisterOutputType(DiskConfigResponseOutput{})
	pulumi.RegisterOutputType(DriverSchedulingConfigOutput{})
	pulumi.RegisterOutputType(DriverSchedulingConfigPtrOutput{})
	pulumi.RegisterOutputType(DriverSchedulingConfigResponseOutput{})
	pulumi.RegisterOutputType(EncryptionConfigOutput{})
	pulumi.RegisterOutputType(EncryptionConfigPtrOutput{})
	pulumi.RegisterOutputType(EncryptionConfigResponseOutput{})
	pulumi.RegisterOutputType(EndpointConfigOutput{})
	pulumi.RegisterOutputType(EndpointConfigPtrOutput{})
	pulumi.RegisterOutputType(EndpointConfigResponseOutput{})
	pulumi.RegisterOutputType(EnvironmentConfigOutput{})
	pulumi.RegisterOutputType(EnvironmentConfigPtrOutput{})
	pulumi.RegisterOutputType(EnvironmentConfigResponseOutput{})
	pulumi.RegisterOutputType(ExecutionConfigOutput{})
	pulumi.RegisterOutputType(ExecutionConfigPtrOutput{})
	pulumi.RegisterOutputType(ExecutionConfigResponseOutput{})
	pulumi.RegisterOutputType(ExprOutput{})
	pulumi.RegisterOutputType(ExprPtrOutput{})
	pulumi.RegisterOutputType(ExprResponseOutput{})
	pulumi.RegisterOutputType(GceClusterConfigOutput{})
	pulumi.RegisterOutputType(GceClusterConfigPtrOutput{})
	pulumi.RegisterOutputType(GceClusterConfigResponseOutput{})
	pulumi.RegisterOutputType(GkeClusterConfigOutput{})
	pulumi.RegisterOutputType(GkeClusterConfigPtrOutput{})
	pulumi.RegisterOutputType(GkeClusterConfigResponseOutput{})
	pulumi.RegisterOutputType(GkeNodeConfigOutput{})
	pulumi.RegisterOutputType(GkeNodeConfigPtrOutput{})
	pulumi.RegisterOutputType(GkeNodeConfigResponseOutput{})
	pulumi.RegisterOutputType(GkeNodePoolAcceleratorConfigOutput{})
	pulumi.RegisterOutputType(GkeNodePoolAcceleratorConfigArrayOutput{})
	pulumi.RegisterOutputType(GkeNodePoolAcceleratorConfigResponseOutput{})
	pulumi.RegisterOutputType(GkeNodePoolAcceleratorConfigResponseArrayOutput{})
	pulumi.RegisterOutputType(GkeNodePoolAutoscalingConfigOutput{})
	pulumi.RegisterOutputType(GkeNodePoolAutoscalingConfigPtrOutput{})
	pulumi.RegisterOutputType(GkeNodePoolAutoscalingConfigResponseOutput{})
	pulumi.RegisterOutputType(GkeNodePoolConfigOutput{})
	pulumi.RegisterOutputType(GkeNodePoolConfigPtrOutput{})
	pulumi.RegisterOutputType(GkeNodePoolConfigResponseOutput{})
	pulumi.RegisterOutputType(GkeNodePoolTargetOutput{})
	pulumi.RegisterOutputType(GkeNodePoolTargetArrayOutput{})
	pulumi.RegisterOutputType(GkeNodePoolTargetResponseOutput{})
	pulumi.RegisterOutputType(GkeNodePoolTargetResponseArrayOutput{})
	pulumi.RegisterOutputType(HadoopJobOutput{})
	pulumi.RegisterOutputType(HadoopJobPtrOutput{})
	pulumi.RegisterOutputType(HadoopJobResponseOutput{})
	pulumi.RegisterOutputType(HiveJobOutput{})
	pulumi.RegisterOutputType(HiveJobPtrOutput{})
	pulumi.RegisterOutputType(HiveJobResponseOutput{})
	pulumi.RegisterOutputType(IdentityConfigOutput{})
	pulumi.RegisterOutputType(IdentityConfigPtrOutput{})
	pulumi.RegisterOutputType(IdentityConfigResponseOutput{})
	pulumi.RegisterOutputType(InstanceGroupAutoscalingPolicyConfigOutput{})
	pulumi.RegisterOutputType(InstanceGroupAutoscalingPolicyConfigPtrOutput{})
	pulumi.RegisterOutputType(InstanceGroupAutoscalingPolicyConfigResponseOutput{})
	pulumi.RegisterOutputType(InstanceGroupConfigOutput{})
	pulumi.RegisterOutputType(InstanceGroupConfigPtrOutput{})
	pulumi.RegisterOutputType(InstanceGroupConfigResponseOutput{})
	pulumi.RegisterOutputType(InstanceReferenceResponseOutput{})
	pulumi.RegisterOutputType(InstanceReferenceResponseArrayOutput{})
	pulumi.RegisterOutputType(JobPlacementOutput{})
	pulumi.RegisterOutputType(JobPlacementResponseOutput{})
	pulumi.RegisterOutputType(JobReferenceOutput{})
	pulumi.RegisterOutputType(JobReferencePtrOutput{})
	pulumi.RegisterOutputType(JobReferenceResponseOutput{})
	pulumi.RegisterOutputType(JobSchedulingOutput{})
	pulumi.RegisterOutputType(JobSchedulingPtrOutput{})
	pulumi.RegisterOutputType(JobSchedulingResponseOutput{})
	pulumi.RegisterOutputType(JobStatusResponseOutput{})
	pulumi.RegisterOutputType(JobStatusResponseArrayOutput{})
	pulumi.RegisterOutputType(KerberosConfigOutput{})
	pulumi.RegisterOutputType(KerberosConfigPtrOutput{})
	pulumi.RegisterOutputType(KerberosConfigResponseOutput{})
	pulumi.RegisterOutputType(KubernetesClusterConfigOutput{})
	pulumi.RegisterOutputType(KubernetesClusterConfigPtrOutput{})
	pulumi.RegisterOutputType(KubernetesClusterConfigResponseOutput{})
	pulumi.RegisterOutputType(KubernetesSoftwareConfigOutput{})
	pulumi.RegisterOutputType(KubernetesSoftwareConfigPtrOutput{})
	pulumi.RegisterOutputType(KubernetesSoftwareConfigResponseOutput{})
	pulumi.RegisterOutputType(LifecycleConfigOutput{})
	pulumi.RegisterOutputType(LifecycleConfigPtrOutput{})
	pulumi.RegisterOutputType(LifecycleConfigResponseOutput{})
	pulumi.RegisterOutputType(LoggingConfigOutput{})
	pulumi.RegisterOutputType(LoggingConfigPtrOutput{})
	pulumi.RegisterOutputType(LoggingConfigResponseOutput{})
	pulumi.RegisterOutputType(ManagedClusterOutput{})
	pulumi.RegisterOutputType(ManagedClusterPtrOutput{})
	pulumi.RegisterOutputType(ManagedClusterResponseOutput{})
	pulumi.RegisterOutputType(ManagedGroupConfigResponseOutput{})
	pulumi.RegisterOutputType(MetastoreConfigOutput{})
	pulumi.RegisterOutputType(MetastoreConfigPtrOutput{})
	pulumi.RegisterOutputType(MetastoreConfigResponseOutput{})
	pulumi.RegisterOutputType(MetricOutput{})
	pulumi.RegisterOutputType(MetricArrayOutput{})
	pulumi.RegisterOutputType(MetricResponseOutput{})
	pulumi.RegisterOutputType(MetricResponseArrayOutput{})
	pulumi.RegisterOutputType(NamespacedGkeDeploymentTargetOutput{})
	pulumi.RegisterOutputType(NamespacedGkeDeploymentTargetPtrOutput{})
	pulumi.RegisterOutputType(NamespacedGkeDeploymentTargetResponseOutput{})
	pulumi.RegisterOutputType(NodeGroupTypeOutput{})
	pulumi.RegisterOutputType(NodeGroupAffinityOutput{})
	pulumi.RegisterOutputType(NodeGroupAffinityPtrOutput{})
	pulumi.RegisterOutputType(NodeGroupAffinityResponseOutput{})
	pulumi.RegisterOutputType(NodeGroupResponseOutput{})
	pulumi.RegisterOutputType(NodeInitializationActionOutput{})
	pulumi.RegisterOutputType(NodeInitializationActionArrayOutput{})
	pulumi.RegisterOutputType(NodeInitializationActionResponseOutput{})
	pulumi.RegisterOutputType(NodeInitializationActionResponseArrayOutput{})
	pulumi.RegisterOutputType(OrderedJobOutput{})
	pulumi.RegisterOutputType(OrderedJobArrayOutput{})
	pulumi.RegisterOutputType(OrderedJobResponseOutput{})
	pulumi.RegisterOutputType(OrderedJobResponseArrayOutput{})
	pulumi.RegisterOutputType(ParameterValidationOutput{})
	pulumi.RegisterOutputType(ParameterValidationPtrOutput{})
	pulumi.RegisterOutputType(ParameterValidationResponseOutput{})
	pulumi.RegisterOutputType(PeripheralsConfigOutput{})
	pulumi.RegisterOutputType(PeripheralsConfigPtrOutput{})
	pulumi.RegisterOutputType(PeripheralsConfigResponseOutput{})
	pulumi.RegisterOutputType(PigJobOutput{})
	pulumi.RegisterOutputType(PigJobPtrOutput{})
	pulumi.RegisterOutputType(PigJobResponseOutput{})
	pulumi.RegisterOutputType(PrestoJobOutput{})
	pulumi.RegisterOutputType(PrestoJobPtrOutput{})
	pulumi.RegisterOutputType(PrestoJobResponseOutput{})
	pulumi.RegisterOutputType(PySparkBatchOutput{})
	pulumi.RegisterOutputType(PySparkBatchPtrOutput{})
	pulumi.RegisterOutputType(PySparkBatchResponseOutput{})
	pulumi.RegisterOutputType(PySparkJobOutput{})
	pulumi.RegisterOutputType(PySparkJobPtrOutput{})
	pulumi.RegisterOutputType(PySparkJobResponseOutput{})
	pulumi.RegisterOutputType(QueryListOutput{})
	pulumi.RegisterOutputType(QueryListPtrOutput{})
	pulumi.RegisterOutputType(QueryListResponseOutput{})
	pulumi.RegisterOutputType(RegexValidationOutput{})
	pulumi.RegisterOutputType(RegexValidationPtrOutput{})
	pulumi.RegisterOutputType(RegexValidationResponseOutput{})
	pulumi.RegisterOutputType(ReservationAffinityOutput{})
	pulumi.RegisterOutputType(ReservationAffinityPtrOutput{})
	pulumi.RegisterOutputType(ReservationAffinityResponseOutput{})
	pulumi.RegisterOutputType(RuntimeConfigOutput{})
	pulumi.RegisterOutputType(RuntimeConfigPtrOutput{})
	pulumi.RegisterOutputType(RuntimeConfigResponseOutput{})
	pulumi.RegisterOutputType(RuntimeInfoResponseOutput{})
	pulumi.RegisterOutputType(SecurityConfigOutput{})
	pulumi.RegisterOutputType(SecurityConfigPtrOutput{})
	pulumi.RegisterOutputType(SecurityConfigResponseOutput{})
	pulumi.RegisterOutputType(ShieldedInstanceConfigOutput{})
	pulumi.RegisterOutputType(ShieldedInstanceConfigPtrOutput{})
	pulumi.RegisterOutputType(ShieldedInstanceConfigResponseOutput{})
	pulumi.RegisterOutputType(SoftwareConfigOutput{})
	pulumi.RegisterOutputType(SoftwareConfigPtrOutput{})
	pulumi.RegisterOutputType(SoftwareConfigResponseOutput{})
	pulumi.RegisterOutputType(SparkBatchOutput{})
	pulumi.RegisterOutputType(SparkBatchPtrOutput{})
	pulumi.RegisterOutputType(SparkBatchResponseOutput{})
	pulumi.RegisterOutputType(SparkHistoryServerConfigOutput{})
	pulumi.RegisterOutputType(SparkHistoryServerConfigPtrOutput{})
	pulumi.RegisterOutputType(SparkHistoryServerConfigResponseOutput{})
	pulumi.RegisterOutputType(SparkJobOutput{})
	pulumi.RegisterOutputType(SparkJobPtrOutput{})
	pulumi.RegisterOutputType(SparkJobResponseOutput{})
	pulumi.RegisterOutputType(SparkRBatchOutput{})
	pulumi.RegisterOutputType(SparkRBatchPtrOutput{})
	pulumi.RegisterOutputType(SparkRBatchResponseOutput{})
	pulumi.RegisterOutputType(SparkRJobOutput{})
	pulumi.RegisterOutputType(SparkRJobPtrOutput{})
	pulumi.RegisterOutputType(SparkRJobResponseOutput{})
	pulumi.RegisterOutputType(SparkSqlBatchOutput{})
	pulumi.RegisterOutputType(SparkSqlBatchPtrOutput{})
	pulumi.RegisterOutputType(SparkSqlBatchResponseOutput{})
	pulumi.RegisterOutputType(SparkSqlJobOutput{})
	pulumi.RegisterOutputType(SparkSqlJobPtrOutput{})
	pulumi.RegisterOutputType(SparkSqlJobResponseOutput{})
	pulumi.RegisterOutputType(SparkStandaloneAutoscalingConfigOutput{})
	pulumi.RegisterOutputType(SparkStandaloneAutoscalingConfigPtrOutput{})
	pulumi.RegisterOutputType(SparkStandaloneAutoscalingConfigResponseOutput{})
	pulumi.RegisterOutputType(StateHistoryResponseOutput{})
	pulumi.RegisterOutputType(StateHistoryResponseArrayOutput{})
	pulumi.RegisterOutputType(TemplateParameterOutput{})
	pulumi.RegisterOutputType(TemplateParameterArrayOutput{})
	pulumi.RegisterOutputType(TemplateParameterResponseOutput{})
	pulumi.RegisterOutputType(TemplateParameterResponseArrayOutput{})
	pulumi.RegisterOutputType(TrinoJobOutput{})
	pulumi.RegisterOutputType(TrinoJobPtrOutput{})
	pulumi.RegisterOutputType(TrinoJobResponseOutput{})
	pulumi.RegisterOutputType(UsageMetricsResponseOutput{})
	pulumi.RegisterOutputType(UsageSnapshotResponseOutput{})
	pulumi.RegisterOutputType(ValueValidationOutput{})
	pulumi.RegisterOutputType(ValueValidationPtrOutput{})
	pulumi.RegisterOutputType(ValueValidationResponseOutput{})
	pulumi.RegisterOutputType(VirtualClusterConfigOutput{})
	pulumi.RegisterOutputType(VirtualClusterConfigPtrOutput{})
	pulumi.RegisterOutputType(VirtualClusterConfigResponseOutput{})
	pulumi.RegisterOutputType(WorkflowTemplatePlacementOutput{})
	pulumi.RegisterOutputType(WorkflowTemplatePlacementResponseOutput{})
	pulumi.RegisterOutputType(YarnApplicationResponseOutput{})
	pulumi.RegisterOutputType(YarnApplicationResponseArrayOutput{})
}
