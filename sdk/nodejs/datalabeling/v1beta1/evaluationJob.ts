// *** WARNING: this file was generated by the Pulumi SDK Generator. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import { input as inputs, output as outputs, enums } from "../../types";
import * as utilities from "../../utilities";

/**
 * Creates an evaluation job.
 * Auto-naming is currently not supported for this resource.
 */
export class EvaluationJob extends pulumi.CustomResource {
    /**
     * Get an existing EvaluationJob resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, opts?: pulumi.CustomResourceOptions): EvaluationJob {
        return new EvaluationJob(name, undefined as any, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'google-native:datalabeling/v1beta1:EvaluationJob';

    /**
     * Returns true if the given object is an instance of EvaluationJob.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is EvaluationJob {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === EvaluationJob.__pulumiType;
    }

    /**
     * Name of the AnnotationSpecSet describing all the labels that your machine learning model outputs. You must create this resource before you create an evaluation job and provide its name in the following format: "projects/{project_id}/annotationSpecSets/{annotation_spec_set_id}"
     */
    public readonly annotationSpecSet!: pulumi.Output<string>;
    /**
     * Every time the evaluation job runs and an error occurs, the failed attempt is appended to this array.
     */
    public /*out*/ readonly attempts!: pulumi.Output<outputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1AttemptResponse[]>;
    /**
     * Timestamp of when this evaluation job was created.
     */
    public /*out*/ readonly createTime!: pulumi.Output<string>;
    /**
     * Description of the job. The description can be up to 25,000 characters long.
     */
    public readonly description!: pulumi.Output<string>;
    /**
     * Configuration details for the evaluation job.
     */
    public readonly evaluationJobConfig!: pulumi.Output<outputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1EvaluationJobConfigResponse>;
    /**
     * Whether you want Data Labeling Service to provide ground truth labels for prediction input. If you want the service to assign human labelers to annotate your data, set this to `true`. If you want to provide your own ground truth labels in the evaluation job's BigQuery table, set this to `false`.
     */
    public readonly labelMissingGroundTruth!: pulumi.Output<boolean>;
    /**
     * The [AI Platform Prediction model version](/ml-engine/docs/prediction-overview) to be evaluated. Prediction input and output is sampled from this model version. When creating an evaluation job, specify the model version in the following format: "projects/{project_id}/models/{model_name}/versions/{version_name}" There can only be one evaluation job per model version.
     */
    public readonly modelVersion!: pulumi.Output<string>;
    /**
     * After you create a job, Data Labeling Service assigns a name to the job with the following format: "projects/{project_id}/evaluationJobs/ {evaluation_job_id}"
     */
    public /*out*/ readonly name!: pulumi.Output<string>;
    public readonly project!: pulumi.Output<string>;
    /**
     * Describes the interval at which the job runs. This interval must be at least 1 day, and it is rounded to the nearest day. For example, if you specify a 50-hour interval, the job runs every 2 days. You can provide the schedule in [crontab format](/scheduler/docs/configuring/cron-job-schedules) or in an [English-like format](/appengine/docs/standard/python/config/cronref#schedule_format). Regardless of what you specify, the job will run at 10:00 AM UTC. Only the interval from this schedule is used, not the specific time of day.
     */
    public readonly schedule!: pulumi.Output<string>;
    /**
     * Describes the current state of the job.
     */
    public /*out*/ readonly state!: pulumi.Output<string>;

    /**
     * Create a EvaluationJob resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: EvaluationJobArgs, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (!opts.id) {
            if ((!args || args.annotationSpecSet === undefined) && !opts.urn) {
                throw new Error("Missing required property 'annotationSpecSet'");
            }
            if ((!args || args.description === undefined) && !opts.urn) {
                throw new Error("Missing required property 'description'");
            }
            if ((!args || args.evaluationJobConfig === undefined) && !opts.urn) {
                throw new Error("Missing required property 'evaluationJobConfig'");
            }
            if ((!args || args.labelMissingGroundTruth === undefined) && !opts.urn) {
                throw new Error("Missing required property 'labelMissingGroundTruth'");
            }
            if ((!args || args.modelVersion === undefined) && !opts.urn) {
                throw new Error("Missing required property 'modelVersion'");
            }
            if ((!args || args.schedule === undefined) && !opts.urn) {
                throw new Error("Missing required property 'schedule'");
            }
            resourceInputs["annotationSpecSet"] = args ? args.annotationSpecSet : undefined;
            resourceInputs["description"] = args ? args.description : undefined;
            resourceInputs["evaluationJobConfig"] = args ? args.evaluationJobConfig : undefined;
            resourceInputs["labelMissingGroundTruth"] = args ? args.labelMissingGroundTruth : undefined;
            resourceInputs["modelVersion"] = args ? args.modelVersion : undefined;
            resourceInputs["project"] = args ? args.project : undefined;
            resourceInputs["schedule"] = args ? args.schedule : undefined;
            resourceInputs["attempts"] = undefined /*out*/;
            resourceInputs["createTime"] = undefined /*out*/;
            resourceInputs["name"] = undefined /*out*/;
            resourceInputs["state"] = undefined /*out*/;
        } else {
            resourceInputs["annotationSpecSet"] = undefined /*out*/;
            resourceInputs["attempts"] = undefined /*out*/;
            resourceInputs["createTime"] = undefined /*out*/;
            resourceInputs["description"] = undefined /*out*/;
            resourceInputs["evaluationJobConfig"] = undefined /*out*/;
            resourceInputs["labelMissingGroundTruth"] = undefined /*out*/;
            resourceInputs["modelVersion"] = undefined /*out*/;
            resourceInputs["name"] = undefined /*out*/;
            resourceInputs["project"] = undefined /*out*/;
            resourceInputs["schedule"] = undefined /*out*/;
            resourceInputs["state"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(EvaluationJob.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * The set of arguments for constructing a EvaluationJob resource.
 */
export interface EvaluationJobArgs {
    /**
     * Name of the AnnotationSpecSet describing all the labels that your machine learning model outputs. You must create this resource before you create an evaluation job and provide its name in the following format: "projects/{project_id}/annotationSpecSets/{annotation_spec_set_id}"
     */
    annotationSpecSet: pulumi.Input<string>;
    /**
     * Description of the job. The description can be up to 25,000 characters long.
     */
    description: pulumi.Input<string>;
    /**
     * Configuration details for the evaluation job.
     */
    evaluationJobConfig: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1EvaluationJobConfigArgs>;
    /**
     * Whether you want Data Labeling Service to provide ground truth labels for prediction input. If you want the service to assign human labelers to annotate your data, set this to `true`. If you want to provide your own ground truth labels in the evaluation job's BigQuery table, set this to `false`.
     */
    labelMissingGroundTruth: pulumi.Input<boolean>;
    /**
     * The [AI Platform Prediction model version](/ml-engine/docs/prediction-overview) to be evaluated. Prediction input and output is sampled from this model version. When creating an evaluation job, specify the model version in the following format: "projects/{project_id}/models/{model_name}/versions/{version_name}" There can only be one evaluation job per model version.
     */
    modelVersion: pulumi.Input<string>;
    project?: pulumi.Input<string>;
    /**
     * Describes the interval at which the job runs. This interval must be at least 1 day, and it is rounded to the nearest day. For example, if you specify a 50-hour interval, the job runs every 2 days. You can provide the schedule in [crontab format](/scheduler/docs/configuring/cron-job-schedules) or in an [English-like format](/appengine/docs/standard/python/config/cronref#schedule_format). Regardless of what you specify, the job will run at 10:00 AM UTC. Only the interval from this schedule is used, not the specific time of day.
     */
    schedule: pulumi.Input<string>;
}
