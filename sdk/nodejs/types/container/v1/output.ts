// *** WARNING: this file was generated by the Pulumi SDK Generator. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../../../types/input";
import * as outputs from "../../../types/output";
import * as enums from "../../../types/enums";
import * as utilities from "../../../utilities";

/**
 * AcceleratorConfig represents a Hardware Accelerator request.
 */
export interface AcceleratorConfigResponse {
    /**
     * The number of the accelerator cards exposed to an instance.
     */
    acceleratorCount: string;
    /**
     * The accelerator type resource name. List of supported accelerators [here](https://cloud.google.com/compute/docs/gpus)
     */
    acceleratorType: string;
    /**
     * Size of partitions to create on the GPU. Valid values are described in the NVIDIA [mig user guide](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#partitioning).
     */
    gpuPartitionSize: string;
    /**
     * The configuration for GPU sharing options.
     */
    gpuSharingConfig: outputs.container.v1.GPUSharingConfigResponse;
}

/**
 * Configuration for the addons that can be automatically spun up in the cluster, enabling additional functionality.
 */
export interface AddonsConfigResponse {
    /**
     * Configuration for the Cloud Run addon, which allows the user to use a managed Knative service.
     */
    cloudRunConfig: outputs.container.v1.CloudRunConfigResponse;
    /**
     * Configuration for the ConfigConnector add-on, a Kubernetes extension to manage hosted GCP services through the Kubernetes API
     */
    configConnectorConfig: outputs.container.v1.ConfigConnectorConfigResponse;
    /**
     * Configuration for NodeLocalDNS, a dns cache running on cluster nodes
     */
    dnsCacheConfig: outputs.container.v1.DnsCacheConfigResponse;
    /**
     * Configuration for the Compute Engine Persistent Disk CSI driver.
     */
    gcePersistentDiskCsiDriverConfig: outputs.container.v1.GcePersistentDiskCsiDriverConfigResponse;
    /**
     * Configuration for the GCP Filestore CSI driver.
     */
    gcpFilestoreCsiDriverConfig: outputs.container.v1.GcpFilestoreCsiDriverConfigResponse;
    /**
     * Configuration for the Backup for GKE agent addon.
     */
    gkeBackupAgentConfig: outputs.container.v1.GkeBackupAgentConfigResponse;
    /**
     * Configuration for the horizontal pod autoscaling feature, which increases or decreases the number of replica pods a replication controller has based on the resource usage of the existing pods.
     */
    horizontalPodAutoscaling: outputs.container.v1.HorizontalPodAutoscalingResponse;
    /**
     * Configuration for the HTTP (L7) load balancing controller addon, which makes it easy to set up HTTP load balancers for services in a cluster.
     */
    httpLoadBalancing: outputs.container.v1.HttpLoadBalancingResponse;
    /**
     * Configuration for the Kubernetes Dashboard. This addon is deprecated, and will be disabled in 1.15. It is recommended to use the Cloud Console to manage and monitor your Kubernetes clusters, workloads and applications. For more information, see: https://cloud.google.com/kubernetes-engine/docs/concepts/dashboards
     */
    kubernetesDashboard: outputs.container.v1.KubernetesDashboardResponse;
    /**
     * Configuration for NetworkPolicy. This only tracks whether the addon is enabled or not on the Master, it does not track whether network policy is enabled for the nodes.
     */
    networkPolicyConfig: outputs.container.v1.NetworkPolicyConfigResponse;
}

/**
 * Specifies options for controlling advanced machine features.
 */
export interface AdvancedMachineFeaturesResponse {
    /**
     * The number of threads per physical core. To disable simultaneous multithreading (SMT) set this to 1. If unset, the maximum number of threads supported per core by the underlying processor is assumed.
     */
    threadsPerCore: string;
}

/**
 * Configuration for returning group information from authenticators.
 */
export interface AuthenticatorGroupsConfigResponse {
    /**
     * Whether this cluster should return group membership lookups during authentication using a group of security groups.
     */
    enabled: boolean;
    /**
     * The name of the security group-of-groups to be used. Only relevant if enabled = true.
     */
    securityGroup: string;
}

/**
 * AutoUpgradeOptions defines the set of options for the user to control how the Auto Upgrades will proceed.
 */
export interface AutoUpgradeOptionsResponse {
    /**
     * [Output only] This field is set when upgrades are about to commence with the approximate start time for the upgrades, in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) text format.
     */
    autoUpgradeStartTime: string;
    /**
     * [Output only] This field is set when upgrades are about to commence with the description of the upgrade.
     */
    description: string;
}

/**
 * Autopilot is the configuration for Autopilot settings on the cluster.
 */
export interface AutopilotResponse {
    /**
     * Enable Autopilot
     */
    enabled: boolean;
}

/**
 * AutoprovisioningNodePoolDefaults contains defaults for a node pool created by NAP.
 */
export interface AutoprovisioningNodePoolDefaultsResponse {
    /**
     * The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool. This should be of the form projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]. For more information about protecting resources with Cloud KMS Keys please see: https://cloud.google.com/compute/docs/disks/customer-managed-encryption
     */
    bootDiskKmsKey: string;
    /**
     * Size of the disk attached to each node, specified in GB. The smallest allowed disk size is 10GB. If unspecified, the default disk size is 100GB.
     */
    diskSizeGb: number;
    /**
     * Type of the disk attached to each node (e.g. 'pd-standard', 'pd-ssd' or 'pd-balanced') If unspecified, the default disk type is 'pd-standard'
     */
    diskType: string;
    /**
     * The image type to use for NAP created node.
     */
    imageType: string;
    /**
     * Specifies the node management options for NAP created node-pools.
     */
    management: outputs.container.v1.NodeManagementResponse;
    /**
     * Deprecated. Minimum CPU platform to be used for NAP created node pools. The instance may be scheduled on the specified or newer CPU platform. Applicable values are the friendly names of CPU platforms, such as minCpuPlatform: Intel Haswell or minCpuPlatform: Intel Sandy Bridge. For more information, read [how to specify min CPU platform](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform). This field is deprecated, min_cpu_platform should be specified using https://cloud.google.com/requested-min-cpu-platform label selector on the pod. To unset the min cpu platform field pass "automatic" as field value.
     *
     * @deprecated Deprecated. Minimum CPU platform to be used for NAP created node pools. The instance may be scheduled on the specified or newer CPU platform. Applicable values are the friendly names of CPU platforms, such as minCpuPlatform: Intel Haswell or minCpuPlatform: Intel Sandy Bridge. For more information, read [how to specify min CPU platform](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform). This field is deprecated, min_cpu_platform should be specified using https://cloud.google.com/requested-min-cpu-platform label selector on the pod. To unset the min cpu platform field pass "automatic" as field value.
     */
    minCpuPlatform: string;
    /**
     * Scopes that are used by NAP when creating node pools.
     */
    oauthScopes: string[];
    /**
     * The Google Cloud Platform Service Account to be used by the node VMs.
     */
    serviceAccount: string;
    /**
     * Shielded Instance options.
     */
    shieldedInstanceConfig: outputs.container.v1.ShieldedInstanceConfigResponse;
    /**
     * Specifies the upgrade settings for NAP created node pools
     */
    upgradeSettings: outputs.container.v1.UpgradeSettingsResponse;
}

/**
 * Parameters for using BigQuery as the destination of resource usage export.
 */
export interface BigQueryDestinationResponse {
    /**
     * The ID of a BigQuery Dataset.
     */
    datasetId: string;
}

/**
 * Configuration for Binary Authorization.
 */
export interface BinaryAuthorizationResponse {
    /**
     * This field is deprecated. Leave this unset and instead configure BinaryAuthorization using evaluation_mode. If evaluation_mode is set to anything other than EVALUATION_MODE_UNSPECIFIED, this field is ignored.
     *
     * @deprecated This field is deprecated. Leave this unset and instead configure BinaryAuthorization using evaluation_mode. If evaluation_mode is set to anything other than EVALUATION_MODE_UNSPECIFIED, this field is ignored.
     */
    enabled: boolean;
    /**
     * Mode of operation for binauthz policy evaluation. Currently the only options are equivalent to enable/disable. If unspecified, defaults to DISABLED.
     */
    evaluationMode: string;
}

/**
 * Information relevant to blue-green upgrade.
 */
export interface BlueGreenInfoResponse {
    /**
     * The resource URLs of the [managed instance groups] (/compute/docs/instance-groups/creating-groups-of-managed-instances) associated with blue pool.
     */
    blueInstanceGroupUrls: string[];
    /**
     * Time to start deleting blue pool to complete blue-green upgrade, in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) text format.
     */
    bluePoolDeletionStartTime: string;
    /**
     * The resource URLs of the [managed instance groups] (/compute/docs/instance-groups/creating-groups-of-managed-instances) associated with green pool.
     */
    greenInstanceGroupUrls: string[];
    /**
     * Version of green pool.
     */
    greenPoolVersion: string;
    /**
     * Current blue-green upgrade phase.
     */
    phase: string;
}

/**
 * Settings for blue-green upgrade.
 */
export interface BlueGreenSettingsResponse {
    /**
     * Time needed after draining entire blue pool. After this period, blue pool will be cleaned up.
     */
    nodePoolSoakDuration: string;
    /**
     * Standard policy for the blue-green upgrade.
     */
    standardRolloutPolicy: outputs.container.v1.StandardRolloutPolicyResponse;
}

/**
 * CidrBlock contains an optional name and one CIDR block.
 */
export interface CidrBlockResponse {
    /**
     * cidr_block must be specified in CIDR notation.
     */
    cidrBlock: string;
    /**
     * display_name is an optional field for users to identify CIDR blocks.
     */
    displayName: string;
}

/**
 * Configuration for client certificates on the cluster.
 */
export interface ClientCertificateConfigResponse {
    /**
     * Issue a client certificate.
     */
    issueClientCertificate: boolean;
}

/**
 * Configuration options for the Cloud Run feature.
 */
export interface CloudRunConfigResponse {
    /**
     * Whether Cloud Run addon is enabled for this cluster.
     */
    disabled: boolean;
    /**
     * Which load balancer type is installed for Cloud Run.
     */
    loadBalancerType: string;
}

/**
 * ClusterAutoscaling contains global, per-cluster information required by Cluster Autoscaler to automatically adjust the size of the cluster and create/delete node pools based on the current needs.
 */
export interface ClusterAutoscalingResponse {
    /**
     * The list of Google Compute Engine [zones](https://cloud.google.com/compute/docs/zones#available) in which the NodePool's nodes can be created by NAP.
     */
    autoprovisioningLocations: string[];
    /**
     * AutoprovisioningNodePoolDefaults contains defaults for a node pool created by NAP.
     */
    autoprovisioningNodePoolDefaults: outputs.container.v1.AutoprovisioningNodePoolDefaultsResponse;
    /**
     * Defines autoscaling behaviour.
     */
    autoscalingProfile: string;
    /**
     * Enables automatic node pool creation and deletion.
     */
    enableNodeAutoprovisioning: boolean;
    /**
     * Contains global constraints regarding minimum and maximum amount of resources in the cluster.
     */
    resourceLimits: outputs.container.v1.ResourceLimitResponse[];
}

/**
 * ConfidentialNodes is configuration for the confidential nodes feature, which makes nodes run on confidential VMs.
 */
export interface ConfidentialNodesResponse {
    /**
     * Whether Confidential Nodes feature is enabled.
     */
    enabled: boolean;
}

/**
 * Configuration options for the Config Connector add-on.
 */
export interface ConfigConnectorConfigResponse {
    /**
     * Whether Cloud Connector is enabled for this cluster.
     */
    enabled: boolean;
}

/**
 * Parameters for controlling consumption metering.
 */
export interface ConsumptionMeteringConfigResponse {
    /**
     * Whether to enable consumption metering for this cluster. If enabled, a second BigQuery table will be created to hold resource consumption records.
     */
    enabled: boolean;
}

/**
 * Configuration for fine-grained cost management feature.
 */
export interface CostManagementConfigResponse {
    /**
     * Whether the feature is enabled or not.
     */
    enabled: boolean;
}

/**
 * DNSConfig contains the desired set of options for configuring clusterDNS.
 */
export interface DNSConfigResponse {
    /**
     * cluster_dns indicates which in-cluster DNS provider should be used.
     */
    clusterDns: string;
    /**
     * cluster_dns_domain is the suffix used for all cluster service records.
     */
    clusterDnsDomain: string;
    /**
     * cluster_dns_scope indicates the scope of access to cluster DNS records.
     */
    clusterDnsScope: string;
}

/**
 * Time window specified for daily maintenance operations.
 */
export interface DailyMaintenanceWindowResponse {
    /**
     * [Output only] Duration of the time window, automatically chosen to be smallest possible in the given scenario. Duration will be in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) format "PTnHnMnS".
     */
    duration: string;
    /**
     * Time within the maintenance window to start the maintenance operations. Time format should be in [RFC3339](https://www.ietf.org/rfc/rfc3339.txt) format "HH:MM", where HH : [00-23] and MM : [00-59] GMT.
     */
    startTime: string;
}

/**
 * Configuration of etcd encryption.
 */
export interface DatabaseEncryptionResponse {
    /**
     * Name of CloudKMS key to use for the encryption of secrets in etcd. Ex. projects/my-project/locations/global/keyRings/my-ring/cryptoKeys/my-key
     */
    keyName: string;
    /**
     * Denotes the state of etcd encryption.
     */
    state: string;
}

/**
 * DefaultSnatStatus contains the desired state of whether default sNAT should be disabled on the cluster.
 */
export interface DefaultSnatStatusResponse {
    /**
     * Disables cluster default sNAT rules.
     */
    disabled: boolean;
}

/**
 * Configuration for NodeLocal DNSCache
 */
export interface DnsCacheConfigResponse {
    /**
     * Whether NodeLocal DNSCache is enabled for this cluster.
     */
    enabled: boolean;
}

/**
 * Allows filtering to one or more specific event types. If event types are present, those and only those event types will be transmitted to the cluster. Other types will be skipped. If no filter is specified, or no event types are present, all event types will be sent
 */
export interface FilterResponse {
    /**
     * Event types to allowlist.
     */
    eventType: string[];
}

/**
 * GPUSharingConfig represents the GPU sharing configuration for Hardware Accelerators.
 */
export interface GPUSharingConfigResponse {
    /**
     * The type of GPU sharing strategy to enable on the GPU node.
     */
    gpuSharingStrategy: string;
    /**
     * The max number of containers that can share a physical GPU.
     */
    maxSharedClientsPerGpu: string;
}

/**
 * GatewayAPIConfig contains the desired config of Gateway API on this cluster.
 */
export interface GatewayAPIConfigResponse {
    /**
     * The Gateway API release channel to use for Gateway API.
     */
    channel: string;
}

/**
 * Configuration for the Compute Engine PD CSI driver.
 */
export interface GcePersistentDiskCsiDriverConfigResponse {
    /**
     * Whether the Compute Engine PD CSI driver is enabled for this cluster.
     */
    enabled: boolean;
}

/**
 * GcfsConfig contains configurations of Google Container File System (image streaming).
 */
export interface GcfsConfigResponse {
    /**
     * Whether to use GCFS.
     */
    enabled: boolean;
}

/**
 * Configuration for the GCP Filestore CSI driver.
 */
export interface GcpFilestoreCsiDriverConfigResponse {
    /**
     * Whether the GCP Filestore CSI driver is enabled for this cluster.
     */
    enabled: boolean;
}

/**
 * Configuration for the Backup for GKE Agent.
 */
export interface GkeBackupAgentConfigResponse {
    /**
     * Whether the Backup for GKE agent is enabled for this cluster.
     */
    enabled: boolean;
}

/**
 * Configuration options for the horizontal pod autoscaling feature, which increases or decreases the number of replica pods a replication controller has based on the resource usage of the existing pods.
 */
export interface HorizontalPodAutoscalingResponse {
    /**
     * Whether the Horizontal Pod Autoscaling feature is enabled in the cluster. When enabled, it ensures that metrics are collected into Stackdriver Monitoring.
     */
    disabled: boolean;
}

/**
 * Configuration options for the HTTP (L7) load balancing controller addon, which makes it easy to set up HTTP load balancers for services in a cluster.
 */
export interface HttpLoadBalancingResponse {
    /**
     * Whether the HTTP Load Balancing controller is enabled in the cluster. When enabled, it runs a small pod in the cluster that manages the load balancers.
     */
    disabled: boolean;
}

/**
 * Configuration for controlling how IPs are allocated in the cluster.
 */
export interface IPAllocationPolicyResponse {
    /**
     * This field is deprecated, use cluster_ipv4_cidr_block.
     *
     * @deprecated This field is deprecated, use cluster_ipv4_cidr_block.
     */
    clusterIpv4Cidr: string;
    /**
     * The IP address range for the cluster pod IPs. If this field is set, then `cluster.cluster_ipv4_cidr` must be left blank. This field is only applicable when `use_ip_aliases` is true. Set to blank to have a range chosen with the default size. Set to /netmask (e.g. `/14`) to have a range chosen with a specific netmask. Set to a [CIDR](http://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) notation (e.g. `10.96.0.0/14`) from the RFC-1918 private networks (e.g. `10.0.0.0/8`, `172.16.0.0/12`, `192.168.0.0/16`) to pick a specific range to use.
     */
    clusterIpv4CidrBlock: string;
    /**
     * The name of the secondary range to be used for the cluster CIDR block. The secondary range will be used for pod IP addresses. This must be an existing secondary range associated with the cluster subnetwork. This field is only applicable with use_ip_aliases is true and create_subnetwork is false.
     */
    clusterSecondaryRangeName: string;
    /**
     * Whether a new subnetwork will be created automatically for the cluster. This field is only applicable when `use_ip_aliases` is true.
     */
    createSubnetwork: boolean;
    /**
     * The ipv6 access type (internal or external) when create_subnetwork is true
     */
    ipv6AccessType: string;
    /**
     * This field is deprecated, use node_ipv4_cidr_block.
     *
     * @deprecated This field is deprecated, use node_ipv4_cidr_block.
     */
    nodeIpv4Cidr: string;
    /**
     * The IP address range of the instance IPs in this cluster. This is applicable only if `create_subnetwork` is true. Set to blank to have a range chosen with the default size. Set to /netmask (e.g. `/14`) to have a range chosen with a specific netmask. Set to a [CIDR](http://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) notation (e.g. `10.96.0.0/14`) from the RFC-1918 private networks (e.g. `10.0.0.0/8`, `172.16.0.0/12`, `192.168.0.0/16`) to pick a specific range to use.
     */
    nodeIpv4CidrBlock: string;
    /**
     * This field is deprecated, use services_ipv4_cidr_block.
     *
     * @deprecated This field is deprecated, use services_ipv4_cidr_block.
     */
    servicesIpv4Cidr: string;
    /**
     * The IP address range of the services IPs in this cluster. If blank, a range will be automatically chosen with the default size. This field is only applicable when `use_ip_aliases` is true. Set to blank to have a range chosen with the default size. Set to /netmask (e.g. `/14`) to have a range chosen with a specific netmask. Set to a [CIDR](http://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) notation (e.g. `10.96.0.0/14`) from the RFC-1918 private networks (e.g. `10.0.0.0/8`, `172.16.0.0/12`, `192.168.0.0/16`) to pick a specific range to use.
     */
    servicesIpv4CidrBlock: string;
    /**
     * The name of the secondary range to be used as for the services CIDR block. The secondary range will be used for service ClusterIPs. This must be an existing secondary range associated with the cluster subnetwork. This field is only applicable with use_ip_aliases is true and create_subnetwork is false.
     */
    servicesSecondaryRangeName: string;
    /**
     * The IP stack type of the cluster
     */
    stackType: string;
    /**
     * A custom subnetwork name to be used if `create_subnetwork` is true. If this field is empty, then an automatic name will be chosen for the new subnetwork.
     */
    subnetworkName: string;
    /**
     * The IP address range of the Cloud TPUs in this cluster. If unspecified, a range will be automatically chosen with the default size. This field is only applicable when `use_ip_aliases` is true. If unspecified, the range will use the default size. Set to /netmask (e.g. `/14`) to have a range chosen with a specific netmask. Set to a [CIDR](http://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) notation (e.g. `10.96.0.0/14`) from the RFC-1918 private networks (e.g. `10.0.0.0/8`, `172.16.0.0/12`, `192.168.0.0/16`) to pick a specific range to use.
     */
    tpuIpv4CidrBlock: string;
    /**
     * Whether alias IPs will be used for pod IPs in the cluster. This is used in conjunction with use_routes. It cannot be true if use_routes is true. If both use_ip_aliases and use_routes are false, then the server picks the default IP allocation mode
     */
    useIpAliases: boolean;
    /**
     * Whether routes will be used for pod IPs in the cluster. This is used in conjunction with use_ip_aliases. It cannot be true if use_ip_aliases is true. If both use_ip_aliases and use_routes are false, then the server picks the default IP allocation mode
     */
    useRoutes: boolean;
}

/**
 * IdentityServiceConfig is configuration for Identity Service which allows customers to use external identity providers with the K8S API
 */
export interface IdentityServiceConfigResponse {
    /**
     * Whether to enable the Identity Service component
     */
    enabled: boolean;
}

/**
 * Configuration for the Kubernetes Dashboard.
 */
export interface KubernetesDashboardResponse {
    /**
     * Whether the Kubernetes Dashboard is enabled for this cluster.
     */
    disabled: boolean;
}

/**
 * Configuration for the legacy Attribute Based Access Control authorization mode.
 */
export interface LegacyAbacResponse {
    /**
     * Whether the ABAC authorizer is enabled for this cluster. When enabled, identities in the system, including service accounts, nodes, and controllers, will have statically granted permissions beyond those provided by the RBAC configuration or IAM.
     */
    enabled: boolean;
}

/**
 * Parameters that can be configured on Linux nodes.
 */
export interface LinuxNodeConfigResponse {
    /**
     * cgroup_mode specifies the cgroup mode to be used on the node.
     */
    cgroupMode: string;
    /**
     * The Linux kernel parameters to be applied to the nodes and all pods running on the nodes. The following parameters are supported. net.core.busy_poll net.core.busy_read net.core.netdev_max_backlog net.core.rmem_max net.core.wmem_default net.core.wmem_max net.core.optmem_max net.core.somaxconn net.ipv4.tcp_rmem net.ipv4.tcp_wmem net.ipv4.tcp_tw_reuse
     */
    sysctls: {[key: string]: string};
}

/**
 * LoggingComponentConfig is cluster logging component configuration.
 */
export interface LoggingComponentConfigResponse {
    /**
     * Select components to collect logs. An empty set would disable all logging.
     */
    enableComponents: string[];
}

/**
 * LoggingConfig is cluster logging configuration.
 */
export interface LoggingConfigResponse {
    /**
     * Logging components configuration
     */
    componentConfig: outputs.container.v1.LoggingComponentConfigResponse;
}

/**
 * LoggingVariantConfig specifies the behaviour of the logging component.
 */
export interface LoggingVariantConfigResponse {
    /**
     * Logging variant deployed on nodes.
     */
    variant: string;
}

/**
 * Represents the Maintenance exclusion option.
 */
export interface MaintenanceExclusionOptionsResponse {
    /**
     * Scope specifies the upgrade scope which upgrades are blocked by the exclusion.
     */
    scope: string;
}

/**
 * MaintenancePolicy defines the maintenance policy to be used for the cluster.
 */
export interface MaintenancePolicyResponse {
    /**
     * A hash identifying the version of this policy, so that updates to fields of the policy won't accidentally undo intermediate changes (and so that users of the API unaware of some fields won't accidentally remove other fields). Make a `get()` request to the cluster to get the current resource version and include it with requests to set the policy.
     */
    resourceVersion: string;
    /**
     * Specifies the maintenance window in which maintenance may be performed.
     */
    window: outputs.container.v1.MaintenanceWindowResponse;
}

/**
 * MaintenanceWindow defines the maintenance window to be used for the cluster.
 */
export interface MaintenanceWindowResponse {
    /**
     * DailyMaintenanceWindow specifies a daily maintenance operation window.
     */
    dailyMaintenanceWindow: outputs.container.v1.DailyMaintenanceWindowResponse;
    /**
     * Exceptions to maintenance window. Non-emergency maintenance should not occur in these windows.
     */
    maintenanceExclusions: {[key: string]: string};
    /**
     * RecurringWindow specifies some number of recurring time periods for maintenance to occur. The time windows may be overlapping. If no maintenance windows are set, maintenance can occur at any time.
     */
    recurringWindow: outputs.container.v1.RecurringTimeWindowResponse;
}

/**
 * ManagedPrometheusConfig defines the configuration for Google Cloud Managed Service for Prometheus.
 */
export interface ManagedPrometheusConfigResponse {
    /**
     * Enable Managed Collection.
     */
    enabled: boolean;
}

/**
 * The authentication information for accessing the master endpoint. Authentication can be done using HTTP basic auth or using client certificates.
 */
export interface MasterAuthResponse {
    /**
     * [Output only] Base64-encoded public certificate used by clients to authenticate to the cluster endpoint.
     */
    clientCertificate: string;
    /**
     * Configuration for client certificate authentication on the cluster. For clusters before v1.12, if no configuration is specified, a client certificate is issued.
     */
    clientCertificateConfig: outputs.container.v1.ClientCertificateConfigResponse;
    /**
     * [Output only] Base64-encoded private key used by clients to authenticate to the cluster endpoint.
     */
    clientKey: string;
    /**
     * [Output only] Base64-encoded public certificate that is the root of trust for the cluster.
     */
    clusterCaCertificate: string;
    /**
     * The password to use for HTTP basic authentication to the master endpoint. Because the master endpoint is open to the Internet, you should create a strong password. If a password is provided for cluster creation, username must be non-empty. Warning: basic authentication is deprecated, and will be removed in GKE control plane versions 1.19 and newer. For a list of recommended authentication methods, see: https://cloud.google.com/kubernetes-engine/docs/how-to/api-server-authentication
     */
    password: string;
    /**
     * The username to use for HTTP basic authentication to the master endpoint. For clusters v1.6.0 and later, basic authentication can be disabled by leaving username unspecified (or setting it to the empty string). Warning: basic authentication is deprecated, and will be removed in GKE control plane versions 1.19 and newer. For a list of recommended authentication methods, see: https://cloud.google.com/kubernetes-engine/docs/how-to/api-server-authentication
     */
    username: string;
}

/**
 * Configuration options for the master authorized networks feature. Enabled master authorized networks will disallow all external traffic to access Kubernetes master through HTTPS except traffic from the given CIDR blocks, Google Compute Engine Public IPs and Google Prod IPs.
 */
export interface MasterAuthorizedNetworksConfigResponse {
    /**
     * cidr_blocks define up to 50 external networks that could access Kubernetes master through HTTPS.
     */
    cidrBlocks: outputs.container.v1.CidrBlockResponse[];
    /**
     * Whether or not master authorized networks is enabled.
     */
    enabled: boolean;
    /**
     * Whether master is accessbile via Google Compute Engine Public IP addresses.
     */
    gcpPublicCidrsAccessEnabled: boolean;
}

/**
 * Constraints applied to pods.
 */
export interface MaxPodsConstraintResponse {
    /**
     * Constraint enforced on the max num of pods per node.
     */
    maxPodsPerNode: string;
}

/**
 * Configuration for issuance of mTLS keys and certificates to Kubernetes pods.
 */
export interface MeshCertificatesResponse {
    /**
     * enable_certificates controls issuance of workload mTLS certificates. If set, the GKE Workload Identity Certificates controller and node agent will be deployed in the cluster, which can then be configured by creating a WorkloadCertificateConfig Custom Resource. Requires Workload Identity (workload_pool must be non-empty).
     */
    enableCertificates: boolean;
}

/**
 * MonitoringComponentConfig is cluster monitoring component configuration.
 */
export interface MonitoringComponentConfigResponse {
    /**
     * Select components to collect metrics. An empty set would disable all monitoring.
     */
    enableComponents: string[];
}

/**
 * MonitoringConfig is cluster monitoring configuration.
 */
export interface MonitoringConfigResponse {
    /**
     * Monitoring components configuration
     */
    componentConfig: outputs.container.v1.MonitoringComponentConfigResponse;
    /**
     * Enable Google Cloud Managed Service for Prometheus in the cluster.
     */
    managedPrometheusConfig: outputs.container.v1.ManagedPrometheusConfigResponse;
}

/**
 * NetworkConfig reports the relative names of network & subnetwork.
 */
export interface NetworkConfigResponse {
    /**
     * The desired datapath provider for this cluster. By default, uses the IPTables-based kube-proxy implementation.
     */
    datapathProvider: string;
    /**
     * Whether the cluster disables default in-node sNAT rules. In-node sNAT rules will be disabled when default_snat_status is disabled. When disabled is set to false, default IP masquerade rules will be applied to the nodes to prevent sNAT on cluster internal traffic.
     */
    defaultSnatStatus: outputs.container.v1.DefaultSnatStatusResponse;
    /**
     * DNSConfig contains clusterDNS config for this cluster.
     */
    dnsConfig: outputs.container.v1.DNSConfigResponse;
    /**
     * Whether Intra-node visibility is enabled for this cluster. This makes same node pod to pod traffic visible for VPC network.
     */
    enableIntraNodeVisibility: boolean;
    /**
     * Whether L4ILB Subsetting is enabled for this cluster.
     */
    enableL4ilbSubsetting: boolean;
    /**
     * GatewayAPIConfig contains the desired config of Gateway API on this cluster.
     */
    gatewayApiConfig: outputs.container.v1.GatewayAPIConfigResponse;
    /**
     * The relative name of the Google Compute Engine network(https://cloud.google.com/compute/docs/networks-and-firewalls#networks) to which the cluster is connected. Example: projects/my-project/global/networks/my-network
     */
    network: string;
    /**
     * The desired state of IPv6 connectivity to Google Services. By default, no private IPv6 access to or from Google Services (all access will be via IPv4)
     */
    privateIpv6GoogleAccess: string;
    /**
     * ServiceExternalIPsConfig specifies if services with externalIPs field are blocked or not.
     */
    serviceExternalIpsConfig: outputs.container.v1.ServiceExternalIPsConfigResponse;
    /**
     * The relative name of the Google Compute Engine [subnetwork](https://cloud.google.com/compute/docs/vpc) to which the cluster is connected. Example: projects/my-project/regions/us-central1/subnetworks/my-subnet
     */
    subnetwork: string;
}

/**
 * Configuration of all network bandwidth tiers
 */
export interface NetworkPerformanceConfigResponse {
    /**
     * Specifies the total network bandwidth tier for the NodePool.
     */
    totalEgressBandwidthTier: string;
}

/**
 * Configuration for NetworkPolicy. This only tracks whether the addon is enabled or not on the Master, it does not track whether network policy is enabled for the nodes.
 */
export interface NetworkPolicyConfigResponse {
    /**
     * Whether NetworkPolicy is enabled for this cluster.
     */
    disabled: boolean;
}

/**
 * Configuration options for the NetworkPolicy feature. https://kubernetes.io/docs/concepts/services-networking/networkpolicies/
 */
export interface NetworkPolicyResponse {
    /**
     * Whether network policy is enabled on the cluster.
     */
    enabled: boolean;
    /**
     * The selected network policy provider.
     */
    provider: string;
}

/**
 * Collection of Compute Engine network tags that can be applied to a node's underlying VM instance.
 */
export interface NetworkTagsResponse {
    /**
     * List of network tags.
     */
    tags: string[];
}

/**
 * Subset of NodeConfig message that has defaults.
 */
export interface NodeConfigDefaultsResponse {
    /**
     * GCFS (Google Container File System, also known as Riptide) options.
     */
    gcfsConfig: outputs.container.v1.GcfsConfigResponse;
    /**
     * Logging configuration for node pools.
     */
    loggingConfig: outputs.container.v1.NodePoolLoggingConfigResponse;
}

/**
 * Parameters that describe the nodes in a cluster. GKE Autopilot clusters do not recognize parameters in `NodeConfig`. Use AutoprovisioningNodePoolDefaults instead.
 */
export interface NodeConfigResponse {
    /**
     * A list of hardware accelerators to be attached to each node. See https://cloud.google.com/compute/docs/gpus for more information about support for GPUs.
     */
    accelerators: outputs.container.v1.AcceleratorConfigResponse[];
    /**
     * Advanced features for the Compute Engine VM.
     */
    advancedMachineFeatures: outputs.container.v1.AdvancedMachineFeaturesResponse;
    /**
     *  The Customer Managed Encryption Key used to encrypt the boot disk attached to each node in the node pool. This should be of the form projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]. For more information about protecting resources with Cloud KMS Keys please see: https://cloud.google.com/compute/docs/disks/customer-managed-encryption
     */
    bootDiskKmsKey: string;
    /**
     * Confidential nodes config. All the nodes in the node pool will be Confidential VM once enabled.
     */
    confidentialNodes: outputs.container.v1.ConfidentialNodesResponse;
    /**
     * Size of the disk attached to each node, specified in GB. The smallest allowed disk size is 10GB. If unspecified, the default disk size is 100GB.
     */
    diskSizeGb: number;
    /**
     * Type of the disk attached to each node (e.g. 'pd-standard', 'pd-ssd' or 'pd-balanced') If unspecified, the default disk type is 'pd-standard'
     */
    diskType: string;
    /**
     * Google Container File System (image streaming) configs.
     */
    gcfsConfig: outputs.container.v1.GcfsConfigResponse;
    /**
     * Enable or disable gvnic in the node pool.
     */
    gvnic: outputs.container.v1.VirtualNICResponse;
    /**
     * The image type to use for this node. Note that for a given image type, the latest version of it will be used.
     */
    imageType: string;
    /**
     * Node kubelet configs.
     */
    kubeletConfig: outputs.container.v1.NodeKubeletConfigResponse;
    /**
     * The map of Kubernetes labels (key/value pairs) to be applied to each node. These will added in addition to any default label(s) that Kubernetes may apply to the node. In case of conflict in label keys, the applied set may differ depending on the Kubernetes version -- it's best to assume the behavior is undefined and conflicts should be avoided. For more information, including usage and the valid values, see: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
     */
    labels: {[key: string]: string};
    /**
     * Parameters that can be configured on Linux nodes.
     */
    linuxNodeConfig: outputs.container.v1.LinuxNodeConfigResponse;
    /**
     * The number of local SSD disks to be attached to the node. The limit for this value is dependent upon the maximum number of disks available on a machine per zone. See: https://cloud.google.com/compute/docs/disks/local-ssd for more information.
     */
    localSsdCount: number;
    /**
     * Logging configuration.
     */
    loggingConfig: outputs.container.v1.NodePoolLoggingConfigResponse;
    /**
     * The name of a Google Compute Engine [machine type](https://cloud.google.com/compute/docs/machine-types) If unspecified, the default machine type is `e2-medium`.
     */
    machineType: string;
    /**
     * The metadata key/value pairs assigned to instances in the cluster. Keys must conform to the regexp `[a-zA-Z0-9-_]+` and be less than 128 bytes in length. These are reflected as part of a URL in the metadata server. Additionally, to avoid ambiguity, keys must not conflict with any other metadata keys for the project or be one of the reserved keys: - "cluster-location" - "cluster-name" - "cluster-uid" - "configure-sh" - "containerd-configure-sh" - "enable-os-login" - "gci-ensure-gke-docker" - "gci-metrics-enabled" - "gci-update-strategy" - "instance-template" - "kube-env" - "startup-script" - "user-data" - "disable-address-manager" - "windows-startup-script-ps1" - "common-psm1" - "k8s-node-setup-psm1" - "install-ssh-psm1" - "user-profile-psm1" Values are free-form strings, and only have meaning as interpreted by the image running in the instance. The only restriction placed on them is that each value's size must be less than or equal to 32 KB. The total size of all keys and values must be less than 512 KB.
     */
    metadata: {[key: string]: string};
    /**
     * Minimum CPU platform to be used by this instance. The instance may be scheduled on the specified or newer CPU platform. Applicable values are the friendly names of CPU platforms, such as `minCpuPlatform: "Intel Haswell"` or `minCpuPlatform: "Intel Sandy Bridge"`. For more information, read [how to specify min CPU platform](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform)
     */
    minCpuPlatform: string;
    /**
     * Setting this field will assign instances of this pool to run on the specified node group. This is useful for running workloads on [sole tenant nodes](https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes).
     */
    nodeGroup: string;
    /**
     * The set of Google API scopes to be made available on all of the node VMs under the "default" service account. The following scopes are recommended, but not required, and by default are not included: * `https://www.googleapis.com/auth/compute` is required for mounting persistent storage on your nodes. * `https://www.googleapis.com/auth/devstorage.read_only` is required for communicating with **gcr.io** (the [Google Container Registry](https://cloud.google.com/container-registry/)). If unspecified, no scopes are added, unless Cloud Logging or Cloud Monitoring are enabled, in which case their required scopes will be added.
     */
    oauthScopes: string[];
    /**
     * Whether the nodes are created as preemptible VM instances. See: https://cloud.google.com/compute/docs/instances/preemptible for more information about preemptible VM instances.
     */
    preemptible: boolean;
    /**
     * The optional reservation affinity. Setting this field will apply the specified [Zonal Compute Reservation](https://cloud.google.com/compute/docs/instances/reserving-zonal-resources) to this node pool.
     */
    reservationAffinity: outputs.container.v1.ReservationAffinityResponse;
    /**
     * The resource labels for the node pool to use to annotate any related Google Compute Engine resources.
     */
    resourceLabels: {[key: string]: string};
    /**
     * Sandbox configuration for this node.
     */
    sandboxConfig: outputs.container.v1.SandboxConfigResponse;
    /**
     * The Google Cloud Platform Service Account to be used by the node VMs. Specify the email address of the Service Account; otherwise, if no Service Account is specified, the "default" service account is used.
     */
    serviceAccount: string;
    /**
     * Shielded Instance options.
     */
    shieldedInstanceConfig: outputs.container.v1.ShieldedInstanceConfigResponse;
    /**
     * Spot flag for enabling Spot VM, which is a rebrand of the existing preemptible flag.
     */
    spot: boolean;
    /**
     * The list of instance tags applied to all nodes. Tags are used to identify valid sources or targets for network firewalls and are specified by the client during cluster or node pool creation. Each tag within the list must comply with RFC1035.
     */
    tags: string[];
    /**
     * List of kubernetes taints to be applied to each node. For more information, including usage and the valid values, see: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
     */
    taints: outputs.container.v1.NodeTaintResponse[];
    /**
     * The workload metadata configuration for this node.
     */
    workloadMetadataConfig: outputs.container.v1.WorkloadMetadataConfigResponse;
}

/**
 * Node kubelet configs.
 */
export interface NodeKubeletConfigResponse {
    /**
     * Enable CPU CFS quota enforcement for containers that specify CPU limits. This option is enabled by default which makes kubelet use CFS quota (https://www.kernel.org/doc/Documentation/scheduler/sched-bwc.txt) to enforce container CPU limits. Otherwise, CPU limits will not be enforced at all. Disable this option to mitigate CPU throttling problems while still having your pods to be in Guaranteed QoS class by specifying the CPU limits. The default value is 'true' if unspecified.
     */
    cpuCfsQuota: boolean;
    /**
     * Set the CPU CFS quota period value 'cpu.cfs_period_us'. The string must be a sequence of decimal numbers, each with optional fraction and a unit suffix, such as "300ms". Valid time units are "ns", "us" (or "Âµs"), "ms", "s", "m", "h". The value must be a positive duration.
     */
    cpuCfsQuotaPeriod: string;
    /**
     * Control the CPU management policy on the node. See https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/ The following values are allowed. * "none": the default, which represents the existing scheduling behavior. * "static": allows pods with certain resource characteristics to be granted increased CPU affinity and exclusivity on the node. The default value is 'none' if unspecified.
     */
    cpuManagerPolicy: string;
    /**
     * Set the Pod PID limits. See https://kubernetes.io/docs/concepts/policy/pid-limiting/#pod-pid-limits Controls the maximum number of processes allowed to run in a pod. The value must be greater than or equal to 1024 and less than 4194304.
     */
    podPidsLimit: string;
}

/**
 * NodeManagement defines the set of node management services turned on for the node pool.
 */
export interface NodeManagementResponse {
    /**
     * A flag that specifies whether the node auto-repair is enabled for the node pool. If enabled, the nodes in this node pool will be monitored and, if they fail health checks too many times, an automatic repair action will be triggered.
     */
    autoRepair: boolean;
    /**
     * A flag that specifies whether node auto-upgrade is enabled for the node pool. If enabled, node auto-upgrade helps keep the nodes in your node pool up to date with the latest release version of Kubernetes.
     */
    autoUpgrade: boolean;
    /**
     * Specifies the Auto Upgrade knobs for the node pool.
     */
    upgradeOptions: outputs.container.v1.AutoUpgradeOptionsResponse;
}

/**
 * Parameters for node pool-level network config.
 */
export interface NodeNetworkConfigResponse {
    /**
     * Input only. Whether to create a new range for pod IPs in this node pool. Defaults are provided for `pod_range` and `pod_ipv4_cidr_block` if they are not specified. If neither `create_pod_range` or `pod_range` are specified, the cluster-level default (`ip_allocation_policy.cluster_ipv4_cidr_block`) is used. Only applicable if `ip_allocation_policy.use_ip_aliases` is true. This field cannot be changed after the node pool has been created.
     */
    createPodRange: boolean;
    /**
     * Whether nodes have internal IP addresses only. If enable_private_nodes is not specified, then the value is derived from cluster.privateClusterConfig.enablePrivateNodes
     */
    enablePrivateNodes: boolean;
    /**
     * Network bandwidth tier configuration.
     */
    networkPerformanceConfig: outputs.container.v1.NetworkPerformanceConfigResponse;
    /**
     * The IP address range for pod IPs in this node pool. Only applicable if `create_pod_range` is true. Set to blank to have a range chosen with the default size. Set to /netmask (e.g. `/14`) to have a range chosen with a specific netmask. Set to a [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing) notation (e.g. `10.96.0.0/14`) to pick a specific range to use. Only applicable if `ip_allocation_policy.use_ip_aliases` is true. This field cannot be changed after the node pool has been created.
     */
    podIpv4CidrBlock: string;
    /**
     * The ID of the secondary range for pod IPs. If `create_pod_range` is true, this ID is used for the new range. If `create_pod_range` is false, uses an existing secondary range with this ID. Only applicable if `ip_allocation_policy.use_ip_aliases` is true. This field cannot be changed after the node pool has been created.
     */
    podRange: string;
}

/**
 * Node pool configs that apply to all auto-provisioned node pools in autopilot clusters and node auto-provisioning enabled clusters.
 */
export interface NodePoolAutoConfigResponse {
    /**
     * The list of instance tags applied to all nodes. Tags are used to identify valid sources or targets for network firewalls and are specified by the client during cluster creation. Each tag within the list must comply with RFC1035.
     */
    networkTags: outputs.container.v1.NetworkTagsResponse;
}

/**
 * NodePoolAutoscaling contains information required by cluster autoscaler to adjust the size of the node pool to the current cluster usage.
 */
export interface NodePoolAutoscalingResponse {
    /**
     * Can this node pool be deleted automatically.
     */
    autoprovisioned: boolean;
    /**
     * Is autoscaling enabled for this node pool.
     */
    enabled: boolean;
    /**
     * Location policy used when scaling up a nodepool.
     */
    locationPolicy: string;
    /**
     * Maximum number of nodes for one location in the NodePool. Must be >= min_node_count. There has to be enough quota to scale up the cluster.
     */
    maxNodeCount: number;
    /**
     * Minimum number of nodes for one location in the NodePool. Must be >= 1 and <= max_node_count.
     */
    minNodeCount: number;
    /**
     * Maximum number of nodes in the node pool. Must be greater than total_min_node_count. There has to be enough quota to scale up the cluster. The total_*_node_count fields are mutually exclusive with the *_node_count fields.
     */
    totalMaxNodeCount: number;
    /**
     * Minimum number of nodes in the node pool. Must be greater than 1 less than total_max_node_count. The total_*_node_count fields are mutually exclusive with the *_node_count fields.
     */
    totalMinNodeCount: number;
}

/**
 * Subset of Nodepool message that has defaults.
 */
export interface NodePoolDefaultsResponse {
    /**
     * Subset of NodeConfig message that has defaults.
     */
    nodeConfigDefaults: outputs.container.v1.NodeConfigDefaultsResponse;
}

/**
 * NodePoolLoggingConfig specifies logging configuration for nodepools.
 */
export interface NodePoolLoggingConfigResponse {
    /**
     * Logging variant configuration.
     */
    variantConfig: outputs.container.v1.LoggingVariantConfigResponse;
}

/**
 * NodePool contains the name and configuration for a cluster's node pool. Node pools are a set of nodes (i.e. VM's), with a common configuration and specification, under the control of the cluster master. They may have a set of Kubernetes labels applied to them, which may be used to reference them during pod scheduling. They may also be resized up or down, to accommodate the workload.
 */
export interface NodePoolResponse {
    /**
     * Autoscaler configuration for this NodePool. Autoscaler is enabled only if a valid configuration is present.
     */
    autoscaling: outputs.container.v1.NodePoolAutoscalingResponse;
    /**
     * Which conditions caused the current node pool state.
     */
    conditions: outputs.container.v1.StatusConditionResponse[];
    /**
     * The node configuration of the pool.
     */
    config: outputs.container.v1.NodeConfigResponse;
    /**
     * The initial node count for the pool. You must ensure that your Compute Engine [resource quota](https://cloud.google.com/compute/quotas) is sufficient for this number of instances. You must also have available firewall and routes quota.
     */
    initialNodeCount: number;
    /**
     * [Output only] The resource URLs of the [managed instance groups](https://cloud.google.com/compute/docs/instance-groups/creating-groups-of-managed-instances) associated with this node pool. During the node pool blue-green upgrade operation, the URLs contain both blue and green resources.
     */
    instanceGroupUrls: string[];
    /**
     * The list of Google Compute Engine [zones](https://cloud.google.com/compute/docs/zones#available) in which the NodePool's nodes should be located. If this value is unspecified during node pool creation, the [Cluster.Locations](https://cloud.google.com/kubernetes-engine/docs/reference/rest/v1/projects.locations.clusters#Cluster.FIELDS.locations) value will be used, instead. Warning: changing node pool locations will result in nodes being added and/or removed.
     */
    locations: string[];
    /**
     * NodeManagement configuration for this NodePool.
     */
    management: outputs.container.v1.NodeManagementResponse;
    /**
     * The constraint on the maximum number of pods that can be run simultaneously on a node in the node pool.
     */
    maxPodsConstraint: outputs.container.v1.MaxPodsConstraintResponse;
    /**
     * The name of the node pool.
     */
    name: string;
    /**
     * Networking configuration for this NodePool. If specified, it overrides the cluster-level defaults.
     */
    networkConfig: outputs.container.v1.NodeNetworkConfigResponse;
    /**
     * [Output only] The pod CIDR block size per node in this node pool.
     */
    podIpv4CidrSize: number;
    /**
     * [Output only] Server-defined URL for the resource.
     */
    selfLink: string;
    /**
     * [Output only] The status of the nodes in this pool instance.
     */
    status: string;
    /**
     * [Output only] Deprecated. Use conditions instead. Additional information about the current status of this node pool instance, if available.
     *
     * @deprecated [Output only] Deprecated. Use conditions instead. Additional information about the current status of this node pool instance, if available.
     */
    statusMessage: string;
    /**
     * [Output only] Update info contains relevant information during a node pool update.
     */
    updateInfo: outputs.container.v1.UpdateInfoResponse;
    /**
     * Upgrade settings control disruption and speed of the upgrade.
     */
    upgradeSettings: outputs.container.v1.UpgradeSettingsResponse;
    /**
     * The version of the Kubernetes of this node.
     */
    version: string;
}

/**
 * Kubernetes taint is comprised of three fields: key, value, and effect. Effect can only be one of three types: NoSchedule, PreferNoSchedule or NoExecute. See [here](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration) for more information, including usage and the valid values.
 */
export interface NodeTaintResponse {
    /**
     * Effect for taint.
     */
    effect: string;
    /**
     * Key for taint.
     */
    key: string;
    /**
     * Value for taint.
     */
    value: string;
}

/**
 * NotificationConfig is the configuration of notifications.
 */
export interface NotificationConfigResponse {
    /**
     * Notification config for Pub/Sub.
     */
    pubsub: outputs.container.v1.PubSubResponse;
}

/**
 * Configuration options for private clusters.
 */
export interface PrivateClusterConfigResponse {
    /**
     * Whether the master's internal IP address is used as the cluster endpoint.
     */
    enablePrivateEndpoint: boolean;
    /**
     * Whether nodes have internal IP addresses only. If enabled, all nodes are given only RFC 1918 private addresses and communicate with the master via private networking.
     */
    enablePrivateNodes: boolean;
    /**
     * Controls master global access settings.
     */
    masterGlobalAccessConfig: outputs.container.v1.PrivateClusterMasterGlobalAccessConfigResponse;
    /**
     * The IP range in CIDR notation to use for the hosted master network. This range will be used for assigning internal IP addresses to the master or set of masters, as well as the ILB VIP. This range must not overlap with any other ranges in use within the cluster's network.
     */
    masterIpv4CidrBlock: string;
    /**
     * The peering name in the customer VPC used by this cluster.
     */
    peeringName: string;
    /**
     * The internal IP address of this cluster's master endpoint.
     */
    privateEndpoint: string;
    /**
     * Subnet to provision the master's private endpoint during cluster creation. Specified in projects/*&#47;regions/*&#47;subnetworks/* format.
     */
    privateEndpointSubnetwork: string;
    /**
     * The external IP address of this cluster's master endpoint.
     */
    publicEndpoint: string;
}

/**
 * Configuration for controlling master global access settings.
 */
export interface PrivateClusterMasterGlobalAccessConfigResponse {
    /**
     * Whenever master is accessible globally or not.
     */
    enabled: boolean;
}

/**
 * Pub/Sub specific notification config.
 */
export interface PubSubResponse {
    /**
     * Enable notifications for Pub/Sub.
     */
    enabled: boolean;
    /**
     * Allows filtering to one or more specific event types. If no filter is specified, or if a filter is specified with no event types, all event types will be sent
     */
    filter: outputs.container.v1.FilterResponse;
    /**
     * The desired Pub/Sub topic to which notifications will be sent by GKE. Format is `projects/{project}/topics/{topic}`.
     */
    topic: string;
}

/**
 * Represents an arbitrary window of time that recurs.
 */
export interface RecurringTimeWindowResponse {
    /**
     * An RRULE (https://tools.ietf.org/html/rfc5545#section-3.8.5.3) for how this window reccurs. They go on for the span of time between the start and end time. For example, to have something repeat every weekday, you'd use: `FREQ=WEEKLY;BYDAY=MO,TU,WE,TH,FR` To repeat some window daily (equivalent to the DailyMaintenanceWindow): `FREQ=DAILY` For the first weekend of every month: `FREQ=MONTHLY;BYSETPOS=1;BYDAY=SA,SU` This specifies how frequently the window starts. Eg, if you wanted to have a 9-5 UTC-4 window every weekday, you'd use something like: ``` start time = 2019-01-01T09:00:00-0400 end time = 2019-01-01T17:00:00-0400 recurrence = FREQ=WEEKLY;BYDAY=MO,TU,WE,TH,FR ``` Windows can span multiple days. Eg, to make the window encompass every weekend from midnight Saturday till the last minute of Sunday UTC: ``` start time = 2019-01-05T00:00:00Z end time = 2019-01-07T23:59:00Z recurrence = FREQ=WEEKLY;BYDAY=SA ``` Note the start and end time's specific dates are largely arbitrary except to specify duration of the window and when it first starts. The FREQ values of HOURLY, MINUTELY, and SECONDLY are not supported.
     */
    recurrence: string;
    /**
     * The window of the first recurrence.
     */
    window: outputs.container.v1.TimeWindowResponse;
}

/**
 * ReleaseChannelConfig exposes configuration for a release channel.
 */
export interface ReleaseChannelConfigResponse {
    /**
     * The release channel this configuration applies to.
     */
    channel: string;
    /**
     * The default version for newly created clusters on the channel.
     */
    defaultVersion: string;
    /**
     * List of valid versions for the channel.
     */
    validVersions: string[];
}

/**
 * ReleaseChannel indicates which release channel a cluster is subscribed to. Release channels are arranged in order of risk. When a cluster is subscribed to a release channel, Google maintains both the master version and the node version. Node auto-upgrade defaults to true and cannot be disabled.
 */
export interface ReleaseChannelResponse {
    /**
     * channel specifies which release channel the cluster is subscribed to.
     */
    channel: string;
}

/**
 * [ReservationAffinity](https://cloud.google.com/compute/docs/instances/reserving-zonal-resources) is the configuration of desired reservation which instances could take capacity from.
 */
export interface ReservationAffinityResponse {
    /**
     * Corresponds to the type of reservation consumption.
     */
    consumeReservationType: string;
    /**
     * Corresponds to the label key of a reservation resource. To target a SPECIFIC_RESERVATION by name, specify "compute.googleapis.com/reservation-name" as the key and specify the name of your reservation as its value.
     */
    key: string;
    /**
     * Corresponds to the label value(s) of reservation resource(s).
     */
    values: string[];
}

/**
 * Contains information about amount of some resource in the cluster. For memory, value should be in GB.
 */
export interface ResourceLimitResponse {
    /**
     * Maximum amount of the resource in the cluster.
     */
    maximum: string;
    /**
     * Minimum amount of the resource in the cluster.
     */
    minimum: string;
    /**
     * Resource name "cpu", "memory" or gpu-specific string.
     */
    resourceType: string;
}

/**
 * Configuration for exporting cluster resource usages.
 */
export interface ResourceUsageExportConfigResponse {
    /**
     * Configuration to use BigQuery as usage export destination.
     */
    bigqueryDestination: outputs.container.v1.BigQueryDestinationResponse;
    /**
     * Configuration to enable resource consumption metering.
     */
    consumptionMeteringConfig: outputs.container.v1.ConsumptionMeteringConfigResponse;
    /**
     * Whether to enable network egress metering for this cluster. If enabled, a daemonset will be created in the cluster to meter network egress traffic.
     */
    enableNetworkEgressMetering: boolean;
}

/**
 * SandboxConfig contains configurations of the sandbox to use for the node.
 */
export interface SandboxConfigResponse {
    /**
     * Type of the sandbox to use for the node.
     */
    type: string;
}

/**
 * Config to block services with externalIPs field.
 */
export interface ServiceExternalIPsConfigResponse {
    /**
     * Whether Services with ExternalIPs field are allowed or not.
     */
    enabled: boolean;
}

/**
 * A set of Shielded Instance options.
 */
export interface ShieldedInstanceConfigResponse {
    /**
     * Defines whether the instance has integrity monitoring enabled. Enables monitoring and attestation of the boot integrity of the instance. The attestation is performed against the integrity policy baseline. This baseline is initially derived from the implicitly trusted boot image when the instance is created.
     */
    enableIntegrityMonitoring: boolean;
    /**
     * Defines whether the instance has Secure Boot enabled. Secure Boot helps ensure that the system only runs authentic software by verifying the digital signature of all boot components, and halting the boot process if signature verification fails.
     */
    enableSecureBoot: boolean;
}

/**
 * Configuration of Shielded Nodes feature.
 */
export interface ShieldedNodesResponse {
    /**
     * Whether Shielded Nodes features are enabled on all nodes in this cluster.
     */
    enabled: boolean;
}

/**
 * Standard rollout policy is the default policy for blue-green.
 */
export interface StandardRolloutPolicyResponse {
    /**
     * Number of blue nodes to drain in a batch.
     */
    batchNodeCount: number;
    /**
     * Percentage of the blue pool nodes to drain in a batch. The range of this field should be (0.0, 1.0].
     */
    batchPercentage: number;
    /**
     * Soak time after each batch gets drained. Default to zero.
     */
    batchSoakDuration: string;
}

/**
 * StatusCondition describes why a cluster or a node pool has a certain status (e.g., ERROR or DEGRADED).
 */
export interface StatusConditionResponse {
    /**
     * Canonical code of the condition.
     */
    canonicalCode: string;
    /**
     * Machine-friendly representation of the condition Deprecated. Use canonical_code instead.
     *
     * @deprecated Machine-friendly representation of the condition Deprecated. Use canonical_code instead.
     */
    code: string;
    /**
     * Human-friendly representation of the condition
     */
    message: string;
}

/**
 * Represents an arbitrary window of time.
 */
export interface TimeWindowResponse {
    /**
     * The time that the window ends. The end time should take place after the start time.
     */
    endTime: string;
    /**
     * MaintenanceExclusionOptions provides maintenance exclusion related options.
     */
    maintenanceExclusionOptions: outputs.container.v1.MaintenanceExclusionOptionsResponse;
    /**
     * The time that the window first starts.
     */
    startTime: string;
}

/**
 * UpdateInfo contains resource (instance groups, etc), status and other intermediate information relevant to a node pool upgrade.
 */
export interface UpdateInfoResponse {
    /**
     * Information of a blue-green upgrade.
     */
    blueGreenInfo: outputs.container.v1.BlueGreenInfoResponse;
}

/**
 * These upgrade settings control the level of parallelism and the level of disruption caused by an upgrade. maxUnavailable controls the number of nodes that can be simultaneously unavailable. maxSurge controls the number of additional nodes that can be added to the node pool temporarily for the time of the upgrade to increase the number of available nodes. (maxUnavailable + maxSurge) determines the level of parallelism (how many nodes are being upgraded at the same time). Note: upgrades inevitably introduce some disruption since workloads need to be moved from old nodes to new, upgraded ones. Even if maxUnavailable=0, this holds true. (Disruption stays within the limits of PodDisruptionBudget, if it is configured.) Consider a hypothetical node pool with 5 nodes having maxSurge=2, maxUnavailable=1. This means the upgrade process upgrades 3 nodes simultaneously. It creates 2 additional (upgraded) nodes, then it brings down 3 old (not yet upgraded) nodes at the same time. This ensures that there are always at least 4 nodes available. These upgrade settings configure the upgrade strategy for the node pool. Use strategy to switch between the strategies applied to the node pool. If the strategy is ROLLING, use max_surge and max_unavailable to control the level of parallelism and the level of disruption caused by upgrade. 1. maxSurge controls the number of additional nodes that can be added to the node pool temporarily for the time of the upgrade to increase the number of available nodes. 2. maxUnavailable controls the number of nodes that can be simultaneously unavailable. 3. (maxUnavailable + maxSurge) determines the level of parallelism (how many nodes are being upgraded at the same time). If the strategy is BLUE_GREEN, use blue_green_settings to configure the blue-green upgrade related settings. 1. standard_rollout_policy is the default policy. The policy is used to control the way blue pool gets drained. The draining is executed in the batch mode. The batch size could be specified as either percentage of the node pool size or the number of nodes. batch_soak_duration is the soak time after each batch gets drained. 2. node_pool_soak_duration is the soak time after all blue nodes are drained. After this period, the blue pool nodes will be deleted.
 */
export interface UpgradeSettingsResponse {
    /**
     * Settings for blue-green upgrade strategy.
     */
    blueGreenSettings: outputs.container.v1.BlueGreenSettingsResponse;
    /**
     * The maximum number of nodes that can be created beyond the current size of the node pool during the upgrade process.
     */
    maxSurge: number;
    /**
     * The maximum number of nodes that can be simultaneously unavailable during the upgrade process. A node is considered available if its status is Ready.
     */
    maxUnavailable: number;
    /**
     * Update strategy of the node pool.
     */
    strategy: string;
}

/**
 * VerticalPodAutoscaling contains global, per-cluster information required by Vertical Pod Autoscaler to automatically adjust the resources of pods controlled by it.
 */
export interface VerticalPodAutoscalingResponse {
    /**
     * Enables vertical pod autoscaling.
     */
    enabled: boolean;
}

/**
 * Configuration of gVNIC feature.
 */
export interface VirtualNICResponse {
    /**
     * Whether gVNIC features are enabled in the node pool.
     */
    enabled: boolean;
}

/**
 * Configuration for the use of Kubernetes Service Accounts in GCP IAM policies.
 */
export interface WorkloadIdentityConfigResponse {
    /**
     * The workload pool to attach all Kubernetes service accounts to.
     */
    workloadPool: string;
}

/**
 * WorkloadMetadataConfig defines the metadata configuration to expose to workloads on the node pool.
 */
export interface WorkloadMetadataConfigResponse {
    /**
     * Mode is the configuration for how to expose metadata to workloads running on the node pool.
     */
    mode: string;
}

