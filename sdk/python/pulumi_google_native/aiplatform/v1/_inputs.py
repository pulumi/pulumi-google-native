# coding=utf-8
# *** WARNING: this file was generated by the Pulumi SDK Generator. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
from ... import _utilities
from ._enums import *

__all__ = [
    'GoogleCloudAiplatformV1ActiveLearningConfigArgs',
    'GoogleCloudAiplatformV1AutoscalingMetricSpecArgs',
    'GoogleCloudAiplatformV1BatchDedicatedResourcesArgs',
    'GoogleCloudAiplatformV1BatchPredictionJobInputConfigArgs',
    'GoogleCloudAiplatformV1BatchPredictionJobInstanceConfigArgs',
    'GoogleCloudAiplatformV1BatchPredictionJobOutputConfigArgs',
    'GoogleCloudAiplatformV1BigQueryDestinationArgs',
    'GoogleCloudAiplatformV1BigQuerySourceArgs',
    'GoogleCloudAiplatformV1BlurBaselineConfigArgs',
    'GoogleCloudAiplatformV1ContainerSpecArgs',
    'GoogleCloudAiplatformV1CreatePipelineJobRequestArgs',
    'GoogleCloudAiplatformV1CustomJobSpecArgs',
    'GoogleCloudAiplatformV1DedicatedResourcesArgs',
    'GoogleCloudAiplatformV1DiskSpecArgs',
    'GoogleCloudAiplatformV1EncryptionSpecArgs',
    'GoogleCloudAiplatformV1EnvVarArgs',
    'GoogleCloudAiplatformV1ExamplesExampleGcsSourceArgs',
    'GoogleCloudAiplatformV1ExamplesArgs',
    'GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainArgs',
    'GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationArgs',
    'GoogleCloudAiplatformV1ExplanationMetadataInputMetadataArgs',
    'GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataArgs',
    'GoogleCloudAiplatformV1ExplanationMetadataArgs',
    'GoogleCloudAiplatformV1ExplanationParametersArgs',
    'GoogleCloudAiplatformV1ExplanationSpecArgs',
    'GoogleCloudAiplatformV1FeatureGroupBigQueryArgs',
    'GoogleCloudAiplatformV1FeatureNoiseSigmaNoiseSigmaForFeatureArgs',
    'GoogleCloudAiplatformV1FeatureNoiseSigmaArgs',
    'GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingArgs',
    'GoogleCloudAiplatformV1FeatureOnlineStoreBigtableArgs',
    'GoogleCloudAiplatformV1FeatureViewBigQuerySourceArgs',
    'GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupArgs',
    'GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceArgs',
    'GoogleCloudAiplatformV1FeatureViewSyncConfigArgs',
    'GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisArgs',
    'GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisArgs',
    'GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigArgs',
    'GoogleCloudAiplatformV1FeaturestoreMonitoringConfigArgs',
    'GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingArgs',
    'GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigArgs',
    'GoogleCloudAiplatformV1FilterSplitArgs',
    'GoogleCloudAiplatformV1FractionSplitArgs',
    'GoogleCloudAiplatformV1GcsDestinationArgs',
    'GoogleCloudAiplatformV1GcsSourceArgs',
    'GoogleCloudAiplatformV1InputDataConfigArgs',
    'GoogleCloudAiplatformV1IntegratedGradientsAttributionArgs',
    'GoogleCloudAiplatformV1MachineSpecArgs',
    'GoogleCloudAiplatformV1ManualBatchTuningParametersArgs',
    'GoogleCloudAiplatformV1ModelContainerSpecArgs',
    'GoogleCloudAiplatformV1ModelDeploymentMonitoringObjectiveConfigArgs',
    'GoogleCloudAiplatformV1ModelDeploymentMonitoringScheduleConfigArgs',
    'GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigArgs',
    'GoogleCloudAiplatformV1ModelMonitoringAlertConfigArgs',
    'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineArgs',
    'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigArgs',
    'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigArgs',
    'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetArgs',
    'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigArgs',
    'GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigArgs',
    'GoogleCloudAiplatformV1ModelArgs',
    'GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecArgs',
    'GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecArgs',
    'GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecArgs',
    'GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecArgs',
    'GoogleCloudAiplatformV1NasJobSpecArgs',
    'GoogleCloudAiplatformV1NetworkSpecArgs',
    'GoogleCloudAiplatformV1NfsMountArgs',
    'GoogleCloudAiplatformV1NotebookEucConfigArgs',
    'GoogleCloudAiplatformV1NotebookIdleShutdownConfigArgs',
    'GoogleCloudAiplatformV1PersistentDiskSpecArgs',
    'GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactArgs',
    'GoogleCloudAiplatformV1PipelineJobRuntimeConfigArgs',
    'GoogleCloudAiplatformV1PipelineJobArgs',
    'GoogleCloudAiplatformV1PortArgs',
    'GoogleCloudAiplatformV1PredefinedSplitArgs',
    'GoogleCloudAiplatformV1PredictRequestResponseLoggingConfigArgs',
    'GoogleCloudAiplatformV1PredictSchemataArgs',
    'GoogleCloudAiplatformV1PresetsArgs',
    'GoogleCloudAiplatformV1PrivateServiceConnectConfigArgs',
    'GoogleCloudAiplatformV1ProbeExecActionArgs',
    'GoogleCloudAiplatformV1ProbeArgs',
    'GoogleCloudAiplatformV1PythonPackageSpecArgs',
    'GoogleCloudAiplatformV1SampleConfigArgs',
    'GoogleCloudAiplatformV1SampledShapleyAttributionArgs',
    'GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigArgs',
    'GoogleCloudAiplatformV1SamplingStrategyArgs',
    'GoogleCloudAiplatformV1SavedQueryArgs',
    'GoogleCloudAiplatformV1SchedulingArgs',
    'GoogleCloudAiplatformV1SmoothGradConfigArgs',
    'GoogleCloudAiplatformV1StratifiedSplitArgs',
    'GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecArgs',
    'GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecArgs',
    'GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecArgs',
    'GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigArgs',
    'GoogleCloudAiplatformV1StudySpecMetricSpecArgs',
    'GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecArgs',
    'GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecCategoricalValueConditionArgs',
    'GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecDiscreteValueConditionArgs',
    'GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecIntValueConditionArgs',
    'GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecArgs',
    'GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecArgs',
    'GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecArgs',
    'GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecArgs',
    'GoogleCloudAiplatformV1StudySpecParameterSpecArgs',
    'GoogleCloudAiplatformV1StudySpecStudyStoppingConfigArgs',
    'GoogleCloudAiplatformV1StudySpecArgs',
    'GoogleCloudAiplatformV1StudyTimeConstraintArgs',
    'GoogleCloudAiplatformV1ThresholdConfigArgs',
    'GoogleCloudAiplatformV1TimestampSplitArgs',
    'GoogleCloudAiplatformV1TrainingConfigArgs',
    'GoogleCloudAiplatformV1UnmanagedContainerModelArgs',
    'GoogleCloudAiplatformV1ValueArgs',
    'GoogleCloudAiplatformV1WorkerPoolSpecArgs',
    'GoogleCloudAiplatformV1XraiAttributionArgs',
    'GoogleIamV1BindingArgs',
    'GoogleTypeExprArgs',
]

@pulumi.input_type
class GoogleCloudAiplatformV1ActiveLearningConfigArgs:
    def __init__(__self__, *,
                 max_data_item_count: Optional[pulumi.Input[str]] = None,
                 max_data_item_percentage: Optional[pulumi.Input[int]] = None,
                 sample_config: Optional[pulumi.Input['GoogleCloudAiplatformV1SampleConfigArgs']] = None,
                 training_config: Optional[pulumi.Input['GoogleCloudAiplatformV1TrainingConfigArgs']] = None):
        """
        Parameters that configure the active learning pipeline. Active learning will label the data incrementally by several iterations. For every iteration, it will select a batch of data based on the sampling strategy.
        :param pulumi.Input[str] max_data_item_count: Max number of human labeled DataItems.
        :param pulumi.Input[int] max_data_item_percentage: Max percent of total DataItems for human labeling.
        :param pulumi.Input['GoogleCloudAiplatformV1SampleConfigArgs'] sample_config: Active learning data sampling config. For every active learning labeling iteration, it will select a batch of data based on the sampling strategy.
        :param pulumi.Input['GoogleCloudAiplatformV1TrainingConfigArgs'] training_config: CMLE training config. For every active learning labeling iteration, system will train a machine learning model on CMLE. The trained model will be used by data sampling algorithm to select DataItems.
        """
        if max_data_item_count is not None:
            pulumi.set(__self__, "max_data_item_count", max_data_item_count)
        if max_data_item_percentage is not None:
            pulumi.set(__self__, "max_data_item_percentage", max_data_item_percentage)
        if sample_config is not None:
            pulumi.set(__self__, "sample_config", sample_config)
        if training_config is not None:
            pulumi.set(__self__, "training_config", training_config)

    @property
    @pulumi.getter(name="maxDataItemCount")
    def max_data_item_count(self) -> Optional[pulumi.Input[str]]:
        """
        Max number of human labeled DataItems.
        """
        return pulumi.get(self, "max_data_item_count")

    @max_data_item_count.setter
    def max_data_item_count(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "max_data_item_count", value)

    @property
    @pulumi.getter(name="maxDataItemPercentage")
    def max_data_item_percentage(self) -> Optional[pulumi.Input[int]]:
        """
        Max percent of total DataItems for human labeling.
        """
        return pulumi.get(self, "max_data_item_percentage")

    @max_data_item_percentage.setter
    def max_data_item_percentage(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_data_item_percentage", value)

    @property
    @pulumi.getter(name="sampleConfig")
    def sample_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1SampleConfigArgs']]:
        """
        Active learning data sampling config. For every active learning labeling iteration, it will select a batch of data based on the sampling strategy.
        """
        return pulumi.get(self, "sample_config")

    @sample_config.setter
    def sample_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1SampleConfigArgs']]):
        pulumi.set(self, "sample_config", value)

    @property
    @pulumi.getter(name="trainingConfig")
    def training_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1TrainingConfigArgs']]:
        """
        CMLE training config. For every active learning labeling iteration, system will train a machine learning model on CMLE. The trained model will be used by data sampling algorithm to select DataItems.
        """
        return pulumi.get(self, "training_config")

    @training_config.setter
    def training_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1TrainingConfigArgs']]):
        pulumi.set(self, "training_config", value)


@pulumi.input_type
class GoogleCloudAiplatformV1AutoscalingMetricSpecArgs:
    def __init__(__self__, *,
                 metric_name: pulumi.Input[str],
                 target: Optional[pulumi.Input[int]] = None):
        """
        The metric specification that defines the target resource utilization (CPU utilization, accelerator's duty cycle, and so on) for calculating the desired replica count.
        :param pulumi.Input[str] metric_name: The resource metric name. Supported metrics: * For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        :param pulumi.Input[int] target: The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
        pulumi.set(__self__, "metric_name", metric_name)
        if target is not None:
            pulumi.set(__self__, "target", target)

    @property
    @pulumi.getter(name="metricName")
    def metric_name(self) -> pulumi.Input[str]:
        """
        The resource metric name. Supported metrics: * For Online Prediction: * `aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle` * `aiplatform.googleapis.com/prediction/online/cpu/utilization`
        """
        return pulumi.get(self, "metric_name")

    @metric_name.setter
    def metric_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "metric_name", value)

    @property
    @pulumi.getter
    def target(self) -> Optional[pulumi.Input[int]]:
        """
        The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
        """
        return pulumi.get(self, "target")

    @target.setter
    def target(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "target", value)


@pulumi.input_type
class GoogleCloudAiplatformV1BatchDedicatedResourcesArgs:
    def __init__(__self__, *,
                 machine_spec: pulumi.Input['GoogleCloudAiplatformV1MachineSpecArgs'],
                 max_replica_count: Optional[pulumi.Input[int]] = None,
                 starting_replica_count: Optional[pulumi.Input[int]] = None):
        """
        A description of resources that are used for performing batch operations, are dedicated to a Model, and need manual configuration.
        :param pulumi.Input['GoogleCloudAiplatformV1MachineSpecArgs'] machine_spec: Immutable. The specification of a single machine.
        :param pulumi.Input[int] max_replica_count: Immutable. The maximum number of machine replicas the batch operation may be scaled to. The default value is 10.
        :param pulumi.Input[int] starting_replica_count: Immutable. The number of machine replicas used at the start of the batch operation. If not set, Vertex AI decides starting number, not greater than max_replica_count
        """
        pulumi.set(__self__, "machine_spec", machine_spec)
        if max_replica_count is not None:
            pulumi.set(__self__, "max_replica_count", max_replica_count)
        if starting_replica_count is not None:
            pulumi.set(__self__, "starting_replica_count", starting_replica_count)

    @property
    @pulumi.getter(name="machineSpec")
    def machine_spec(self) -> pulumi.Input['GoogleCloudAiplatformV1MachineSpecArgs']:
        """
        Immutable. The specification of a single machine.
        """
        return pulumi.get(self, "machine_spec")

    @machine_spec.setter
    def machine_spec(self, value: pulumi.Input['GoogleCloudAiplatformV1MachineSpecArgs']):
        pulumi.set(self, "machine_spec", value)

    @property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> Optional[pulumi.Input[int]]:
        """
        Immutable. The maximum number of machine replicas the batch operation may be scaled to. The default value is 10.
        """
        return pulumi.get(self, "max_replica_count")

    @max_replica_count.setter
    def max_replica_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_replica_count", value)

    @property
    @pulumi.getter(name="startingReplicaCount")
    def starting_replica_count(self) -> Optional[pulumi.Input[int]]:
        """
        Immutable. The number of machine replicas used at the start of the batch operation. If not set, Vertex AI decides starting number, not greater than max_replica_count
        """
        return pulumi.get(self, "starting_replica_count")

    @starting_replica_count.setter
    def starting_replica_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "starting_replica_count", value)


@pulumi.input_type
class GoogleCloudAiplatformV1BatchPredictionJobInputConfigArgs:
    def __init__(__self__, *,
                 instances_format: pulumi.Input[str],
                 bigquery_source: Optional[pulumi.Input['GoogleCloudAiplatformV1BigQuerySourceArgs']] = None,
                 gcs_source: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsSourceArgs']] = None):
        """
        Configures the input to BatchPredictionJob. See Model.supported_input_storage_formats for Model's supported input formats, and how instances should be expressed via any of them.
        :param pulumi.Input[str] instances_format: The format in which instances are given, must be one of the Model's supported_input_storage_formats.
        :param pulumi.Input['GoogleCloudAiplatformV1BigQuerySourceArgs'] bigquery_source: The BigQuery location of the input table. The schema of the table should be in the format described by the given context OpenAPI Schema, if one is provided. The table may contain additional columns that are not described by the schema, and they will be ignored.
        :param pulumi.Input['GoogleCloudAiplatformV1GcsSourceArgs'] gcs_source: The Cloud Storage location for the input instances.
        """
        pulumi.set(__self__, "instances_format", instances_format)
        if bigquery_source is not None:
            pulumi.set(__self__, "bigquery_source", bigquery_source)
        if gcs_source is not None:
            pulumi.set(__self__, "gcs_source", gcs_source)

    @property
    @pulumi.getter(name="instancesFormat")
    def instances_format(self) -> pulumi.Input[str]:
        """
        The format in which instances are given, must be one of the Model's supported_input_storage_formats.
        """
        return pulumi.get(self, "instances_format")

    @instances_format.setter
    def instances_format(self, value: pulumi.Input[str]):
        pulumi.set(self, "instances_format", value)

    @property
    @pulumi.getter(name="bigquerySource")
    def bigquery_source(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1BigQuerySourceArgs']]:
        """
        The BigQuery location of the input table. The schema of the table should be in the format described by the given context OpenAPI Schema, if one is provided. The table may contain additional columns that are not described by the schema, and they will be ignored.
        """
        return pulumi.get(self, "bigquery_source")

    @bigquery_source.setter
    def bigquery_source(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1BigQuerySourceArgs']]):
        pulumi.set(self, "bigquery_source", value)

    @property
    @pulumi.getter(name="gcsSource")
    def gcs_source(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1GcsSourceArgs']]:
        """
        The Cloud Storage location for the input instances.
        """
        return pulumi.get(self, "gcs_source")

    @gcs_source.setter
    def gcs_source(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsSourceArgs']]):
        pulumi.set(self, "gcs_source", value)


@pulumi.input_type
class GoogleCloudAiplatformV1BatchPredictionJobInstanceConfigArgs:
    def __init__(__self__, *,
                 excluded_fields: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 included_fields: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 instance_type: Optional[pulumi.Input[str]] = None,
                 key_field: Optional[pulumi.Input[str]] = None):
        """
        Configuration defining how to transform batch prediction input instances to the instances that the Model accepts.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] excluded_fields: Fields that will be excluded in the prediction instance that is sent to the Model. Excluded will be attached to the batch prediction output if key_field is not specified. When excluded_fields is populated, included_fields must be empty. The input must be JSONL with objects at each line, CSV, BigQuery or TfRecord.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] included_fields: Fields that will be included in the prediction instance that is sent to the Model. If instance_type is `array`, the order of field names in included_fields also determines the order of the values in the array. When included_fields is populated, excluded_fields must be empty. The input must be JSONL with objects at each line, CSV, BigQuery or TfRecord.
        :param pulumi.Input[str] instance_type: The format of the instance that the Model accepts. Vertex AI will convert compatible batch prediction input instance formats to the specified format. Supported values are: * `object`: Each input is converted to JSON object format. * For `bigquery`, each row is converted to an object. * For `jsonl`, each line of the JSONL input must be an object. * Does not apply to `csv`, `file-list`, `tf-record`, or `tf-record-gzip`. * `array`: Each input is converted to JSON array format. * For `bigquery`, each row is converted to an array. The order of columns is determined by the BigQuery column order, unless included_fields is populated. included_fields must be populated for specifying field orders. * For `jsonl`, if each line of the JSONL input is an object, included_fields must be populated for specifying field orders. * Does not apply to `csv`, `file-list`, `tf-record`, or `tf-record-gzip`. If not specified, Vertex AI converts the batch prediction input as follows: * For `bigquery` and `csv`, the behavior is the same as `array`. The order of columns is the same as defined in the file or table, unless included_fields is populated. * For `jsonl`, the prediction instance format is determined by each line of the input. * For `tf-record`/`tf-record-gzip`, each record will be converted to an object in the format of `{"b64": }`, where `` is the Base64-encoded string of the content of the record. * For `file-list`, each file in the list will be converted to an object in the format of `{"b64": }`, where `` is the Base64-encoded string of the content of the file.
        :param pulumi.Input[str] key_field: The name of the field that is considered as a key. The values identified by the key field is not included in the transformed instances that is sent to the Model. This is similar to specifying this name of the field in excluded_fields. In addition, the batch prediction output will not include the instances. Instead the output will only include the value of the key field, in a field named `key` in the output: * For `jsonl` output format, the output will have a `key` field instead of the `instance` field. * For `csv`/`bigquery` output format, the output will have have a `key` column instead of the instance feature columns. The input must be JSONL with objects at each line, CSV, BigQuery or TfRecord.
        """
        if excluded_fields is not None:
            pulumi.set(__self__, "excluded_fields", excluded_fields)
        if included_fields is not None:
            pulumi.set(__self__, "included_fields", included_fields)
        if instance_type is not None:
            pulumi.set(__self__, "instance_type", instance_type)
        if key_field is not None:
            pulumi.set(__self__, "key_field", key_field)

    @property
    @pulumi.getter(name="excludedFields")
    def excluded_fields(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Fields that will be excluded in the prediction instance that is sent to the Model. Excluded will be attached to the batch prediction output if key_field is not specified. When excluded_fields is populated, included_fields must be empty. The input must be JSONL with objects at each line, CSV, BigQuery or TfRecord.
        """
        return pulumi.get(self, "excluded_fields")

    @excluded_fields.setter
    def excluded_fields(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "excluded_fields", value)

    @property
    @pulumi.getter(name="includedFields")
    def included_fields(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Fields that will be included in the prediction instance that is sent to the Model. If instance_type is `array`, the order of field names in included_fields also determines the order of the values in the array. When included_fields is populated, excluded_fields must be empty. The input must be JSONL with objects at each line, CSV, BigQuery or TfRecord.
        """
        return pulumi.get(self, "included_fields")

    @included_fields.setter
    def included_fields(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "included_fields", value)

    @property
    @pulumi.getter(name="instanceType")
    def instance_type(self) -> Optional[pulumi.Input[str]]:
        """
        The format of the instance that the Model accepts. Vertex AI will convert compatible batch prediction input instance formats to the specified format. Supported values are: * `object`: Each input is converted to JSON object format. * For `bigquery`, each row is converted to an object. * For `jsonl`, each line of the JSONL input must be an object. * Does not apply to `csv`, `file-list`, `tf-record`, or `tf-record-gzip`. * `array`: Each input is converted to JSON array format. * For `bigquery`, each row is converted to an array. The order of columns is determined by the BigQuery column order, unless included_fields is populated. included_fields must be populated for specifying field orders. * For `jsonl`, if each line of the JSONL input is an object, included_fields must be populated for specifying field orders. * Does not apply to `csv`, `file-list`, `tf-record`, or `tf-record-gzip`. If not specified, Vertex AI converts the batch prediction input as follows: * For `bigquery` and `csv`, the behavior is the same as `array`. The order of columns is the same as defined in the file or table, unless included_fields is populated. * For `jsonl`, the prediction instance format is determined by each line of the input. * For `tf-record`/`tf-record-gzip`, each record will be converted to an object in the format of `{"b64": }`, where `` is the Base64-encoded string of the content of the record. * For `file-list`, each file in the list will be converted to an object in the format of `{"b64": }`, where `` is the Base64-encoded string of the content of the file.
        """
        return pulumi.get(self, "instance_type")

    @instance_type.setter
    def instance_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "instance_type", value)

    @property
    @pulumi.getter(name="keyField")
    def key_field(self) -> Optional[pulumi.Input[str]]:
        """
        The name of the field that is considered as a key. The values identified by the key field is not included in the transformed instances that is sent to the Model. This is similar to specifying this name of the field in excluded_fields. In addition, the batch prediction output will not include the instances. Instead the output will only include the value of the key field, in a field named `key` in the output: * For `jsonl` output format, the output will have a `key` field instead of the `instance` field. * For `csv`/`bigquery` output format, the output will have have a `key` column instead of the instance feature columns. The input must be JSONL with objects at each line, CSV, BigQuery or TfRecord.
        """
        return pulumi.get(self, "key_field")

    @key_field.setter
    def key_field(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "key_field", value)


@pulumi.input_type
class GoogleCloudAiplatformV1BatchPredictionJobOutputConfigArgs:
    def __init__(__self__, *,
                 predictions_format: pulumi.Input[str],
                 bigquery_destination: Optional[pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs']] = None,
                 gcs_destination: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs']] = None):
        """
        Configures the output of BatchPredictionJob. See Model.supported_output_storage_formats for supported output formats, and how predictions are expressed via any of them.
        :param pulumi.Input[str] predictions_format: The format in which Vertex AI gives the predictions, must be one of the Model's supported_output_storage_formats.
        :param pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs'] bigquery_destination: The BigQuery project or dataset location where the output is to be written to. If project is provided, a new dataset is created with name `prediction__` where is made BigQuery-dataset-name compatible (for example, most special characters become underscores), and timestamp is in YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset two tables will be created, `predictions`, and `errors`. If the Model has both instance and prediction schemata defined then the tables have columns as follows: The `predictions` table contains instances for which the prediction succeeded, it has columns as per a concatenation of the Model's instance and prediction schemata. The `errors` table contains rows for which the prediction has failed, it has instance columns, as per the instance schema, followed by a single "errors" column, which as values has google.rpc.Status represented as a STRUCT, and containing only `code` and `message`.
        :param pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs'] gcs_destination: The Cloud Storage location of the directory where the output is to be written to. In the given directory a new directory is created. Its name is `prediction--`, where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. Inside of it files `predictions_0001.`, `predictions_0002.`, ..., `predictions_N.` are created where `` depends on chosen predictions_format, and N may equal 0001 and depends on the total number of successfully predicted instances. If the Model has both instance and prediction schemata defined then each such file contains predictions as per the predictions_format. If prediction for any instance failed (partially or completely), then an additional `errors_0001.`, `errors_0002.`,..., `errors_N.` files are created (N depends on total number of failed predictions). These files contain the failed instances, as per their schema, followed by an additional `error` field which as value has google.rpc.Status containing only `code` and `message` fields.
        """
        pulumi.set(__self__, "predictions_format", predictions_format)
        if bigquery_destination is not None:
            pulumi.set(__self__, "bigquery_destination", bigquery_destination)
        if gcs_destination is not None:
            pulumi.set(__self__, "gcs_destination", gcs_destination)

    @property
    @pulumi.getter(name="predictionsFormat")
    def predictions_format(self) -> pulumi.Input[str]:
        """
        The format in which Vertex AI gives the predictions, must be one of the Model's supported_output_storage_formats.
        """
        return pulumi.get(self, "predictions_format")

    @predictions_format.setter
    def predictions_format(self, value: pulumi.Input[str]):
        pulumi.set(self, "predictions_format", value)

    @property
    @pulumi.getter(name="bigqueryDestination")
    def bigquery_destination(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs']]:
        """
        The BigQuery project or dataset location where the output is to be written to. If project is provided, a new dataset is created with name `prediction__` where is made BigQuery-dataset-name compatible (for example, most special characters become underscores), and timestamp is in YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset two tables will be created, `predictions`, and `errors`. If the Model has both instance and prediction schemata defined then the tables have columns as follows: The `predictions` table contains instances for which the prediction succeeded, it has columns as per a concatenation of the Model's instance and prediction schemata. The `errors` table contains rows for which the prediction has failed, it has instance columns, as per the instance schema, followed by a single "errors" column, which as values has google.rpc.Status represented as a STRUCT, and containing only `code` and `message`.
        """
        return pulumi.get(self, "bigquery_destination")

    @bigquery_destination.setter
    def bigquery_destination(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs']]):
        pulumi.set(self, "bigquery_destination", value)

    @property
    @pulumi.getter(name="gcsDestination")
    def gcs_destination(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs']]:
        """
        The Cloud Storage location of the directory where the output is to be written to. In the given directory a new directory is created. Its name is `prediction--`, where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. Inside of it files `predictions_0001.`, `predictions_0002.`, ..., `predictions_N.` are created where `` depends on chosen predictions_format, and N may equal 0001 and depends on the total number of successfully predicted instances. If the Model has both instance and prediction schemata defined then each such file contains predictions as per the predictions_format. If prediction for any instance failed (partially or completely), then an additional `errors_0001.`, `errors_0002.`,..., `errors_N.` files are created (N depends on total number of failed predictions). These files contain the failed instances, as per their schema, followed by an additional `error` field which as value has google.rpc.Status containing only `code` and `message` fields.
        """
        return pulumi.get(self, "gcs_destination")

    @gcs_destination.setter
    def gcs_destination(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs']]):
        pulumi.set(self, "gcs_destination", value)


@pulumi.input_type
class GoogleCloudAiplatformV1BigQueryDestinationArgs:
    def __init__(__self__, *,
                 output_uri: pulumi.Input[str]):
        """
        The BigQuery location for the output content.
        :param pulumi.Input[str] output_uri: BigQuery URI to a project or table, up to 2000 characters long. When only the project is specified, the Dataset and Table is created. When the full table reference is specified, the Dataset must exist and table must not exist. Accepted forms: * BigQuery path. For example: `bq://projectId` or `bq://projectId.bqDatasetId` or `bq://projectId.bqDatasetId.bqTableId`.
        """
        pulumi.set(__self__, "output_uri", output_uri)

    @property
    @pulumi.getter(name="outputUri")
    def output_uri(self) -> pulumi.Input[str]:
        """
        BigQuery URI to a project or table, up to 2000 characters long. When only the project is specified, the Dataset and Table is created. When the full table reference is specified, the Dataset must exist and table must not exist. Accepted forms: * BigQuery path. For example: `bq://projectId` or `bq://projectId.bqDatasetId` or `bq://projectId.bqDatasetId.bqTableId`.
        """
        return pulumi.get(self, "output_uri")

    @output_uri.setter
    def output_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "output_uri", value)


@pulumi.input_type
class GoogleCloudAiplatformV1BigQuerySourceArgs:
    def __init__(__self__, *,
                 input_uri: pulumi.Input[str]):
        """
        The BigQuery location for the input content.
        :param pulumi.Input[str] input_uri: BigQuery URI to a table, up to 2000 characters long. Accepted forms: * BigQuery path. For example: `bq://projectId.bqDatasetId.bqTableId`.
        """
        pulumi.set(__self__, "input_uri", input_uri)

    @property
    @pulumi.getter(name="inputUri")
    def input_uri(self) -> pulumi.Input[str]:
        """
        BigQuery URI to a table, up to 2000 characters long. Accepted forms: * BigQuery path. For example: `bq://projectId.bqDatasetId.bqTableId`.
        """
        return pulumi.get(self, "input_uri")

    @input_uri.setter
    def input_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "input_uri", value)


@pulumi.input_type
class GoogleCloudAiplatformV1BlurBaselineConfigArgs:
    def __init__(__self__, *,
                 max_blur_sigma: Optional[pulumi.Input[float]] = None):
        """
        Config for blur baseline. When enabled, a linear path from the maximally blurred image to the input image is created. Using a blurred baseline instead of zero (black image) is motivated by the BlurIG approach explained here: https://arxiv.org/abs/2004.03383
        :param pulumi.Input[float] max_blur_sigma: The standard deviation of the blur kernel for the blurred baseline. The same blurring parameter is used for both the height and the width dimension. If not set, the method defaults to the zero (i.e. black for images) baseline.
        """
        if max_blur_sigma is not None:
            pulumi.set(__self__, "max_blur_sigma", max_blur_sigma)

    @property
    @pulumi.getter(name="maxBlurSigma")
    def max_blur_sigma(self) -> Optional[pulumi.Input[float]]:
        """
        The standard deviation of the blur kernel for the blurred baseline. The same blurring parameter is used for both the height and the width dimension. If not set, the method defaults to the zero (i.e. black for images) baseline.
        """
        return pulumi.get(self, "max_blur_sigma")

    @max_blur_sigma.setter
    def max_blur_sigma(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "max_blur_sigma", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ContainerSpecArgs:
    def __init__(__self__, *,
                 image_uri: pulumi.Input[str],
                 args: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 command: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 env: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1EnvVarArgs']]]] = None):
        """
        The spec of a Container.
        :param pulumi.Input[str] image_uri: The URI of a container image in the Container Registry that is to be run on each worker replica.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] args: The arguments to be passed when starting the container.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] command: The command to be invoked when the container is started. It overrides the entrypoint instruction in Dockerfile when provided.
        :param pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1EnvVarArgs']]] env: Environment variables to be passed to the container. Maximum limit is 100.
        """
        pulumi.set(__self__, "image_uri", image_uri)
        if args is not None:
            pulumi.set(__self__, "args", args)
        if command is not None:
            pulumi.set(__self__, "command", command)
        if env is not None:
            pulumi.set(__self__, "env", env)

    @property
    @pulumi.getter(name="imageUri")
    def image_uri(self) -> pulumi.Input[str]:
        """
        The URI of a container image in the Container Registry that is to be run on each worker replica.
        """
        return pulumi.get(self, "image_uri")

    @image_uri.setter
    def image_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "image_uri", value)

    @property
    @pulumi.getter
    def args(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The arguments to be passed when starting the container.
        """
        return pulumi.get(self, "args")

    @args.setter
    def args(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "args", value)

    @property
    @pulumi.getter
    def command(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The command to be invoked when the container is started. It overrides the entrypoint instruction in Dockerfile when provided.
        """
        return pulumi.get(self, "command")

    @command.setter
    def command(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "command", value)

    @property
    @pulumi.getter
    def env(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1EnvVarArgs']]]]:
        """
        Environment variables to be passed to the container. Maximum limit is 100.
        """
        return pulumi.get(self, "env")

    @env.setter
    def env(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1EnvVarArgs']]]]):
        pulumi.set(self, "env", value)


@pulumi.input_type
class GoogleCloudAiplatformV1CreatePipelineJobRequestArgs:
    def __init__(__self__, *,
                 parent: pulumi.Input[str],
                 pipeline_job: pulumi.Input['GoogleCloudAiplatformV1PipelineJobArgs'],
                 pipeline_job_id: Optional[pulumi.Input[str]] = None):
        """
        Request message for PipelineService.CreatePipelineJob.
        :param pulumi.Input[str] parent: The resource name of the Location to create the PipelineJob in. Format: `projects/{project}/locations/{location}`
        :param pulumi.Input['GoogleCloudAiplatformV1PipelineJobArgs'] pipeline_job: The PipelineJob to create.
        :param pulumi.Input[str] pipeline_job_id: The ID to use for the PipelineJob, which will become the final component of the PipelineJob name. If not provided, an ID will be automatically generated. This value should be less than 128 characters, and valid characters are `/a-z-/`.
        """
        pulumi.set(__self__, "parent", parent)
        pulumi.set(__self__, "pipeline_job", pipeline_job)
        if pipeline_job_id is not None:
            pulumi.set(__self__, "pipeline_job_id", pipeline_job_id)

    @property
    @pulumi.getter
    def parent(self) -> pulumi.Input[str]:
        """
        The resource name of the Location to create the PipelineJob in. Format: `projects/{project}/locations/{location}`
        """
        return pulumi.get(self, "parent")

    @parent.setter
    def parent(self, value: pulumi.Input[str]):
        pulumi.set(self, "parent", value)

    @property
    @pulumi.getter(name="pipelineJob")
    def pipeline_job(self) -> pulumi.Input['GoogleCloudAiplatformV1PipelineJobArgs']:
        """
        The PipelineJob to create.
        """
        return pulumi.get(self, "pipeline_job")

    @pipeline_job.setter
    def pipeline_job(self, value: pulumi.Input['GoogleCloudAiplatformV1PipelineJobArgs']):
        pulumi.set(self, "pipeline_job", value)

    @property
    @pulumi.getter(name="pipelineJobId")
    def pipeline_job_id(self) -> Optional[pulumi.Input[str]]:
        """
        The ID to use for the PipelineJob, which will become the final component of the PipelineJob name. If not provided, an ID will be automatically generated. This value should be less than 128 characters, and valid characters are `/a-z-/`.
        """
        return pulumi.get(self, "pipeline_job_id")

    @pipeline_job_id.setter
    def pipeline_job_id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "pipeline_job_id", value)


@pulumi.input_type
class GoogleCloudAiplatformV1CustomJobSpecArgs:
    def __init__(__self__, *,
                 worker_pool_specs: pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1WorkerPoolSpecArgs']]],
                 base_output_directory: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs']] = None,
                 enable_dashboard_access: Optional[pulumi.Input[bool]] = None,
                 enable_web_access: Optional[pulumi.Input[bool]] = None,
                 experiment: Optional[pulumi.Input[str]] = None,
                 experiment_run: Optional[pulumi.Input[str]] = None,
                 network: Optional[pulumi.Input[str]] = None,
                 protected_artifact_location_id: Optional[pulumi.Input[str]] = None,
                 reserved_ip_ranges: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 scheduling: Optional[pulumi.Input['GoogleCloudAiplatformV1SchedulingArgs']] = None,
                 service_account: Optional[pulumi.Input[str]] = None,
                 tensorboard: Optional[pulumi.Input[str]] = None):
        """
        Represents the spec of a CustomJob.
        :param pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1WorkerPoolSpecArgs']]] worker_pool_specs: The spec of the worker pools including machine type and Docker image. All worker pools except the first one are optional and can be skipped by providing an empty value.
        :param pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs'] base_output_directory: The Cloud Storage location to store the output of this CustomJob or HyperparameterTuningJob. For HyperparameterTuningJob, the baseOutputDirectory of each child CustomJob backing a Trial is set to a subdirectory of name id under its parent HyperparameterTuningJob's baseOutputDirectory. The following Vertex AI environment variables will be passed to containers or python modules when this field is set: For CustomJob: * AIP_MODEL_DIR = `/model/` * AIP_CHECKPOINT_DIR = `/checkpoints/` * AIP_TENSORBOARD_LOG_DIR = `/logs/` For CustomJob backing a Trial of HyperparameterTuningJob: * AIP_MODEL_DIR = `//model/` * AIP_CHECKPOINT_DIR = `//checkpoints/` * AIP_TENSORBOARD_LOG_DIR = `//logs/`
        :param pulumi.Input[bool] enable_dashboard_access: Optional. Whether you want Vertex AI to enable access to the customized dashboard in training chief container. If set to `true`, you can access the dashboard at the URIs given by CustomJob.web_access_uris or Trial.web_access_uris (within HyperparameterTuningJob.trials).
        :param pulumi.Input[bool] enable_web_access: Optional. Whether you want Vertex AI to enable [interactive shell access](https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell) to training containers. If set to `true`, you can access interactive shells at the URIs given by CustomJob.web_access_uris or Trial.web_access_uris (within HyperparameterTuningJob.trials).
        :param pulumi.Input[str] experiment: Optional. The Experiment associated with this job. Format: `projects/{project}/locations/{location}/metadataStores/{metadataStores}/contexts/{experiment-name}`
        :param pulumi.Input[str] experiment_run: Optional. The Experiment Run associated with this job. Format: `projects/{project}/locations/{location}/metadataStores/{metadataStores}/contexts/{experiment-name}-{experiment-run-name}`
        :param pulumi.Input[str] network: Optional. The full name of the Compute Engine [network](/compute/docs/networks-and-firewalls#networks) to which the Job should be peered. For example, `projects/12345/global/networks/myVPC`. [Format](/compute/docs/reference/rest/v1/networks/insert) is of the form `projects/{project}/global/networks/{network}`. Where {project} is a project number, as in `12345`, and {network} is a network name. To specify this field, you must have already [configured VPC Network Peering for Vertex AI](https://cloud.google.com/vertex-ai/docs/general/vpc-peering). If this field is left unspecified, the job is not peered with any network.
        :param pulumi.Input[str] protected_artifact_location_id: The ID of the location to store protected artifacts. e.g. us-central1. Populate only when the location is different than CustomJob location. List of supported locations: https://cloud.google.com/vertex-ai/docs/general/locations
        :param pulumi.Input[Sequence[pulumi.Input[str]]] reserved_ip_ranges: Optional. A list of names for the reserved ip ranges under the VPC network that can be used for this job. If set, we will deploy the job within the provided ip ranges. Otherwise, the job will be deployed to any ip ranges under the provided VPC network. Example: ['vertex-ai-ip-range'].
        :param pulumi.Input['GoogleCloudAiplatformV1SchedulingArgs'] scheduling: Scheduling options for a CustomJob.
        :param pulumi.Input[str] service_account: Specifies the service account for workload run-as account. Users submitting jobs must have act-as permission on this run-as account. If unspecified, the [Vertex AI Custom Code Service Agent](https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents) for the CustomJob's project is used.
        :param pulumi.Input[str] tensorboard: Optional. The name of a Vertex AI Tensorboard resource to which this CustomJob will upload Tensorboard logs. Format: `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
        """
        pulumi.set(__self__, "worker_pool_specs", worker_pool_specs)
        if base_output_directory is not None:
            pulumi.set(__self__, "base_output_directory", base_output_directory)
        if enable_dashboard_access is not None:
            pulumi.set(__self__, "enable_dashboard_access", enable_dashboard_access)
        if enable_web_access is not None:
            pulumi.set(__self__, "enable_web_access", enable_web_access)
        if experiment is not None:
            pulumi.set(__self__, "experiment", experiment)
        if experiment_run is not None:
            pulumi.set(__self__, "experiment_run", experiment_run)
        if network is not None:
            pulumi.set(__self__, "network", network)
        if protected_artifact_location_id is not None:
            pulumi.set(__self__, "protected_artifact_location_id", protected_artifact_location_id)
        if reserved_ip_ranges is not None:
            pulumi.set(__self__, "reserved_ip_ranges", reserved_ip_ranges)
        if scheduling is not None:
            pulumi.set(__self__, "scheduling", scheduling)
        if service_account is not None:
            pulumi.set(__self__, "service_account", service_account)
        if tensorboard is not None:
            pulumi.set(__self__, "tensorboard", tensorboard)

    @property
    @pulumi.getter(name="workerPoolSpecs")
    def worker_pool_specs(self) -> pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1WorkerPoolSpecArgs']]]:
        """
        The spec of the worker pools including machine type and Docker image. All worker pools except the first one are optional and can be skipped by providing an empty value.
        """
        return pulumi.get(self, "worker_pool_specs")

    @worker_pool_specs.setter
    def worker_pool_specs(self, value: pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1WorkerPoolSpecArgs']]]):
        pulumi.set(self, "worker_pool_specs", value)

    @property
    @pulumi.getter(name="baseOutputDirectory")
    def base_output_directory(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs']]:
        """
        The Cloud Storage location to store the output of this CustomJob or HyperparameterTuningJob. For HyperparameterTuningJob, the baseOutputDirectory of each child CustomJob backing a Trial is set to a subdirectory of name id under its parent HyperparameterTuningJob's baseOutputDirectory. The following Vertex AI environment variables will be passed to containers or python modules when this field is set: For CustomJob: * AIP_MODEL_DIR = `/model/` * AIP_CHECKPOINT_DIR = `/checkpoints/` * AIP_TENSORBOARD_LOG_DIR = `/logs/` For CustomJob backing a Trial of HyperparameterTuningJob: * AIP_MODEL_DIR = `//model/` * AIP_CHECKPOINT_DIR = `//checkpoints/` * AIP_TENSORBOARD_LOG_DIR = `//logs/`
        """
        return pulumi.get(self, "base_output_directory")

    @base_output_directory.setter
    def base_output_directory(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs']]):
        pulumi.set(self, "base_output_directory", value)

    @property
    @pulumi.getter(name="enableDashboardAccess")
    def enable_dashboard_access(self) -> Optional[pulumi.Input[bool]]:
        """
        Optional. Whether you want Vertex AI to enable access to the customized dashboard in training chief container. If set to `true`, you can access the dashboard at the URIs given by CustomJob.web_access_uris or Trial.web_access_uris (within HyperparameterTuningJob.trials).
        """
        return pulumi.get(self, "enable_dashboard_access")

    @enable_dashboard_access.setter
    def enable_dashboard_access(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_dashboard_access", value)

    @property
    @pulumi.getter(name="enableWebAccess")
    def enable_web_access(self) -> Optional[pulumi.Input[bool]]:
        """
        Optional. Whether you want Vertex AI to enable [interactive shell access](https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell) to training containers. If set to `true`, you can access interactive shells at the URIs given by CustomJob.web_access_uris or Trial.web_access_uris (within HyperparameterTuningJob.trials).
        """
        return pulumi.get(self, "enable_web_access")

    @enable_web_access.setter
    def enable_web_access(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_web_access", value)

    @property
    @pulumi.getter
    def experiment(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. The Experiment associated with this job. Format: `projects/{project}/locations/{location}/metadataStores/{metadataStores}/contexts/{experiment-name}`
        """
        return pulumi.get(self, "experiment")

    @experiment.setter
    def experiment(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "experiment", value)

    @property
    @pulumi.getter(name="experimentRun")
    def experiment_run(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. The Experiment Run associated with this job. Format: `projects/{project}/locations/{location}/metadataStores/{metadataStores}/contexts/{experiment-name}-{experiment-run-name}`
        """
        return pulumi.get(self, "experiment_run")

    @experiment_run.setter
    def experiment_run(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "experiment_run", value)

    @property
    @pulumi.getter
    def network(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. The full name of the Compute Engine [network](/compute/docs/networks-and-firewalls#networks) to which the Job should be peered. For example, `projects/12345/global/networks/myVPC`. [Format](/compute/docs/reference/rest/v1/networks/insert) is of the form `projects/{project}/global/networks/{network}`. Where {project} is a project number, as in `12345`, and {network} is a network name. To specify this field, you must have already [configured VPC Network Peering for Vertex AI](https://cloud.google.com/vertex-ai/docs/general/vpc-peering). If this field is left unspecified, the job is not peered with any network.
        """
        return pulumi.get(self, "network")

    @network.setter
    def network(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "network", value)

    @property
    @pulumi.getter(name="protectedArtifactLocationId")
    def protected_artifact_location_id(self) -> Optional[pulumi.Input[str]]:
        """
        The ID of the location to store protected artifacts. e.g. us-central1. Populate only when the location is different than CustomJob location. List of supported locations: https://cloud.google.com/vertex-ai/docs/general/locations
        """
        return pulumi.get(self, "protected_artifact_location_id")

    @protected_artifact_location_id.setter
    def protected_artifact_location_id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "protected_artifact_location_id", value)

    @property
    @pulumi.getter(name="reservedIpRanges")
    def reserved_ip_ranges(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Optional. A list of names for the reserved ip ranges under the VPC network that can be used for this job. If set, we will deploy the job within the provided ip ranges. Otherwise, the job will be deployed to any ip ranges under the provided VPC network. Example: ['vertex-ai-ip-range'].
        """
        return pulumi.get(self, "reserved_ip_ranges")

    @reserved_ip_ranges.setter
    def reserved_ip_ranges(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "reserved_ip_ranges", value)

    @property
    @pulumi.getter
    def scheduling(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1SchedulingArgs']]:
        """
        Scheduling options for a CustomJob.
        """
        return pulumi.get(self, "scheduling")

    @scheduling.setter
    def scheduling(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1SchedulingArgs']]):
        pulumi.set(self, "scheduling", value)

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> Optional[pulumi.Input[str]]:
        """
        Specifies the service account for workload run-as account. Users submitting jobs must have act-as permission on this run-as account. If unspecified, the [Vertex AI Custom Code Service Agent](https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents) for the CustomJob's project is used.
        """
        return pulumi.get(self, "service_account")

    @service_account.setter
    def service_account(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "service_account", value)

    @property
    @pulumi.getter
    def tensorboard(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. The name of a Vertex AI Tensorboard resource to which this CustomJob will upload Tensorboard logs. Format: `projects/{project}/locations/{location}/tensorboards/{tensorboard}`
        """
        return pulumi.get(self, "tensorboard")

    @tensorboard.setter
    def tensorboard(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "tensorboard", value)


@pulumi.input_type
class GoogleCloudAiplatformV1DedicatedResourcesArgs:
    def __init__(__self__, *,
                 machine_spec: pulumi.Input['GoogleCloudAiplatformV1MachineSpecArgs'],
                 min_replica_count: pulumi.Input[int],
                 autoscaling_metric_specs: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1AutoscalingMetricSpecArgs']]]] = None,
                 max_replica_count: Optional[pulumi.Input[int]] = None):
        """
        A description of resources that are dedicated to a DeployedModel, and that need a higher degree of manual configuration.
        :param pulumi.Input['GoogleCloudAiplatformV1MachineSpecArgs'] machine_spec: Immutable. The specification of a single machine used by the prediction.
        :param pulumi.Input[int] min_replica_count: Immutable. The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
        :param pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1AutoscalingMetricSpecArgs']]] autoscaling_metric_specs: Immutable. The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator's duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator's duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
        :param pulumi.Input[int] max_replica_count: Immutable. The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for (max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
        """
        pulumi.set(__self__, "machine_spec", machine_spec)
        pulumi.set(__self__, "min_replica_count", min_replica_count)
        if autoscaling_metric_specs is not None:
            pulumi.set(__self__, "autoscaling_metric_specs", autoscaling_metric_specs)
        if max_replica_count is not None:
            pulumi.set(__self__, "max_replica_count", max_replica_count)

    @property
    @pulumi.getter(name="machineSpec")
    def machine_spec(self) -> pulumi.Input['GoogleCloudAiplatformV1MachineSpecArgs']:
        """
        Immutable. The specification of a single machine used by the prediction.
        """
        return pulumi.get(self, "machine_spec")

    @machine_spec.setter
    def machine_spec(self, value: pulumi.Input['GoogleCloudAiplatformV1MachineSpecArgs']):
        pulumi.set(self, "machine_spec", value)

    @property
    @pulumi.getter(name="minReplicaCount")
    def min_replica_count(self) -> pulumi.Input[int]:
        """
        Immutable. The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
        """
        return pulumi.get(self, "min_replica_count")

    @min_replica_count.setter
    def min_replica_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "min_replica_count", value)

    @property
    @pulumi.getter(name="autoscalingMetricSpecs")
    def autoscaling_metric_specs(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1AutoscalingMetricSpecArgs']]]]:
        """
        Immutable. The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator's duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator's duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to `aiplatform.googleapis.com/prediction/online/cpu/utilization` and autoscaling_metric_specs.target to `80`.
        """
        return pulumi.get(self, "autoscaling_metric_specs")

    @autoscaling_metric_specs.setter
    def autoscaling_metric_specs(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1AutoscalingMetricSpecArgs']]]]):
        pulumi.set(self, "autoscaling_metric_specs", value)

    @property
    @pulumi.getter(name="maxReplicaCount")
    def max_replica_count(self) -> Optional[pulumi.Input[int]]:
        """
        Immutable. The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for (max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
        """
        return pulumi.get(self, "max_replica_count")

    @max_replica_count.setter
    def max_replica_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_replica_count", value)


@pulumi.input_type
class GoogleCloudAiplatformV1DiskSpecArgs:
    def __init__(__self__, *,
                 boot_disk_size_gb: Optional[pulumi.Input[int]] = None,
                 boot_disk_type: Optional[pulumi.Input[str]] = None):
        """
        Represents the spec of disk options.
        :param pulumi.Input[int] boot_disk_size_gb: Size in GB of the boot disk (default is 100GB).
        :param pulumi.Input[str] boot_disk_type: Type of the boot disk (default is "pd-ssd"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        """
        if boot_disk_size_gb is not None:
            pulumi.set(__self__, "boot_disk_size_gb", boot_disk_size_gb)
        if boot_disk_type is not None:
            pulumi.set(__self__, "boot_disk_type", boot_disk_type)

    @property
    @pulumi.getter(name="bootDiskSizeGb")
    def boot_disk_size_gb(self) -> Optional[pulumi.Input[int]]:
        """
        Size in GB of the boot disk (default is 100GB).
        """
        return pulumi.get(self, "boot_disk_size_gb")

    @boot_disk_size_gb.setter
    def boot_disk_size_gb(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "boot_disk_size_gb", value)

    @property
    @pulumi.getter(name="bootDiskType")
    def boot_disk_type(self) -> Optional[pulumi.Input[str]]:
        """
        Type of the boot disk (default is "pd-ssd"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or "pd-standard" (Persistent Disk Hard Disk Drive).
        """
        return pulumi.get(self, "boot_disk_type")

    @boot_disk_type.setter
    def boot_disk_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "boot_disk_type", value)


@pulumi.input_type
class GoogleCloudAiplatformV1EncryptionSpecArgs:
    def __init__(__self__, *,
                 kms_key_name: pulumi.Input[str]):
        """
        Represents a customer-managed encryption key spec that can be applied to a top-level resource.
        :param pulumi.Input[str] kms_key_name: The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
        """
        pulumi.set(__self__, "kms_key_name", kms_key_name)

    @property
    @pulumi.getter(name="kmsKeyName")
    def kms_key_name(self) -> pulumi.Input[str]:
        """
        The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: `projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.
        """
        return pulumi.get(self, "kms_key_name")

    @kms_key_name.setter
    def kms_key_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "kms_key_name", value)


@pulumi.input_type
class GoogleCloudAiplatformV1EnvVarArgs:
    def __init__(__self__, *,
                 name: pulumi.Input[str],
                 value: pulumi.Input[str]):
        """
        Represents an environment variable present in a Container or Python Module.
        :param pulumi.Input[str] name: Name of the environment variable. Must be a valid C identifier.
        :param pulumi.Input[str] value: Variables that reference a $(VAR_NAME) are expanded using the previous defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not.
        """
        pulumi.set(__self__, "name", name)
        pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter
    def name(self) -> pulumi.Input[str]:
        """
        Name of the environment variable. Must be a valid C identifier.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: pulumi.Input[str]):
        pulumi.set(self, "name", value)

    @property
    @pulumi.getter
    def value(self) -> pulumi.Input[str]:
        """
        Variables that reference a $(VAR_NAME) are expanded using the previous defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not.
        """
        return pulumi.get(self, "value")

    @value.setter
    def value(self, value: pulumi.Input[str]):
        pulumi.set(self, "value", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ExamplesExampleGcsSourceArgs:
    def __init__(__self__, *,
                 data_format: Optional[pulumi.Input['GoogleCloudAiplatformV1ExamplesExampleGcsSourceDataFormat']] = None,
                 gcs_source: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsSourceArgs']] = None):
        """
        The Cloud Storage input instances.
        :param pulumi.Input['GoogleCloudAiplatformV1ExamplesExampleGcsSourceDataFormat'] data_format: The format in which instances are given, if not specified, assume it's JSONL format. Currently only JSONL format is supported.
        :param pulumi.Input['GoogleCloudAiplatformV1GcsSourceArgs'] gcs_source: The Cloud Storage location for the input instances.
        """
        if data_format is not None:
            pulumi.set(__self__, "data_format", data_format)
        if gcs_source is not None:
            pulumi.set(__self__, "gcs_source", gcs_source)

    @property
    @pulumi.getter(name="dataFormat")
    def data_format(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ExamplesExampleGcsSourceDataFormat']]:
        """
        The format in which instances are given, if not specified, assume it's JSONL format. Currently only JSONL format is supported.
        """
        return pulumi.get(self, "data_format")

    @data_format.setter
    def data_format(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ExamplesExampleGcsSourceDataFormat']]):
        pulumi.set(self, "data_format", value)

    @property
    @pulumi.getter(name="gcsSource")
    def gcs_source(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1GcsSourceArgs']]:
        """
        The Cloud Storage location for the input instances.
        """
        return pulumi.get(self, "gcs_source")

    @gcs_source.setter
    def gcs_source(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsSourceArgs']]):
        pulumi.set(self, "gcs_source", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ExamplesArgs:
    def __init__(__self__, *,
                 example_gcs_source: Optional[pulumi.Input['GoogleCloudAiplatformV1ExamplesExampleGcsSourceArgs']] = None,
                 nearest_neighbor_search_config: Optional[Any] = None,
                 neighbor_count: Optional[pulumi.Input[int]] = None,
                 presets: Optional[pulumi.Input['GoogleCloudAiplatformV1PresetsArgs']] = None):
        """
        Example-based explainability that returns the nearest neighbors from the provided dataset.
        :param pulumi.Input['GoogleCloudAiplatformV1ExamplesExampleGcsSourceArgs'] example_gcs_source: The Cloud Storage input instances.
        :param Any nearest_neighbor_search_config: The full configuration for the generated index, the semantics are the same as metadata and should match [NearestNeighborSearchConfig](https://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations-example-based#nearest-neighbor-search-config).
        :param pulumi.Input[int] neighbor_count: The number of neighbors to return when querying for examples.
        :param pulumi.Input['GoogleCloudAiplatformV1PresetsArgs'] presets: Simplified preset configuration, which automatically sets configuration values based on the desired query speed-precision trade-off and modality.
        """
        if example_gcs_source is not None:
            pulumi.set(__self__, "example_gcs_source", example_gcs_source)
        if nearest_neighbor_search_config is not None:
            pulumi.set(__self__, "nearest_neighbor_search_config", nearest_neighbor_search_config)
        if neighbor_count is not None:
            pulumi.set(__self__, "neighbor_count", neighbor_count)
        if presets is not None:
            pulumi.set(__self__, "presets", presets)

    @property
    @pulumi.getter(name="exampleGcsSource")
    def example_gcs_source(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ExamplesExampleGcsSourceArgs']]:
        """
        The Cloud Storage input instances.
        """
        return pulumi.get(self, "example_gcs_source")

    @example_gcs_source.setter
    def example_gcs_source(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ExamplesExampleGcsSourceArgs']]):
        pulumi.set(self, "example_gcs_source", value)

    @property
    @pulumi.getter(name="nearestNeighborSearchConfig")
    def nearest_neighbor_search_config(self) -> Optional[Any]:
        """
        The full configuration for the generated index, the semantics are the same as metadata and should match [NearestNeighborSearchConfig](https://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations-example-based#nearest-neighbor-search-config).
        """
        return pulumi.get(self, "nearest_neighbor_search_config")

    @nearest_neighbor_search_config.setter
    def nearest_neighbor_search_config(self, value: Optional[Any]):
        pulumi.set(self, "nearest_neighbor_search_config", value)

    @property
    @pulumi.getter(name="neighborCount")
    def neighbor_count(self) -> Optional[pulumi.Input[int]]:
        """
        The number of neighbors to return when querying for examples.
        """
        return pulumi.get(self, "neighbor_count")

    @neighbor_count.setter
    def neighbor_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "neighbor_count", value)

    @property
    @pulumi.getter
    def presets(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1PresetsArgs']]:
        """
        Simplified preset configuration, which automatically sets configuration values based on the desired query speed-precision trade-off and modality.
        """
        return pulumi.get(self, "presets")

    @presets.setter
    def presets(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1PresetsArgs']]):
        pulumi.set(self, "presets", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainArgs:
    def __init__(__self__, *,
                 max_value: Optional[pulumi.Input[float]] = None,
                 min_value: Optional[pulumi.Input[float]] = None,
                 original_mean: Optional[pulumi.Input[float]] = None,
                 original_stddev: Optional[pulumi.Input[float]] = None):
        """
        Domain details of the input feature value. Provides numeric information about the feature, such as its range (min, max). If the feature has been pre-processed, for example with z-scoring, then it provides information about how to recover the original feature. For example, if the input feature is an image and it has been pre-processed to obtain 0-mean and stddev = 1 values, then original_mean, and original_stddev refer to the mean and stddev of the original feature (e.g. image tensor) from which input feature (with mean = 0 and stddev = 1) was obtained.
        :param pulumi.Input[float] max_value: The maximum permissible value for this feature.
        :param pulumi.Input[float] min_value: The minimum permissible value for this feature.
        :param pulumi.Input[float] original_mean: If this input feature has been normalized to a mean value of 0, the original_mean specifies the mean value of the domain prior to normalization.
        :param pulumi.Input[float] original_stddev: If this input feature has been normalized to a standard deviation of 1.0, the original_stddev specifies the standard deviation of the domain prior to normalization.
        """
        if max_value is not None:
            pulumi.set(__self__, "max_value", max_value)
        if min_value is not None:
            pulumi.set(__self__, "min_value", min_value)
        if original_mean is not None:
            pulumi.set(__self__, "original_mean", original_mean)
        if original_stddev is not None:
            pulumi.set(__self__, "original_stddev", original_stddev)

    @property
    @pulumi.getter(name="maxValue")
    def max_value(self) -> Optional[pulumi.Input[float]]:
        """
        The maximum permissible value for this feature.
        """
        return pulumi.get(self, "max_value")

    @max_value.setter
    def max_value(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "max_value", value)

    @property
    @pulumi.getter(name="minValue")
    def min_value(self) -> Optional[pulumi.Input[float]]:
        """
        The minimum permissible value for this feature.
        """
        return pulumi.get(self, "min_value")

    @min_value.setter
    def min_value(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "min_value", value)

    @property
    @pulumi.getter(name="originalMean")
    def original_mean(self) -> Optional[pulumi.Input[float]]:
        """
        If this input feature has been normalized to a mean value of 0, the original_mean specifies the mean value of the domain prior to normalization.
        """
        return pulumi.get(self, "original_mean")

    @original_mean.setter
    def original_mean(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "original_mean", value)

    @property
    @pulumi.getter(name="originalStddev")
    def original_stddev(self) -> Optional[pulumi.Input[float]]:
        """
        If this input feature has been normalized to a standard deviation of 1.0, the original_stddev specifies the standard deviation of the domain prior to normalization.
        """
        return pulumi.get(self, "original_stddev")

    @original_stddev.setter
    def original_stddev(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "original_stddev", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationArgs:
    def __init__(__self__, *,
                 clip_percent_lowerbound: Optional[pulumi.Input[float]] = None,
                 clip_percent_upperbound: Optional[pulumi.Input[float]] = None,
                 color_map: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationColorMap']] = None,
                 overlay_type: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationOverlayType']] = None,
                 polarity: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationPolarity']] = None,
                 type: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationType']] = None):
        """
        Visualization configurations for image explanation.
        :param pulumi.Input[float] clip_percent_lowerbound: Excludes attributions below the specified percentile, from the highlighted areas. Defaults to 62.
        :param pulumi.Input[float] clip_percent_upperbound: Excludes attributions above the specified percentile from the highlighted areas. Using the clip_percent_upperbound and clip_percent_lowerbound together can be useful for filtering out noise and making it easier to see areas of strong attribution. Defaults to 99.9.
        :param pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationColorMap'] color_map: The color scheme used for the highlighted areas. Defaults to PINK_GREEN for Integrated Gradients attribution, which shows positive attributions in green and negative in pink. Defaults to VIRIDIS for XRAI attribution, which highlights the most influential regions in yellow and the least influential in blue.
        :param pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationOverlayType'] overlay_type: How the original image is displayed in the visualization. Adjusting the overlay can help increase visual clarity if the original image makes it difficult to view the visualization. Defaults to NONE.
        :param pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationPolarity'] polarity: Whether to only highlight pixels with positive contributions, negative or both. Defaults to POSITIVE.
        :param pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationType'] type: Type of the image visualization. Only applicable to Integrated Gradients attribution. OUTLINES shows regions of attribution, while PIXELS shows per-pixel attribution. Defaults to OUTLINES.
        """
        if clip_percent_lowerbound is not None:
            pulumi.set(__self__, "clip_percent_lowerbound", clip_percent_lowerbound)
        if clip_percent_upperbound is not None:
            pulumi.set(__self__, "clip_percent_upperbound", clip_percent_upperbound)
        if color_map is not None:
            pulumi.set(__self__, "color_map", color_map)
        if overlay_type is not None:
            pulumi.set(__self__, "overlay_type", overlay_type)
        if polarity is not None:
            pulumi.set(__self__, "polarity", polarity)
        if type is not None:
            pulumi.set(__self__, "type", type)

    @property
    @pulumi.getter(name="clipPercentLowerbound")
    def clip_percent_lowerbound(self) -> Optional[pulumi.Input[float]]:
        """
        Excludes attributions below the specified percentile, from the highlighted areas. Defaults to 62.
        """
        return pulumi.get(self, "clip_percent_lowerbound")

    @clip_percent_lowerbound.setter
    def clip_percent_lowerbound(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "clip_percent_lowerbound", value)

    @property
    @pulumi.getter(name="clipPercentUpperbound")
    def clip_percent_upperbound(self) -> Optional[pulumi.Input[float]]:
        """
        Excludes attributions above the specified percentile from the highlighted areas. Using the clip_percent_upperbound and clip_percent_lowerbound together can be useful for filtering out noise and making it easier to see areas of strong attribution. Defaults to 99.9.
        """
        return pulumi.get(self, "clip_percent_upperbound")

    @clip_percent_upperbound.setter
    def clip_percent_upperbound(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "clip_percent_upperbound", value)

    @property
    @pulumi.getter(name="colorMap")
    def color_map(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationColorMap']]:
        """
        The color scheme used for the highlighted areas. Defaults to PINK_GREEN for Integrated Gradients attribution, which shows positive attributions in green and negative in pink. Defaults to VIRIDIS for XRAI attribution, which highlights the most influential regions in yellow and the least influential in blue.
        """
        return pulumi.get(self, "color_map")

    @color_map.setter
    def color_map(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationColorMap']]):
        pulumi.set(self, "color_map", value)

    @property
    @pulumi.getter(name="overlayType")
    def overlay_type(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationOverlayType']]:
        """
        How the original image is displayed in the visualization. Adjusting the overlay can help increase visual clarity if the original image makes it difficult to view the visualization. Defaults to NONE.
        """
        return pulumi.get(self, "overlay_type")

    @overlay_type.setter
    def overlay_type(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationOverlayType']]):
        pulumi.set(self, "overlay_type", value)

    @property
    @pulumi.getter
    def polarity(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationPolarity']]:
        """
        Whether to only highlight pixels with positive contributions, negative or both. Defaults to POSITIVE.
        """
        return pulumi.get(self, "polarity")

    @polarity.setter
    def polarity(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationPolarity']]):
        pulumi.set(self, "polarity", value)

    @property
    @pulumi.getter
    def type(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationType']]:
        """
        Type of the image visualization. Only applicable to Integrated Gradients attribution. OUTLINES shows regions of attribution, while PIXELS shows per-pixel attribution. Defaults to OUTLINES.
        """
        return pulumi.get(self, "type")

    @type.setter
    def type(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationType']]):
        pulumi.set(self, "type", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ExplanationMetadataInputMetadataArgs:
    def __init__(__self__, *,
                 dense_shape_tensor_name: Optional[pulumi.Input[str]] = None,
                 encoded_baselines: Optional[pulumi.Input[Sequence[Any]]] = None,
                 encoded_tensor_name: Optional[pulumi.Input[str]] = None,
                 encoding: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataEncoding']] = None,
                 feature_value_domain: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainArgs']] = None,
                 group_name: Optional[pulumi.Input[str]] = None,
                 index_feature_mapping: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 indices_tensor_name: Optional[pulumi.Input[str]] = None,
                 input_baselines: Optional[pulumi.Input[Sequence[Any]]] = None,
                 input_tensor_name: Optional[pulumi.Input[str]] = None,
                 modality: Optional[pulumi.Input[str]] = None,
                 visualization: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationArgs']] = None):
        """
        Metadata of the input of a feature. Fields other than InputMetadata.input_baselines are applicable only for Models that are using Vertex AI-provided images for Tensorflow.
        :param pulumi.Input[str] dense_shape_tensor_name: Specifies the shape of the values of the input if the input is a sparse representation. Refer to Tensorflow documentation for more details: https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
        :param pulumi.Input[Sequence[Any]] encoded_baselines: A list of baselines for the encoded tensor. The shape of each baseline should match the shape of the encoded tensor. If a scalar is provided, Vertex AI broadcasts to the same shape as the encoded tensor.
        :param pulumi.Input[str] encoded_tensor_name: Encoded tensor is a transformation of the input tensor. Must be provided if choosing Integrated Gradients attribution or XRAI attribution and the input tensor is not differentiable. An encoded tensor is generated if the input tensor is encoded by a lookup table.
        :param pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataEncoding'] encoding: Defines how the feature is encoded into the input tensor. Defaults to IDENTITY.
        :param pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainArgs'] feature_value_domain: The domain details of the input feature value. Like min/max, original mean or standard deviation if normalized.
        :param pulumi.Input[str] group_name: Name of the group that the input belongs to. Features with the same group name will be treated as one feature when computing attributions. Features grouped together can have different shapes in value. If provided, there will be one single attribution generated in Attribution.feature_attributions, keyed by the group name.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] index_feature_mapping: A list of feature names for each index in the input tensor. Required when the input InputMetadata.encoding is BAG_OF_FEATURES, BAG_OF_FEATURES_SPARSE, INDICATOR.
        :param pulumi.Input[str] indices_tensor_name: Specifies the index of the values of the input tensor. Required when the input tensor is a sparse representation. Refer to Tensorflow documentation for more details: https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
        :param pulumi.Input[Sequence[Any]] input_baselines: Baseline inputs for this feature. If no baseline is specified, Vertex AI chooses the baseline for this feature. If multiple baselines are specified, Vertex AI returns the average attributions across them in Attribution.feature_attributions. For Vertex AI-provided Tensorflow images (both 1.x and 2.x), the shape of each baseline must match the shape of the input tensor. If a scalar is provided, we broadcast to the same shape as the input tensor. For custom images, the element of the baselines must be in the same format as the feature's input in the instance[]. The schema of any single instance may be specified via Endpoint's DeployedModels' Model's PredictSchemata's instance_schema_uri.
        :param pulumi.Input[str] input_tensor_name: Name of the input tensor for this feature. Required and is only applicable to Vertex AI-provided images for Tensorflow.
        :param pulumi.Input[str] modality: Modality of the feature. Valid values are: numeric, image. Defaults to numeric.
        :param pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationArgs'] visualization: Visualization configurations for image explanation.
        """
        if dense_shape_tensor_name is not None:
            pulumi.set(__self__, "dense_shape_tensor_name", dense_shape_tensor_name)
        if encoded_baselines is not None:
            pulumi.set(__self__, "encoded_baselines", encoded_baselines)
        if encoded_tensor_name is not None:
            pulumi.set(__self__, "encoded_tensor_name", encoded_tensor_name)
        if encoding is not None:
            pulumi.set(__self__, "encoding", encoding)
        if feature_value_domain is not None:
            pulumi.set(__self__, "feature_value_domain", feature_value_domain)
        if group_name is not None:
            pulumi.set(__self__, "group_name", group_name)
        if index_feature_mapping is not None:
            pulumi.set(__self__, "index_feature_mapping", index_feature_mapping)
        if indices_tensor_name is not None:
            pulumi.set(__self__, "indices_tensor_name", indices_tensor_name)
        if input_baselines is not None:
            pulumi.set(__self__, "input_baselines", input_baselines)
        if input_tensor_name is not None:
            pulumi.set(__self__, "input_tensor_name", input_tensor_name)
        if modality is not None:
            pulumi.set(__self__, "modality", modality)
        if visualization is not None:
            pulumi.set(__self__, "visualization", visualization)

    @property
    @pulumi.getter(name="denseShapeTensorName")
    def dense_shape_tensor_name(self) -> Optional[pulumi.Input[str]]:
        """
        Specifies the shape of the values of the input if the input is a sparse representation. Refer to Tensorflow documentation for more details: https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
        """
        return pulumi.get(self, "dense_shape_tensor_name")

    @dense_shape_tensor_name.setter
    def dense_shape_tensor_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "dense_shape_tensor_name", value)

    @property
    @pulumi.getter(name="encodedBaselines")
    def encoded_baselines(self) -> Optional[pulumi.Input[Sequence[Any]]]:
        """
        A list of baselines for the encoded tensor. The shape of each baseline should match the shape of the encoded tensor. If a scalar is provided, Vertex AI broadcasts to the same shape as the encoded tensor.
        """
        return pulumi.get(self, "encoded_baselines")

    @encoded_baselines.setter
    def encoded_baselines(self, value: Optional[pulumi.Input[Sequence[Any]]]):
        pulumi.set(self, "encoded_baselines", value)

    @property
    @pulumi.getter(name="encodedTensorName")
    def encoded_tensor_name(self) -> Optional[pulumi.Input[str]]:
        """
        Encoded tensor is a transformation of the input tensor. Must be provided if choosing Integrated Gradients attribution or XRAI attribution and the input tensor is not differentiable. An encoded tensor is generated if the input tensor is encoded by a lookup table.
        """
        return pulumi.get(self, "encoded_tensor_name")

    @encoded_tensor_name.setter
    def encoded_tensor_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "encoded_tensor_name", value)

    @property
    @pulumi.getter
    def encoding(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataEncoding']]:
        """
        Defines how the feature is encoded into the input tensor. Defaults to IDENTITY.
        """
        return pulumi.get(self, "encoding")

    @encoding.setter
    def encoding(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataEncoding']]):
        pulumi.set(self, "encoding", value)

    @property
    @pulumi.getter(name="featureValueDomain")
    def feature_value_domain(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainArgs']]:
        """
        The domain details of the input feature value. Like min/max, original mean or standard deviation if normalized.
        """
        return pulumi.get(self, "feature_value_domain")

    @feature_value_domain.setter
    def feature_value_domain(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataFeatureValueDomainArgs']]):
        pulumi.set(self, "feature_value_domain", value)

    @property
    @pulumi.getter(name="groupName")
    def group_name(self) -> Optional[pulumi.Input[str]]:
        """
        Name of the group that the input belongs to. Features with the same group name will be treated as one feature when computing attributions. Features grouped together can have different shapes in value. If provided, there will be one single attribution generated in Attribution.feature_attributions, keyed by the group name.
        """
        return pulumi.get(self, "group_name")

    @group_name.setter
    def group_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "group_name", value)

    @property
    @pulumi.getter(name="indexFeatureMapping")
    def index_feature_mapping(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        A list of feature names for each index in the input tensor. Required when the input InputMetadata.encoding is BAG_OF_FEATURES, BAG_OF_FEATURES_SPARSE, INDICATOR.
        """
        return pulumi.get(self, "index_feature_mapping")

    @index_feature_mapping.setter
    def index_feature_mapping(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "index_feature_mapping", value)

    @property
    @pulumi.getter(name="indicesTensorName")
    def indices_tensor_name(self) -> Optional[pulumi.Input[str]]:
        """
        Specifies the index of the values of the input tensor. Required when the input tensor is a sparse representation. Refer to Tensorflow documentation for more details: https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
        """
        return pulumi.get(self, "indices_tensor_name")

    @indices_tensor_name.setter
    def indices_tensor_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "indices_tensor_name", value)

    @property
    @pulumi.getter(name="inputBaselines")
    def input_baselines(self) -> Optional[pulumi.Input[Sequence[Any]]]:
        """
        Baseline inputs for this feature. If no baseline is specified, Vertex AI chooses the baseline for this feature. If multiple baselines are specified, Vertex AI returns the average attributions across them in Attribution.feature_attributions. For Vertex AI-provided Tensorflow images (both 1.x and 2.x), the shape of each baseline must match the shape of the input tensor. If a scalar is provided, we broadcast to the same shape as the input tensor. For custom images, the element of the baselines must be in the same format as the feature's input in the instance[]. The schema of any single instance may be specified via Endpoint's DeployedModels' Model's PredictSchemata's instance_schema_uri.
        """
        return pulumi.get(self, "input_baselines")

    @input_baselines.setter
    def input_baselines(self, value: Optional[pulumi.Input[Sequence[Any]]]):
        pulumi.set(self, "input_baselines", value)

    @property
    @pulumi.getter(name="inputTensorName")
    def input_tensor_name(self) -> Optional[pulumi.Input[str]]:
        """
        Name of the input tensor for this feature. Required and is only applicable to Vertex AI-provided images for Tensorflow.
        """
        return pulumi.get(self, "input_tensor_name")

    @input_tensor_name.setter
    def input_tensor_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "input_tensor_name", value)

    @property
    @pulumi.getter
    def modality(self) -> Optional[pulumi.Input[str]]:
        """
        Modality of the feature. Valid values are: numeric, image. Defaults to numeric.
        """
        return pulumi.get(self, "modality")

    @modality.setter
    def modality(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "modality", value)

    @property
    @pulumi.getter
    def visualization(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationArgs']]:
        """
        Visualization configurations for image explanation.
        """
        return pulumi.get(self, "visualization")

    @visualization.setter
    def visualization(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataVisualizationArgs']]):
        pulumi.set(self, "visualization", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataArgs:
    def __init__(__self__, *,
                 display_name_mapping_key: Optional[pulumi.Input[str]] = None,
                 index_display_name_mapping: Optional[Any] = None,
                 output_tensor_name: Optional[pulumi.Input[str]] = None):
        """
        Metadata of the prediction output to be explained.
        :param pulumi.Input[str] display_name_mapping_key: Specify a field name in the prediction to look for the display name. Use this if the prediction contains the display names for the outputs. The display names in the prediction must have the same shape of the outputs, so that it can be located by Attribution.output_index for a specific output.
        :param Any index_display_name_mapping: Static mapping between the index and display name. Use this if the outputs are a deterministic n-dimensional array, e.g. a list of scores of all the classes in a pre-defined order for a multi-classification Model. It's not feasible if the outputs are non-deterministic, e.g. the Model produces top-k classes or sort the outputs by their values. The shape of the value must be an n-dimensional array of strings. The number of dimensions must match that of the outputs to be explained. The Attribution.output_display_name is populated by locating in the mapping with Attribution.output_index.
        :param pulumi.Input[str] output_tensor_name: Name of the output tensor. Required and is only applicable to Vertex AI provided images for Tensorflow.
        """
        if display_name_mapping_key is not None:
            pulumi.set(__self__, "display_name_mapping_key", display_name_mapping_key)
        if index_display_name_mapping is not None:
            pulumi.set(__self__, "index_display_name_mapping", index_display_name_mapping)
        if output_tensor_name is not None:
            pulumi.set(__self__, "output_tensor_name", output_tensor_name)

    @property
    @pulumi.getter(name="displayNameMappingKey")
    def display_name_mapping_key(self) -> Optional[pulumi.Input[str]]:
        """
        Specify a field name in the prediction to look for the display name. Use this if the prediction contains the display names for the outputs. The display names in the prediction must have the same shape of the outputs, so that it can be located by Attribution.output_index for a specific output.
        """
        return pulumi.get(self, "display_name_mapping_key")

    @display_name_mapping_key.setter
    def display_name_mapping_key(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "display_name_mapping_key", value)

    @property
    @pulumi.getter(name="indexDisplayNameMapping")
    def index_display_name_mapping(self) -> Optional[Any]:
        """
        Static mapping between the index and display name. Use this if the outputs are a deterministic n-dimensional array, e.g. a list of scores of all the classes in a pre-defined order for a multi-classification Model. It's not feasible if the outputs are non-deterministic, e.g. the Model produces top-k classes or sort the outputs by their values. The shape of the value must be an n-dimensional array of strings. The number of dimensions must match that of the outputs to be explained. The Attribution.output_display_name is populated by locating in the mapping with Attribution.output_index.
        """
        return pulumi.get(self, "index_display_name_mapping")

    @index_display_name_mapping.setter
    def index_display_name_mapping(self, value: Optional[Any]):
        pulumi.set(self, "index_display_name_mapping", value)

    @property
    @pulumi.getter(name="outputTensorName")
    def output_tensor_name(self) -> Optional[pulumi.Input[str]]:
        """
        Name of the output tensor. Required and is only applicable to Vertex AI provided images for Tensorflow.
        """
        return pulumi.get(self, "output_tensor_name")

    @output_tensor_name.setter
    def output_tensor_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "output_tensor_name", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ExplanationMetadataArgs:
    def __init__(__self__, *,
                 inputs: pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataArgs']]],
                 outputs: pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataArgs']]],
                 feature_attributions_schema_uri: Optional[pulumi.Input[str]] = None,
                 latent_space_source: Optional[pulumi.Input[str]] = None):
        """
        Metadata describing the Model's input and output for explanation.
        :param pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataArgs']]] inputs: Map from feature names to feature input metadata. Keys are the name of the features. Values are the specification of the feature. An empty InputMetadata is valid. It describes a text feature which has the name specified as the key in ExplanationMetadata.inputs. The baseline of the empty feature is chosen by Vertex AI. For Vertex AI-provided Tensorflow images, the key can be any friendly name of the feature. Once specified, featureAttributions are keyed by this key (if not grouped with another feature). For custom images, the key must match with the key in instance.
        :param pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataArgs']]] outputs: Map from output names to output metadata. For Vertex AI-provided Tensorflow images, keys can be any user defined string that consists of any UTF-8 characters. For custom images, keys are the name of the output field in the prediction to be explained. Currently only one key is allowed.
        :param pulumi.Input[str] feature_attributions_schema_uri: Points to a YAML file stored on Google Cloud Storage describing the format of the feature attributions. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML tabular Models always have this field populated by Vertex AI. Note: The URI given on output may be different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        :param pulumi.Input[str] latent_space_source: Name of the source to generate embeddings for example based explanations.
        """
        pulumi.set(__self__, "inputs", inputs)
        pulumi.set(__self__, "outputs", outputs)
        if feature_attributions_schema_uri is not None:
            pulumi.set(__self__, "feature_attributions_schema_uri", feature_attributions_schema_uri)
        if latent_space_source is not None:
            pulumi.set(__self__, "latent_space_source", latent_space_source)

    @property
    @pulumi.getter
    def inputs(self) -> pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataArgs']]]:
        """
        Map from feature names to feature input metadata. Keys are the name of the features. Values are the specification of the feature. An empty InputMetadata is valid. It describes a text feature which has the name specified as the key in ExplanationMetadata.inputs. The baseline of the empty feature is chosen by Vertex AI. For Vertex AI-provided Tensorflow images, the key can be any friendly name of the feature. Once specified, featureAttributions are keyed by this key (if not grouped with another feature). For custom images, the key must match with the key in instance.
        """
        return pulumi.get(self, "inputs")

    @inputs.setter
    def inputs(self, value: pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataInputMetadataArgs']]]):
        pulumi.set(self, "inputs", value)

    @property
    @pulumi.getter
    def outputs(self) -> pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataArgs']]]:
        """
        Map from output names to output metadata. For Vertex AI-provided Tensorflow images, keys can be any user defined string that consists of any UTF-8 characters. For custom images, keys are the name of the output field in the prediction to be explained. Currently only one key is allowed.
        """
        return pulumi.get(self, "outputs")

    @outputs.setter
    def outputs(self, value: pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataOutputMetadataArgs']]]):
        pulumi.set(self, "outputs", value)

    @property
    @pulumi.getter(name="featureAttributionsSchemaUri")
    def feature_attributions_schema_uri(self) -> Optional[pulumi.Input[str]]:
        """
        Points to a YAML file stored on Google Cloud Storage describing the format of the feature attributions. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML tabular Models always have this field populated by Vertex AI. Note: The URI given on output may be different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        """
        return pulumi.get(self, "feature_attributions_schema_uri")

    @feature_attributions_schema_uri.setter
    def feature_attributions_schema_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "feature_attributions_schema_uri", value)

    @property
    @pulumi.getter(name="latentSpaceSource")
    def latent_space_source(self) -> Optional[pulumi.Input[str]]:
        """
        Name of the source to generate embeddings for example based explanations.
        """
        return pulumi.get(self, "latent_space_source")

    @latent_space_source.setter
    def latent_space_source(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "latent_space_source", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ExplanationParametersArgs:
    def __init__(__self__, *,
                 examples: Optional[pulumi.Input['GoogleCloudAiplatformV1ExamplesArgs']] = None,
                 integrated_gradients_attribution: Optional[pulumi.Input['GoogleCloudAiplatformV1IntegratedGradientsAttributionArgs']] = None,
                 output_indices: Optional[pulumi.Input[Sequence[Any]]] = None,
                 sampled_shapley_attribution: Optional[pulumi.Input['GoogleCloudAiplatformV1SampledShapleyAttributionArgs']] = None,
                 top_k: Optional[pulumi.Input[int]] = None,
                 xrai_attribution: Optional[pulumi.Input['GoogleCloudAiplatformV1XraiAttributionArgs']] = None):
        """
        Parameters to configure explaining for Model's predictions.
        :param pulumi.Input['GoogleCloudAiplatformV1ExamplesArgs'] examples: Example-based explanations that returns the nearest neighbors from the provided dataset.
        :param pulumi.Input['GoogleCloudAiplatformV1IntegratedGradientsAttributionArgs'] integrated_gradients_attribution: An attribution method that computes Aumann-Shapley values taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1703.01365
        :param pulumi.Input[Sequence[Any]] output_indices: If populated, only returns attributions that have output_index contained in output_indices. It must be an ndarray of integers, with the same shape of the output it's explaining. If not populated, returns attributions for top_k indices of outputs. If neither top_k nor output_indices is populated, returns the argmax index of the outputs. Only applicable to Models that predict multiple outputs (e,g, multi-class Models that predict multiple classes).
        :param pulumi.Input['GoogleCloudAiplatformV1SampledShapleyAttributionArgs'] sampled_shapley_attribution: An attribution method that approximates Shapley values for features that contribute to the label being predicted. A sampling strategy is used to approximate the value rather than considering all subsets of features. Refer to this paper for model details: https://arxiv.org/abs/1306.4265.
        :param pulumi.Input[int] top_k: If populated, returns attributions for top K indices of outputs (defaults to 1). Only applies to Models that predicts more than one outputs (e,g, multi-class Models). When set to -1, returns explanations for all outputs.
        :param pulumi.Input['GoogleCloudAiplatformV1XraiAttributionArgs'] xrai_attribution: An attribution method that redistributes Integrated Gradients attribution to segmented regions, taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1906.02825 XRAI currently performs better on natural images, like a picture of a house or an animal. If the images are taken in artificial environments, like a lab or manufacturing line, or from diagnostic equipment, like x-rays or quality-control cameras, use Integrated Gradients instead.
        """
        if examples is not None:
            pulumi.set(__self__, "examples", examples)
        if integrated_gradients_attribution is not None:
            pulumi.set(__self__, "integrated_gradients_attribution", integrated_gradients_attribution)
        if output_indices is not None:
            pulumi.set(__self__, "output_indices", output_indices)
        if sampled_shapley_attribution is not None:
            pulumi.set(__self__, "sampled_shapley_attribution", sampled_shapley_attribution)
        if top_k is not None:
            pulumi.set(__self__, "top_k", top_k)
        if xrai_attribution is not None:
            pulumi.set(__self__, "xrai_attribution", xrai_attribution)

    @property
    @pulumi.getter
    def examples(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ExamplesArgs']]:
        """
        Example-based explanations that returns the nearest neighbors from the provided dataset.
        """
        return pulumi.get(self, "examples")

    @examples.setter
    def examples(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ExamplesArgs']]):
        pulumi.set(self, "examples", value)

    @property
    @pulumi.getter(name="integratedGradientsAttribution")
    def integrated_gradients_attribution(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1IntegratedGradientsAttributionArgs']]:
        """
        An attribution method that computes Aumann-Shapley values taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1703.01365
        """
        return pulumi.get(self, "integrated_gradients_attribution")

    @integrated_gradients_attribution.setter
    def integrated_gradients_attribution(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1IntegratedGradientsAttributionArgs']]):
        pulumi.set(self, "integrated_gradients_attribution", value)

    @property
    @pulumi.getter(name="outputIndices")
    def output_indices(self) -> Optional[pulumi.Input[Sequence[Any]]]:
        """
        If populated, only returns attributions that have output_index contained in output_indices. It must be an ndarray of integers, with the same shape of the output it's explaining. If not populated, returns attributions for top_k indices of outputs. If neither top_k nor output_indices is populated, returns the argmax index of the outputs. Only applicable to Models that predict multiple outputs (e,g, multi-class Models that predict multiple classes).
        """
        return pulumi.get(self, "output_indices")

    @output_indices.setter
    def output_indices(self, value: Optional[pulumi.Input[Sequence[Any]]]):
        pulumi.set(self, "output_indices", value)

    @property
    @pulumi.getter(name="sampledShapleyAttribution")
    def sampled_shapley_attribution(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1SampledShapleyAttributionArgs']]:
        """
        An attribution method that approximates Shapley values for features that contribute to the label being predicted. A sampling strategy is used to approximate the value rather than considering all subsets of features. Refer to this paper for model details: https://arxiv.org/abs/1306.4265.
        """
        return pulumi.get(self, "sampled_shapley_attribution")

    @sampled_shapley_attribution.setter
    def sampled_shapley_attribution(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1SampledShapleyAttributionArgs']]):
        pulumi.set(self, "sampled_shapley_attribution", value)

    @property
    @pulumi.getter(name="topK")
    def top_k(self) -> Optional[pulumi.Input[int]]:
        """
        If populated, returns attributions for top K indices of outputs (defaults to 1). Only applies to Models that predicts more than one outputs (e,g, multi-class Models). When set to -1, returns explanations for all outputs.
        """
        return pulumi.get(self, "top_k")

    @top_k.setter
    def top_k(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "top_k", value)

    @property
    @pulumi.getter(name="xraiAttribution")
    def xrai_attribution(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1XraiAttributionArgs']]:
        """
        An attribution method that redistributes Integrated Gradients attribution to segmented regions, taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1906.02825 XRAI currently performs better on natural images, like a picture of a house or an animal. If the images are taken in artificial environments, like a lab or manufacturing line, or from diagnostic equipment, like x-rays or quality-control cameras, use Integrated Gradients instead.
        """
        return pulumi.get(self, "xrai_attribution")

    @xrai_attribution.setter
    def xrai_attribution(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1XraiAttributionArgs']]):
        pulumi.set(self, "xrai_attribution", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ExplanationSpecArgs:
    def __init__(__self__, *,
                 parameters: pulumi.Input['GoogleCloudAiplatformV1ExplanationParametersArgs'],
                 metadata: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataArgs']] = None):
        """
        Specification of Model explanation.
        :param pulumi.Input['GoogleCloudAiplatformV1ExplanationParametersArgs'] parameters: Parameters that configure explaining of the Model's predictions.
        :param pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataArgs'] metadata: Optional. Metadata describing the Model's input and output for explanation.
        """
        pulumi.set(__self__, "parameters", parameters)
        if metadata is not None:
            pulumi.set(__self__, "metadata", metadata)

    @property
    @pulumi.getter
    def parameters(self) -> pulumi.Input['GoogleCloudAiplatformV1ExplanationParametersArgs']:
        """
        Parameters that configure explaining of the Model's predictions.
        """
        return pulumi.get(self, "parameters")

    @parameters.setter
    def parameters(self, value: pulumi.Input['GoogleCloudAiplatformV1ExplanationParametersArgs']):
        pulumi.set(self, "parameters", value)

    @property
    @pulumi.getter
    def metadata(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataArgs']]:
        """
        Optional. Metadata describing the Model's input and output for explanation.
        """
        return pulumi.get(self, "metadata")

    @metadata.setter
    def metadata(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationMetadataArgs']]):
        pulumi.set(self, "metadata", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeatureGroupBigQueryArgs:
    def __init__(__self__, *,
                 big_query_source: pulumi.Input['GoogleCloudAiplatformV1BigQuerySourceArgs'],
                 entity_id_columns: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        Input source type for BigQuery Tables and Views.
        :param pulumi.Input['GoogleCloudAiplatformV1BigQuerySourceArgs'] big_query_source: Immutable. The BigQuery source URI that points to either a BigQuery Table or View.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] entity_id_columns: Optional. Columns to construct entity_id / row keys. Currently only supports 1 entity_id_column. If not provided defaults to `entity_id`.
        """
        pulumi.set(__self__, "big_query_source", big_query_source)
        if entity_id_columns is not None:
            pulumi.set(__self__, "entity_id_columns", entity_id_columns)

    @property
    @pulumi.getter(name="bigQuerySource")
    def big_query_source(self) -> pulumi.Input['GoogleCloudAiplatformV1BigQuerySourceArgs']:
        """
        Immutable. The BigQuery source URI that points to either a BigQuery Table or View.
        """
        return pulumi.get(self, "big_query_source")

    @big_query_source.setter
    def big_query_source(self, value: pulumi.Input['GoogleCloudAiplatformV1BigQuerySourceArgs']):
        pulumi.set(self, "big_query_source", value)

    @property
    @pulumi.getter(name="entityIdColumns")
    def entity_id_columns(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Optional. Columns to construct entity_id / row keys. Currently only supports 1 entity_id_column. If not provided defaults to `entity_id`.
        """
        return pulumi.get(self, "entity_id_columns")

    @entity_id_columns.setter
    def entity_id_columns(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "entity_id_columns", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeatureNoiseSigmaNoiseSigmaForFeatureArgs:
    def __init__(__self__, *,
                 name: Optional[pulumi.Input[str]] = None,
                 sigma: Optional[pulumi.Input[float]] = None):
        """
        Noise sigma for a single feature.
        :param pulumi.Input[str] name: The name of the input feature for which noise sigma is provided. The features are defined in explanation metadata inputs.
        :param pulumi.Input[float] sigma: This represents the standard deviation of the Gaussian kernel that will be used to add noise to the feature prior to computing gradients. Similar to noise_sigma but represents the noise added to the current feature. Defaults to 0.1.
        """
        if name is not None:
            pulumi.set(__self__, "name", name)
        if sigma is not None:
            pulumi.set(__self__, "sigma", sigma)

    @property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[str]]:
        """
        The name of the input feature for which noise sigma is provided. The features are defined in explanation metadata inputs.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "name", value)

    @property
    @pulumi.getter
    def sigma(self) -> Optional[pulumi.Input[float]]:
        """
        This represents the standard deviation of the Gaussian kernel that will be used to add noise to the feature prior to computing gradients. Similar to noise_sigma but represents the noise added to the current feature. Defaults to 0.1.
        """
        return pulumi.get(self, "sigma")

    @sigma.setter
    def sigma(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "sigma", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeatureNoiseSigmaArgs:
    def __init__(__self__, *,
                 noise_sigma: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1FeatureNoiseSigmaNoiseSigmaForFeatureArgs']]]] = None):
        """
        Noise sigma by features. Noise sigma represents the standard deviation of the gaussian kernel that will be used to add noise to interpolated inputs prior to computing gradients.
        :param pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1FeatureNoiseSigmaNoiseSigmaForFeatureArgs']]] noise_sigma: Noise sigma per feature. No noise is added to features that are not set.
        """
        if noise_sigma is not None:
            pulumi.set(__self__, "noise_sigma", noise_sigma)

    @property
    @pulumi.getter(name="noiseSigma")
    def noise_sigma(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1FeatureNoiseSigmaNoiseSigmaForFeatureArgs']]]]:
        """
        Noise sigma per feature. No noise is added to features that are not set.
        """
        return pulumi.get(self, "noise_sigma")

    @noise_sigma.setter
    def noise_sigma(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1FeatureNoiseSigmaNoiseSigmaForFeatureArgs']]]]):
        pulumi.set(self, "noise_sigma", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingArgs:
    def __init__(__self__, *,
                 max_node_count: pulumi.Input[int],
                 min_node_count: pulumi.Input[int],
                 cpu_utilization_target: Optional[pulumi.Input[int]] = None):
        """
        :param pulumi.Input[int] max_node_count: The maximum number of nodes to scale up to. Must be greater than or equal to min_node_count, and less than or equal to 10 times of 'min_node_count'.
        :param pulumi.Input[int] min_node_count: The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        :param pulumi.Input[int] cpu_utilization_target: Optional. A percentage of the cluster's CPU capacity. Can be from 10% to 80%. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set will default to 50%.
        """
        pulumi.set(__self__, "max_node_count", max_node_count)
        pulumi.set(__self__, "min_node_count", min_node_count)
        if cpu_utilization_target is not None:
            pulumi.set(__self__, "cpu_utilization_target", cpu_utilization_target)

    @property
    @pulumi.getter(name="maxNodeCount")
    def max_node_count(self) -> pulumi.Input[int]:
        """
        The maximum number of nodes to scale up to. Must be greater than or equal to min_node_count, and less than or equal to 10 times of 'min_node_count'.
        """
        return pulumi.get(self, "max_node_count")

    @max_node_count.setter
    def max_node_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "max_node_count", value)

    @property
    @pulumi.getter(name="minNodeCount")
    def min_node_count(self) -> pulumi.Input[int]:
        """
        The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
        return pulumi.get(self, "min_node_count")

    @min_node_count.setter
    def min_node_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "min_node_count", value)

    @property
    @pulumi.getter(name="cpuUtilizationTarget")
    def cpu_utilization_target(self) -> Optional[pulumi.Input[int]]:
        """
        Optional. A percentage of the cluster's CPU capacity. Can be from 10% to 80%. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set will default to 50%.
        """
        return pulumi.get(self, "cpu_utilization_target")

    @cpu_utilization_target.setter
    def cpu_utilization_target(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "cpu_utilization_target", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeatureOnlineStoreBigtableArgs:
    def __init__(__self__, *,
                 auto_scaling: pulumi.Input['GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingArgs']):
        """
        :param pulumi.Input['GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingArgs'] auto_scaling: Autoscaling config applied to Bigtable Instance.
        """
        pulumi.set(__self__, "auto_scaling", auto_scaling)

    @property
    @pulumi.getter(name="autoScaling")
    def auto_scaling(self) -> pulumi.Input['GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingArgs']:
        """
        Autoscaling config applied to Bigtable Instance.
        """
        return pulumi.get(self, "auto_scaling")

    @auto_scaling.setter
    def auto_scaling(self, value: pulumi.Input['GoogleCloudAiplatformV1FeatureOnlineStoreBigtableAutoScalingArgs']):
        pulumi.set(self, "auto_scaling", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeatureViewBigQuerySourceArgs:
    def __init__(__self__, *,
                 entity_id_columns: pulumi.Input[Sequence[pulumi.Input[str]]],
                 uri: pulumi.Input[str]):
        """
        :param pulumi.Input[Sequence[pulumi.Input[str]]] entity_id_columns: Columns to construct entity_id / row keys. Start by supporting 1 only.
        :param pulumi.Input[str] uri: The BigQuery view URI that will be materialized on each sync trigger based on FeatureView.SyncConfig.
        """
        pulumi.set(__self__, "entity_id_columns", entity_id_columns)
        pulumi.set(__self__, "uri", uri)

    @property
    @pulumi.getter(name="entityIdColumns")
    def entity_id_columns(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Columns to construct entity_id / row keys. Start by supporting 1 only.
        """
        return pulumi.get(self, "entity_id_columns")

    @entity_id_columns.setter
    def entity_id_columns(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "entity_id_columns", value)

    @property
    @pulumi.getter
    def uri(self) -> pulumi.Input[str]:
        """
        The BigQuery view URI that will be materialized on each sync trigger based on FeatureView.SyncConfig.
        """
        return pulumi.get(self, "uri")

    @uri.setter
    def uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "uri", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupArgs:
    def __init__(__self__, *,
                 feature_group_id: pulumi.Input[str],
                 feature_ids: pulumi.Input[Sequence[pulumi.Input[str]]]):
        """
        Features belonging to a single feature group that will be synced to Online Store.
        :param pulumi.Input[str] feature_group_id: Identifier of the feature group.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] feature_ids: Identifiers of features under the feature group.
        """
        pulumi.set(__self__, "feature_group_id", feature_group_id)
        pulumi.set(__self__, "feature_ids", feature_ids)

    @property
    @pulumi.getter(name="featureGroupId")
    def feature_group_id(self) -> pulumi.Input[str]:
        """
        Identifier of the feature group.
        """
        return pulumi.get(self, "feature_group_id")

    @feature_group_id.setter
    def feature_group_id(self, value: pulumi.Input[str]):
        pulumi.set(self, "feature_group_id", value)

    @property
    @pulumi.getter(name="featureIds")
    def feature_ids(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Identifiers of features under the feature group.
        """
        return pulumi.get(self, "feature_ids")

    @feature_ids.setter
    def feature_ids(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "feature_ids", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceArgs:
    def __init__(__self__, *,
                 feature_groups: pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupArgs']]]):
        """
        A Feature Registry source for features that need to be synced to Online Store.
        :param pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupArgs']]] feature_groups: List of features that need to be synced to Online Store.
        """
        pulumi.set(__self__, "feature_groups", feature_groups)

    @property
    @pulumi.getter(name="featureGroups")
    def feature_groups(self) -> pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupArgs']]]:
        """
        List of features that need to be synced to Online Store.
        """
        return pulumi.get(self, "feature_groups")

    @feature_groups.setter
    def feature_groups(self, value: pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1FeatureViewFeatureRegistrySourceFeatureGroupArgs']]]):
        pulumi.set(self, "feature_groups", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeatureViewSyncConfigArgs:
    def __init__(__self__, *,
                 cron: Optional[pulumi.Input[str]] = None):
        """
        :param pulumi.Input[str] cron: Cron schedule (https://en.wikipedia.org/wiki/Cron) to launch scheduled runs. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or "TZ=${IANA_TIME_ZONE}". The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, "CRON_TZ=America/New_York 1 * * * *", or "TZ=America/New_York 1 * * * *".
        """
        if cron is not None:
            pulumi.set(__self__, "cron", cron)

    @property
    @pulumi.getter
    def cron(self) -> Optional[pulumi.Input[str]]:
        """
        Cron schedule (https://en.wikipedia.org/wiki/Cron) to launch scheduled runs. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or "TZ=${IANA_TIME_ZONE}". The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, "CRON_TZ=America/New_York 1 * * * *", or "TZ=America/New_York 1 * * * *".
        """
        return pulumi.get(self, "cron")

    @cron.setter
    def cron(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "cron", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisArgs:
    def __init__(__self__, *,
                 anomaly_detection_baseline: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisAnomalyDetectionBaseline']] = None,
                 state: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisState']] = None):
        """
        Configuration of the Featurestore's ImportFeature Analysis Based Monitoring. This type of analysis generates statistics for values of each Feature imported by every ImportFeatureValues operation.
        :param pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisAnomalyDetectionBaseline'] anomaly_detection_baseline: The baseline used to do anomaly detection for the statistics generated by import features analysis.
        :param pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisState'] state: Whether to enable / disable / inherite default hebavior for import features analysis.
        """
        if anomaly_detection_baseline is not None:
            pulumi.set(__self__, "anomaly_detection_baseline", anomaly_detection_baseline)
        if state is not None:
            pulumi.set(__self__, "state", state)

    @property
    @pulumi.getter(name="anomalyDetectionBaseline")
    def anomaly_detection_baseline(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisAnomalyDetectionBaseline']]:
        """
        The baseline used to do anomaly detection for the statistics generated by import features analysis.
        """
        return pulumi.get(self, "anomaly_detection_baseline")

    @anomaly_detection_baseline.setter
    def anomaly_detection_baseline(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisAnomalyDetectionBaseline']]):
        pulumi.set(self, "anomaly_detection_baseline", value)

    @property
    @pulumi.getter
    def state(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisState']]:
        """
        Whether to enable / disable / inherite default hebavior for import features analysis.
        """
        return pulumi.get(self, "state")

    @state.setter
    def state(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisState']]):
        pulumi.set(self, "state", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisArgs:
    def __init__(__self__, *,
                 disabled: Optional[pulumi.Input[bool]] = None,
                 monitoring_interval_days: Optional[pulumi.Input[int]] = None,
                 staleness_days: Optional[pulumi.Input[int]] = None):
        """
        Configuration of the Featurestore's Snapshot Analysis Based Monitoring. This type of analysis generates statistics for each Feature based on a snapshot of the latest feature value of each entities every monitoring_interval.
        :param pulumi.Input[bool] disabled: The monitoring schedule for snapshot analysis. For EntityType-level config: unset / disabled = true indicates disabled by default for Features under it; otherwise by default enable snapshot analysis monitoring with monitoring_interval for Features under it. Feature-level config: disabled = true indicates disabled regardless of the EntityType-level config; unset monitoring_interval indicates going with EntityType-level config; otherwise run snapshot analysis monitoring with monitoring_interval regardless of the EntityType-level config. Explicitly Disable the snapshot analysis based monitoring.
        :param pulumi.Input[int] monitoring_interval_days: Configuration of the snapshot analysis based monitoring pipeline running interval. The value indicates number of days.
        :param pulumi.Input[int] staleness_days: Customized export features time window for snapshot analysis. Unit is one day. Default value is 3 weeks. Minimum value is 1 day. Maximum value is 4000 days.
        """
        if disabled is not None:
            pulumi.set(__self__, "disabled", disabled)
        if monitoring_interval_days is not None:
            pulumi.set(__self__, "monitoring_interval_days", monitoring_interval_days)
        if staleness_days is not None:
            pulumi.set(__self__, "staleness_days", staleness_days)

    @property
    @pulumi.getter
    def disabled(self) -> Optional[pulumi.Input[bool]]:
        """
        The monitoring schedule for snapshot analysis. For EntityType-level config: unset / disabled = true indicates disabled by default for Features under it; otherwise by default enable snapshot analysis monitoring with monitoring_interval for Features under it. Feature-level config: disabled = true indicates disabled regardless of the EntityType-level config; unset monitoring_interval indicates going with EntityType-level config; otherwise run snapshot analysis monitoring with monitoring_interval regardless of the EntityType-level config. Explicitly Disable the snapshot analysis based monitoring.
        """
        return pulumi.get(self, "disabled")

    @disabled.setter
    def disabled(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "disabled", value)

    @property
    @pulumi.getter(name="monitoringIntervalDays")
    def monitoring_interval_days(self) -> Optional[pulumi.Input[int]]:
        """
        Configuration of the snapshot analysis based monitoring pipeline running interval. The value indicates number of days.
        """
        return pulumi.get(self, "monitoring_interval_days")

    @monitoring_interval_days.setter
    def monitoring_interval_days(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "monitoring_interval_days", value)

    @property
    @pulumi.getter(name="stalenessDays")
    def staleness_days(self) -> Optional[pulumi.Input[int]]:
        """
        Customized export features time window for snapshot analysis. Unit is one day. Default value is 3 weeks. Minimum value is 1 day. Maximum value is 4000 days.
        """
        return pulumi.get(self, "staleness_days")

    @staleness_days.setter
    def staleness_days(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "staleness_days", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigArgs:
    def __init__(__self__, *,
                 value: Optional[pulumi.Input[float]] = None):
        """
        The config for Featurestore Monitoring threshold.
        :param pulumi.Input[float] value: Specify a threshold value that can trigger the alert. 1. For categorical feature, the distribution distance is calculated by L-inifinity norm. 2. For numerical feature, the distribution distance is calculated by JensenShannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature.
        """
        if value is not None:
            pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter
    def value(self) -> Optional[pulumi.Input[float]]:
        """
        Specify a threshold value that can trigger the alert. 1. For categorical feature, the distribution distance is calculated by L-inifinity norm. 2. For numerical feature, the distribution distance is calculated by JensenShannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature.
        """
        return pulumi.get(self, "value")

    @value.setter
    def value(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "value", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeaturestoreMonitoringConfigArgs:
    def __init__(__self__, *,
                 categorical_threshold_config: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigArgs']] = None,
                 import_features_analysis: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisArgs']] = None,
                 numerical_threshold_config: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigArgs']] = None,
                 snapshot_analysis: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisArgs']] = None):
        """
        Configuration of how features in Featurestore are monitored.
        :param pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigArgs'] categorical_threshold_config: Threshold for categorical features of anomaly detection. This is shared by all types of Featurestore Monitoring for categorical features (i.e. Features with type (Feature.ValueType) BOOL or STRING).
        :param pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisArgs'] import_features_analysis: The config for ImportFeatures Analysis Based Feature Monitoring.
        :param pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigArgs'] numerical_threshold_config: Threshold for numerical features of anomaly detection. This is shared by all objectives of Featurestore Monitoring for numerical features (i.e. Features with type (Feature.ValueType) DOUBLE or INT64).
        :param pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisArgs'] snapshot_analysis: The config for Snapshot Analysis Based Feature Monitoring.
        """
        if categorical_threshold_config is not None:
            pulumi.set(__self__, "categorical_threshold_config", categorical_threshold_config)
        if import_features_analysis is not None:
            pulumi.set(__self__, "import_features_analysis", import_features_analysis)
        if numerical_threshold_config is not None:
            pulumi.set(__self__, "numerical_threshold_config", numerical_threshold_config)
        if snapshot_analysis is not None:
            pulumi.set(__self__, "snapshot_analysis", snapshot_analysis)

    @property
    @pulumi.getter(name="categoricalThresholdConfig")
    def categorical_threshold_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigArgs']]:
        """
        Threshold for categorical features of anomaly detection. This is shared by all types of Featurestore Monitoring for categorical features (i.e. Features with type (Feature.ValueType) BOOL or STRING).
        """
        return pulumi.get(self, "categorical_threshold_config")

    @categorical_threshold_config.setter
    def categorical_threshold_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigArgs']]):
        pulumi.set(self, "categorical_threshold_config", value)

    @property
    @pulumi.getter(name="importFeaturesAnalysis")
    def import_features_analysis(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisArgs']]:
        """
        The config for ImportFeatures Analysis Based Feature Monitoring.
        """
        return pulumi.get(self, "import_features_analysis")

    @import_features_analysis.setter
    def import_features_analysis(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigImportFeaturesAnalysisArgs']]):
        pulumi.set(self, "import_features_analysis", value)

    @property
    @pulumi.getter(name="numericalThresholdConfig")
    def numerical_threshold_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigArgs']]:
        """
        Threshold for numerical features of anomaly detection. This is shared by all objectives of Featurestore Monitoring for numerical features (i.e. Features with type (Feature.ValueType) DOUBLE or INT64).
        """
        return pulumi.get(self, "numerical_threshold_config")

    @numerical_threshold_config.setter
    def numerical_threshold_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigThresholdConfigArgs']]):
        pulumi.set(self, "numerical_threshold_config", value)

    @property
    @pulumi.getter(name="snapshotAnalysis")
    def snapshot_analysis(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisArgs']]:
        """
        The config for Snapshot Analysis Based Feature Monitoring.
        """
        return pulumi.get(self, "snapshot_analysis")

    @snapshot_analysis.setter
    def snapshot_analysis(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreMonitoringConfigSnapshotAnalysisArgs']]):
        pulumi.set(self, "snapshot_analysis", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingArgs:
    def __init__(__self__, *,
                 min_node_count: pulumi.Input[int],
                 cpu_utilization_target: Optional[pulumi.Input[int]] = None,
                 max_node_count: Optional[pulumi.Input[int]] = None):
        """
        Online serving scaling configuration. If min_node_count and max_node_count are set to the same value, the cluster will be configured with the fixed number of node (no auto-scaling).
        :param pulumi.Input[int] min_node_count: The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        :param pulumi.Input[int] cpu_utilization_target: Optional. The cpu utilization that the Autoscaler should be trying to achieve. This number is on a scale from 0 (no utilization) to 100 (total utilization), and is limited between 10 and 80. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set or set to 0, default to 50.
        :param pulumi.Input[int] max_node_count: The maximum number of nodes to scale up to. Must be greater than min_node_count, and less than or equal to 10 times of 'min_node_count'.
        """
        pulumi.set(__self__, "min_node_count", min_node_count)
        if cpu_utilization_target is not None:
            pulumi.set(__self__, "cpu_utilization_target", cpu_utilization_target)
        if max_node_count is not None:
            pulumi.set(__self__, "max_node_count", max_node_count)

    @property
    @pulumi.getter(name="minNodeCount")
    def min_node_count(self) -> pulumi.Input[int]:
        """
        The minimum number of nodes to scale down to. Must be greater than or equal to 1.
        """
        return pulumi.get(self, "min_node_count")

    @min_node_count.setter
    def min_node_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "min_node_count", value)

    @property
    @pulumi.getter(name="cpuUtilizationTarget")
    def cpu_utilization_target(self) -> Optional[pulumi.Input[int]]:
        """
        Optional. The cpu utilization that the Autoscaler should be trying to achieve. This number is on a scale from 0 (no utilization) to 100 (total utilization), and is limited between 10 and 80. When a cluster's CPU utilization exceeds the target that you have set, Bigtable immediately adds nodes to the cluster. When CPU utilization is substantially lower than the target, Bigtable removes nodes. If not set or set to 0, default to 50.
        """
        return pulumi.get(self, "cpu_utilization_target")

    @cpu_utilization_target.setter
    def cpu_utilization_target(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "cpu_utilization_target", value)

    @property
    @pulumi.getter(name="maxNodeCount")
    def max_node_count(self) -> Optional[pulumi.Input[int]]:
        """
        The maximum number of nodes to scale up to. Must be greater than min_node_count, and less than or equal to 10 times of 'min_node_count'.
        """
        return pulumi.get(self, "max_node_count")

    @max_node_count.setter
    def max_node_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_node_count", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigArgs:
    def __init__(__self__, *,
                 fixed_node_count: Optional[pulumi.Input[int]] = None,
                 scaling: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingArgs']] = None):
        """
        OnlineServingConfig specifies the details for provisioning online serving resources.
        :param pulumi.Input[int] fixed_node_count: The number of nodes for the online store. The number of nodes doesn't scale automatically, but you can manually update the number of nodes. If set to 0, the featurestore will not have an online store and cannot be used for online serving.
        :param pulumi.Input['GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingArgs'] scaling: Online serving scaling configuration. Only one of `fixed_node_count` and `scaling` can be set. Setting one will reset the other.
        """
        if fixed_node_count is not None:
            pulumi.set(__self__, "fixed_node_count", fixed_node_count)
        if scaling is not None:
            pulumi.set(__self__, "scaling", scaling)

    @property
    @pulumi.getter(name="fixedNodeCount")
    def fixed_node_count(self) -> Optional[pulumi.Input[int]]:
        """
        The number of nodes for the online store. The number of nodes doesn't scale automatically, but you can manually update the number of nodes. If set to 0, the featurestore will not have an online store and cannot be used for online serving.
        """
        return pulumi.get(self, "fixed_node_count")

    @fixed_node_count.setter
    def fixed_node_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "fixed_node_count", value)

    @property
    @pulumi.getter
    def scaling(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingArgs']]:
        """
        Online serving scaling configuration. Only one of `fixed_node_count` and `scaling` can be set. Setting one will reset the other.
        """
        return pulumi.get(self, "scaling")

    @scaling.setter
    def scaling(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1FeaturestoreOnlineServingConfigScalingArgs']]):
        pulumi.set(self, "scaling", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FilterSplitArgs:
    def __init__(__self__, *,
                 test_filter: pulumi.Input[str],
                 training_filter: pulumi.Input[str],
                 validation_filter: pulumi.Input[str]):
        """
        Assigns input data to training, validation, and test sets based on the given filters, data pieces not matched by any filter are ignored. Currently only supported for Datasets containing DataItems. If any of the filters in this message are to match nothing, then they can be set as '-' (the minus sign). Supported only for unstructured Datasets. 
        :param pulumi.Input[str] test_filter: A filter on DataItems of the Dataset. DataItems that match this filter are used to test the Model. A filter with same syntax as the one used in DatasetService.ListDataItems may be used. If a single DataItem is matched by more than one of the FilterSplit filters, then it is assigned to the first set that applies to it in the training, validation, test order.
        :param pulumi.Input[str] training_filter: A filter on DataItems of the Dataset. DataItems that match this filter are used to train the Model. A filter with same syntax as the one used in DatasetService.ListDataItems may be used. If a single DataItem is matched by more than one of the FilterSplit filters, then it is assigned to the first set that applies to it in the training, validation, test order.
        :param pulumi.Input[str] validation_filter: A filter on DataItems of the Dataset. DataItems that match this filter are used to validate the Model. A filter with same syntax as the one used in DatasetService.ListDataItems may be used. If a single DataItem is matched by more than one of the FilterSplit filters, then it is assigned to the first set that applies to it in the training, validation, test order.
        """
        pulumi.set(__self__, "test_filter", test_filter)
        pulumi.set(__self__, "training_filter", training_filter)
        pulumi.set(__self__, "validation_filter", validation_filter)

    @property
    @pulumi.getter(name="testFilter")
    def test_filter(self) -> pulumi.Input[str]:
        """
        A filter on DataItems of the Dataset. DataItems that match this filter are used to test the Model. A filter with same syntax as the one used in DatasetService.ListDataItems may be used. If a single DataItem is matched by more than one of the FilterSplit filters, then it is assigned to the first set that applies to it in the training, validation, test order.
        """
        return pulumi.get(self, "test_filter")

    @test_filter.setter
    def test_filter(self, value: pulumi.Input[str]):
        pulumi.set(self, "test_filter", value)

    @property
    @pulumi.getter(name="trainingFilter")
    def training_filter(self) -> pulumi.Input[str]:
        """
        A filter on DataItems of the Dataset. DataItems that match this filter are used to train the Model. A filter with same syntax as the one used in DatasetService.ListDataItems may be used. If a single DataItem is matched by more than one of the FilterSplit filters, then it is assigned to the first set that applies to it in the training, validation, test order.
        """
        return pulumi.get(self, "training_filter")

    @training_filter.setter
    def training_filter(self, value: pulumi.Input[str]):
        pulumi.set(self, "training_filter", value)

    @property
    @pulumi.getter(name="validationFilter")
    def validation_filter(self) -> pulumi.Input[str]:
        """
        A filter on DataItems of the Dataset. DataItems that match this filter are used to validate the Model. A filter with same syntax as the one used in DatasetService.ListDataItems may be used. If a single DataItem is matched by more than one of the FilterSplit filters, then it is assigned to the first set that applies to it in the training, validation, test order.
        """
        return pulumi.get(self, "validation_filter")

    @validation_filter.setter
    def validation_filter(self, value: pulumi.Input[str]):
        pulumi.set(self, "validation_filter", value)


@pulumi.input_type
class GoogleCloudAiplatformV1FractionSplitArgs:
    def __init__(__self__, *,
                 test_fraction: Optional[pulumi.Input[float]] = None,
                 training_fraction: Optional[pulumi.Input[float]] = None,
                 validation_fraction: Optional[pulumi.Input[float]] = None):
        """
        Assigns the input data to training, validation, and test sets as per the given fractions. Any of `training_fraction`, `validation_fraction` and `test_fraction` may optionally be provided, they must sum to up to 1. If the provided ones sum to less than 1, the remainder is assigned to sets as decided by Vertex AI. If none of the fractions are set, by default roughly 80% of data is used for training, 10% for validation, and 10% for test.
        :param pulumi.Input[float] test_fraction: The fraction of the input data that is to be used to evaluate the Model.
        :param pulumi.Input[float] training_fraction: The fraction of the input data that is to be used to train the Model.
        :param pulumi.Input[float] validation_fraction: The fraction of the input data that is to be used to validate the Model.
        """
        if test_fraction is not None:
            pulumi.set(__self__, "test_fraction", test_fraction)
        if training_fraction is not None:
            pulumi.set(__self__, "training_fraction", training_fraction)
        if validation_fraction is not None:
            pulumi.set(__self__, "validation_fraction", validation_fraction)

    @property
    @pulumi.getter(name="testFraction")
    def test_fraction(self) -> Optional[pulumi.Input[float]]:
        """
        The fraction of the input data that is to be used to evaluate the Model.
        """
        return pulumi.get(self, "test_fraction")

    @test_fraction.setter
    def test_fraction(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "test_fraction", value)

    @property
    @pulumi.getter(name="trainingFraction")
    def training_fraction(self) -> Optional[pulumi.Input[float]]:
        """
        The fraction of the input data that is to be used to train the Model.
        """
        return pulumi.get(self, "training_fraction")

    @training_fraction.setter
    def training_fraction(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "training_fraction", value)

    @property
    @pulumi.getter(name="validationFraction")
    def validation_fraction(self) -> Optional[pulumi.Input[float]]:
        """
        The fraction of the input data that is to be used to validate the Model.
        """
        return pulumi.get(self, "validation_fraction")

    @validation_fraction.setter
    def validation_fraction(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "validation_fraction", value)


@pulumi.input_type
class GoogleCloudAiplatformV1GcsDestinationArgs:
    def __init__(__self__, *,
                 output_uri_prefix: pulumi.Input[str]):
        """
        The Google Cloud Storage location where the output is to be written to.
        :param pulumi.Input[str] output_uri_prefix: Google Cloud Storage URI to output directory. If the uri doesn't end with '/', a '/' will be automatically appended. The directory is created if it doesn't exist.
        """
        pulumi.set(__self__, "output_uri_prefix", output_uri_prefix)

    @property
    @pulumi.getter(name="outputUriPrefix")
    def output_uri_prefix(self) -> pulumi.Input[str]:
        """
        Google Cloud Storage URI to output directory. If the uri doesn't end with '/', a '/' will be automatically appended. The directory is created if it doesn't exist.
        """
        return pulumi.get(self, "output_uri_prefix")

    @output_uri_prefix.setter
    def output_uri_prefix(self, value: pulumi.Input[str]):
        pulumi.set(self, "output_uri_prefix", value)


@pulumi.input_type
class GoogleCloudAiplatformV1GcsSourceArgs:
    def __init__(__self__, *,
                 uris: pulumi.Input[Sequence[pulumi.Input[str]]]):
        """
        The Google Cloud Storage location for the input content.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] uris: Google Cloud Storage URI(-s) to the input file(s). May contain wildcards. For more information on wildcards, see https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames.
        """
        pulumi.set(__self__, "uris", uris)

    @property
    @pulumi.getter
    def uris(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Google Cloud Storage URI(-s) to the input file(s). May contain wildcards. For more information on wildcards, see https://cloud.google.com/storage/docs/gsutil/addlhelp/WildcardNames.
        """
        return pulumi.get(self, "uris")

    @uris.setter
    def uris(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "uris", value)


@pulumi.input_type
class GoogleCloudAiplatformV1InputDataConfigArgs:
    def __init__(__self__, *,
                 dataset_id: pulumi.Input[str],
                 annotation_schema_uri: Optional[pulumi.Input[str]] = None,
                 annotations_filter: Optional[pulumi.Input[str]] = None,
                 bigquery_destination: Optional[pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs']] = None,
                 filter_split: Optional[pulumi.Input['GoogleCloudAiplatformV1FilterSplitArgs']] = None,
                 fraction_split: Optional[pulumi.Input['GoogleCloudAiplatformV1FractionSplitArgs']] = None,
                 gcs_destination: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs']] = None,
                 persist_ml_use_assignment: Optional[pulumi.Input[bool]] = None,
                 predefined_split: Optional[pulumi.Input['GoogleCloudAiplatformV1PredefinedSplitArgs']] = None,
                 saved_query_id: Optional[pulumi.Input[str]] = None,
                 stratified_split: Optional[pulumi.Input['GoogleCloudAiplatformV1StratifiedSplitArgs']] = None,
                 timestamp_split: Optional[pulumi.Input['GoogleCloudAiplatformV1TimestampSplitArgs']] = None):
        """
        Specifies Vertex AI owned input data to be used for training, and possibly evaluating, the Model.
        :param pulumi.Input[str] dataset_id: The ID of the Dataset in the same Project and Location which data will be used to train the Model. The Dataset must use schema compatible with Model being trained, and what is compatible should be described in the used TrainingPipeline's training_task_definition. For tabular Datasets, all their data is exported to training, to pick and choose from.
        :param pulumi.Input[str] annotation_schema_uri: Applicable only to custom training with Datasets that have DataItems and Annotations. Cloud Storage URI that points to a YAML file describing the annotation schema. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). The schema files that can be used here are found in gs://google-cloud-aiplatform/schema/dataset/annotation/ , note that the chosen schema must be consistent with metadata of the Dataset specified by dataset_id. Only Annotations that both match this schema and belong to DataItems not ignored by the split method are used in respectively training, validation or test role, depending on the role of the DataItem they are on. When used in conjunction with annotations_filter, the Annotations used for training are filtered by both annotations_filter and annotation_schema_uri.
        :param pulumi.Input[str] annotations_filter: Applicable only to Datasets that have DataItems and Annotations. A filter on Annotations of the Dataset. Only Annotations that both match this filter and belong to DataItems not ignored by the split method are used in respectively training, validation or test role, depending on the role of the DataItem they are on (for the auto-assigned that role is decided by Vertex AI). A filter with same syntax as the one used in ListAnnotations may be used, but note here it filters across all Annotations of the Dataset, and not just within a single DataItem.
        :param pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs'] bigquery_destination: Only applicable to custom training with tabular Dataset with BigQuery source. The BigQuery project location where the training data is to be written to. In the given project a new dataset is created with name `dataset___` where timestamp is in YYYY_MM_DDThh_mm_ss_sssZ format. All training input data is written into that dataset. In the dataset three tables are created, `training`, `validation` and `test`. * AIP_DATA_FORMAT = "bigquery". * AIP_TRAINING_DATA_URI = "bigquery_destination.dataset___.training" * AIP_VALIDATION_DATA_URI = "bigquery_destination.dataset___.validation" * AIP_TEST_DATA_URI = "bigquery_destination.dataset___.test"
        :param pulumi.Input['GoogleCloudAiplatformV1FilterSplitArgs'] filter_split: Split based on the provided filters for each set.
        :param pulumi.Input['GoogleCloudAiplatformV1FractionSplitArgs'] fraction_split: Split based on fractions defining the size of each set.
        :param pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs'] gcs_destination: The Cloud Storage location where the training data is to be written to. In the given directory a new directory is created with name: `dataset---` where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. All training input data is written into that directory. The Vertex AI environment variables representing Cloud Storage data URIs are represented in the Cloud Storage wildcard format to support sharded data. e.g.: "gs://.../training-*.jsonl" * AIP_DATA_FORMAT = "jsonl" for non-tabular data, "csv" for tabular data * AIP_TRAINING_DATA_URI = "gcs_destination/dataset---/training-*.${AIP_DATA_FORMAT}" * AIP_VALIDATION_DATA_URI = "gcs_destination/dataset---/validation-*.${AIP_DATA_FORMAT}" * AIP_TEST_DATA_URI = "gcs_destination/dataset---/test-*.${AIP_DATA_FORMAT}"
        :param pulumi.Input[bool] persist_ml_use_assignment: Whether to persist the ML use assignment to data item system labels.
        :param pulumi.Input['GoogleCloudAiplatformV1PredefinedSplitArgs'] predefined_split: Supported only for tabular Datasets. Split based on a predefined key.
        :param pulumi.Input[str] saved_query_id: Only applicable to Datasets that have SavedQueries. The ID of a SavedQuery (annotation set) under the Dataset specified by dataset_id used for filtering Annotations for training. Only Annotations that are associated with this SavedQuery are used in respectively training. When used in conjunction with annotations_filter, the Annotations used for training are filtered by both saved_query_id and annotations_filter. Only one of saved_query_id and annotation_schema_uri should be specified as both of them represent the same thing: problem type.
        :param pulumi.Input['GoogleCloudAiplatformV1StratifiedSplitArgs'] stratified_split: Supported only for tabular Datasets. Split based on the distribution of the specified column.
        :param pulumi.Input['GoogleCloudAiplatformV1TimestampSplitArgs'] timestamp_split: Supported only for tabular Datasets. Split based on the timestamp of the input data pieces.
        """
        pulumi.set(__self__, "dataset_id", dataset_id)
        if annotation_schema_uri is not None:
            pulumi.set(__self__, "annotation_schema_uri", annotation_schema_uri)
        if annotations_filter is not None:
            pulumi.set(__self__, "annotations_filter", annotations_filter)
        if bigquery_destination is not None:
            pulumi.set(__self__, "bigquery_destination", bigquery_destination)
        if filter_split is not None:
            pulumi.set(__self__, "filter_split", filter_split)
        if fraction_split is not None:
            pulumi.set(__self__, "fraction_split", fraction_split)
        if gcs_destination is not None:
            pulumi.set(__self__, "gcs_destination", gcs_destination)
        if persist_ml_use_assignment is not None:
            pulumi.set(__self__, "persist_ml_use_assignment", persist_ml_use_assignment)
        if predefined_split is not None:
            pulumi.set(__self__, "predefined_split", predefined_split)
        if saved_query_id is not None:
            pulumi.set(__self__, "saved_query_id", saved_query_id)
        if stratified_split is not None:
            pulumi.set(__self__, "stratified_split", stratified_split)
        if timestamp_split is not None:
            pulumi.set(__self__, "timestamp_split", timestamp_split)

    @property
    @pulumi.getter(name="datasetId")
    def dataset_id(self) -> pulumi.Input[str]:
        """
        The ID of the Dataset in the same Project and Location which data will be used to train the Model. The Dataset must use schema compatible with Model being trained, and what is compatible should be described in the used TrainingPipeline's training_task_definition. For tabular Datasets, all their data is exported to training, to pick and choose from.
        """
        return pulumi.get(self, "dataset_id")

    @dataset_id.setter
    def dataset_id(self, value: pulumi.Input[str]):
        pulumi.set(self, "dataset_id", value)

    @property
    @pulumi.getter(name="annotationSchemaUri")
    def annotation_schema_uri(self) -> Optional[pulumi.Input[str]]:
        """
        Applicable only to custom training with Datasets that have DataItems and Annotations. Cloud Storage URI that points to a YAML file describing the annotation schema. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). The schema files that can be used here are found in gs://google-cloud-aiplatform/schema/dataset/annotation/ , note that the chosen schema must be consistent with metadata of the Dataset specified by dataset_id. Only Annotations that both match this schema and belong to DataItems not ignored by the split method are used in respectively training, validation or test role, depending on the role of the DataItem they are on. When used in conjunction with annotations_filter, the Annotations used for training are filtered by both annotations_filter and annotation_schema_uri.
        """
        return pulumi.get(self, "annotation_schema_uri")

    @annotation_schema_uri.setter
    def annotation_schema_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "annotation_schema_uri", value)

    @property
    @pulumi.getter(name="annotationsFilter")
    def annotations_filter(self) -> Optional[pulumi.Input[str]]:
        """
        Applicable only to Datasets that have DataItems and Annotations. A filter on Annotations of the Dataset. Only Annotations that both match this filter and belong to DataItems not ignored by the split method are used in respectively training, validation or test role, depending on the role of the DataItem they are on (for the auto-assigned that role is decided by Vertex AI). A filter with same syntax as the one used in ListAnnotations may be used, but note here it filters across all Annotations of the Dataset, and not just within a single DataItem.
        """
        return pulumi.get(self, "annotations_filter")

    @annotations_filter.setter
    def annotations_filter(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "annotations_filter", value)

    @property
    @pulumi.getter(name="bigqueryDestination")
    def bigquery_destination(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs']]:
        """
        Only applicable to custom training with tabular Dataset with BigQuery source. The BigQuery project location where the training data is to be written to. In the given project a new dataset is created with name `dataset___` where timestamp is in YYYY_MM_DDThh_mm_ss_sssZ format. All training input data is written into that dataset. In the dataset three tables are created, `training`, `validation` and `test`. * AIP_DATA_FORMAT = "bigquery". * AIP_TRAINING_DATA_URI = "bigquery_destination.dataset___.training" * AIP_VALIDATION_DATA_URI = "bigquery_destination.dataset___.validation" * AIP_TEST_DATA_URI = "bigquery_destination.dataset___.test"
        """
        return pulumi.get(self, "bigquery_destination")

    @bigquery_destination.setter
    def bigquery_destination(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs']]):
        pulumi.set(self, "bigquery_destination", value)

    @property
    @pulumi.getter(name="filterSplit")
    def filter_split(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1FilterSplitArgs']]:
        """
        Split based on the provided filters for each set.
        """
        return pulumi.get(self, "filter_split")

    @filter_split.setter
    def filter_split(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1FilterSplitArgs']]):
        pulumi.set(self, "filter_split", value)

    @property
    @pulumi.getter(name="fractionSplit")
    def fraction_split(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1FractionSplitArgs']]:
        """
        Split based on fractions defining the size of each set.
        """
        return pulumi.get(self, "fraction_split")

    @fraction_split.setter
    def fraction_split(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1FractionSplitArgs']]):
        pulumi.set(self, "fraction_split", value)

    @property
    @pulumi.getter(name="gcsDestination")
    def gcs_destination(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs']]:
        """
        The Cloud Storage location where the training data is to be written to. In the given directory a new directory is created with name: `dataset---` where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format. All training input data is written into that directory. The Vertex AI environment variables representing Cloud Storage data URIs are represented in the Cloud Storage wildcard format to support sharded data. e.g.: "gs://.../training-*.jsonl" * AIP_DATA_FORMAT = "jsonl" for non-tabular data, "csv" for tabular data * AIP_TRAINING_DATA_URI = "gcs_destination/dataset---/training-*.${AIP_DATA_FORMAT}" * AIP_VALIDATION_DATA_URI = "gcs_destination/dataset---/validation-*.${AIP_DATA_FORMAT}" * AIP_TEST_DATA_URI = "gcs_destination/dataset---/test-*.${AIP_DATA_FORMAT}"
        """
        return pulumi.get(self, "gcs_destination")

    @gcs_destination.setter
    def gcs_destination(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs']]):
        pulumi.set(self, "gcs_destination", value)

    @property
    @pulumi.getter(name="persistMlUseAssignment")
    def persist_ml_use_assignment(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether to persist the ML use assignment to data item system labels.
        """
        return pulumi.get(self, "persist_ml_use_assignment")

    @persist_ml_use_assignment.setter
    def persist_ml_use_assignment(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "persist_ml_use_assignment", value)

    @property
    @pulumi.getter(name="predefinedSplit")
    def predefined_split(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1PredefinedSplitArgs']]:
        """
        Supported only for tabular Datasets. Split based on a predefined key.
        """
        return pulumi.get(self, "predefined_split")

    @predefined_split.setter
    def predefined_split(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1PredefinedSplitArgs']]):
        pulumi.set(self, "predefined_split", value)

    @property
    @pulumi.getter(name="savedQueryId")
    def saved_query_id(self) -> Optional[pulumi.Input[str]]:
        """
        Only applicable to Datasets that have SavedQueries. The ID of a SavedQuery (annotation set) under the Dataset specified by dataset_id used for filtering Annotations for training. Only Annotations that are associated with this SavedQuery are used in respectively training. When used in conjunction with annotations_filter, the Annotations used for training are filtered by both saved_query_id and annotations_filter. Only one of saved_query_id and annotation_schema_uri should be specified as both of them represent the same thing: problem type.
        """
        return pulumi.get(self, "saved_query_id")

    @saved_query_id.setter
    def saved_query_id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "saved_query_id", value)

    @property
    @pulumi.getter(name="stratifiedSplit")
    def stratified_split(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StratifiedSplitArgs']]:
        """
        Supported only for tabular Datasets. Split based on the distribution of the specified column.
        """
        return pulumi.get(self, "stratified_split")

    @stratified_split.setter
    def stratified_split(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StratifiedSplitArgs']]):
        pulumi.set(self, "stratified_split", value)

    @property
    @pulumi.getter(name="timestampSplit")
    def timestamp_split(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1TimestampSplitArgs']]:
        """
        Supported only for tabular Datasets. Split based on the timestamp of the input data pieces.
        """
        return pulumi.get(self, "timestamp_split")

    @timestamp_split.setter
    def timestamp_split(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1TimestampSplitArgs']]):
        pulumi.set(self, "timestamp_split", value)


@pulumi.input_type
class GoogleCloudAiplatformV1IntegratedGradientsAttributionArgs:
    def __init__(__self__, *,
                 step_count: pulumi.Input[int],
                 blur_baseline_config: Optional[pulumi.Input['GoogleCloudAiplatformV1BlurBaselineConfigArgs']] = None,
                 smooth_grad_config: Optional[pulumi.Input['GoogleCloudAiplatformV1SmoothGradConfigArgs']] = None):
        """
        An attribution method that computes the Aumann-Shapley value taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1703.01365
        :param pulumi.Input[int] step_count: The number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is within the desired error range. Valid range of its value is [1, 100], inclusively.
        :param pulumi.Input['GoogleCloudAiplatformV1BlurBaselineConfigArgs'] blur_baseline_config: Config for IG with blur baseline. When enabled, a linear path from the maximally blurred image to the input image is created. Using a blurred baseline instead of zero (black image) is motivated by the BlurIG approach explained here: https://arxiv.org/abs/2004.03383
        :param pulumi.Input['GoogleCloudAiplatformV1SmoothGradConfigArgs'] smooth_grad_config: Config for SmoothGrad approximation of gradients. When enabled, the gradients are approximated by averaging the gradients from noisy samples in the vicinity of the inputs. Adding noise can help improve the computed gradients. Refer to this paper for more details: https://arxiv.org/pdf/1706.03825.pdf
        """
        pulumi.set(__self__, "step_count", step_count)
        if blur_baseline_config is not None:
            pulumi.set(__self__, "blur_baseline_config", blur_baseline_config)
        if smooth_grad_config is not None:
            pulumi.set(__self__, "smooth_grad_config", smooth_grad_config)

    @property
    @pulumi.getter(name="stepCount")
    def step_count(self) -> pulumi.Input[int]:
        """
        The number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is within the desired error range. Valid range of its value is [1, 100], inclusively.
        """
        return pulumi.get(self, "step_count")

    @step_count.setter
    def step_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "step_count", value)

    @property
    @pulumi.getter(name="blurBaselineConfig")
    def blur_baseline_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1BlurBaselineConfigArgs']]:
        """
        Config for IG with blur baseline. When enabled, a linear path from the maximally blurred image to the input image is created. Using a blurred baseline instead of zero (black image) is motivated by the BlurIG approach explained here: https://arxiv.org/abs/2004.03383
        """
        return pulumi.get(self, "blur_baseline_config")

    @blur_baseline_config.setter
    def blur_baseline_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1BlurBaselineConfigArgs']]):
        pulumi.set(self, "blur_baseline_config", value)

    @property
    @pulumi.getter(name="smoothGradConfig")
    def smooth_grad_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1SmoothGradConfigArgs']]:
        """
        Config for SmoothGrad approximation of gradients. When enabled, the gradients are approximated by averaging the gradients from noisy samples in the vicinity of the inputs. Adding noise can help improve the computed gradients. Refer to this paper for more details: https://arxiv.org/pdf/1706.03825.pdf
        """
        return pulumi.get(self, "smooth_grad_config")

    @smooth_grad_config.setter
    def smooth_grad_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1SmoothGradConfigArgs']]):
        pulumi.set(self, "smooth_grad_config", value)


@pulumi.input_type
class GoogleCloudAiplatformV1MachineSpecArgs:
    def __init__(__self__, *,
                 accelerator_count: Optional[pulumi.Input[int]] = None,
                 accelerator_type: Optional[pulumi.Input['GoogleCloudAiplatformV1MachineSpecAcceleratorType']] = None,
                 machine_type: Optional[pulumi.Input[str]] = None,
                 tpu_topology: Optional[pulumi.Input[str]] = None):
        """
        Specification of a single machine.
        :param pulumi.Input[int] accelerator_count: The number of accelerators to attach to the machine.
        :param pulumi.Input['GoogleCloudAiplatformV1MachineSpecAcceleratorType'] accelerator_type: Immutable. The type of accelerator(s) that may be attached to the machine as per accelerator_count.
        :param pulumi.Input[str] machine_type: Immutable. The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types) See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types). For DeployedModel this field is optional, and the default value is `n1-standard-2`. For BatchPredictionJob or as part of WorkerPoolSpec this field is required.
        :param pulumi.Input[str] tpu_topology: Immutable. The topology of the TPUs. Corresponds to the TPU topologies available from GKE. (Example: tpu_topology: "2x2x1").
        """
        if accelerator_count is not None:
            pulumi.set(__self__, "accelerator_count", accelerator_count)
        if accelerator_type is not None:
            pulumi.set(__self__, "accelerator_type", accelerator_type)
        if machine_type is not None:
            pulumi.set(__self__, "machine_type", machine_type)
        if tpu_topology is not None:
            pulumi.set(__self__, "tpu_topology", tpu_topology)

    @property
    @pulumi.getter(name="acceleratorCount")
    def accelerator_count(self) -> Optional[pulumi.Input[int]]:
        """
        The number of accelerators to attach to the machine.
        """
        return pulumi.get(self, "accelerator_count")

    @accelerator_count.setter
    def accelerator_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "accelerator_count", value)

    @property
    @pulumi.getter(name="acceleratorType")
    def accelerator_type(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1MachineSpecAcceleratorType']]:
        """
        Immutable. The type of accelerator(s) that may be attached to the machine as per accelerator_count.
        """
        return pulumi.get(self, "accelerator_type")

    @accelerator_type.setter
    def accelerator_type(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1MachineSpecAcceleratorType']]):
        pulumi.set(self, "accelerator_type", value)

    @property
    @pulumi.getter(name="machineType")
    def machine_type(self) -> Optional[pulumi.Input[str]]:
        """
        Immutable. The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types) See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types). For DeployedModel this field is optional, and the default value is `n1-standard-2`. For BatchPredictionJob or as part of WorkerPoolSpec this field is required.
        """
        return pulumi.get(self, "machine_type")

    @machine_type.setter
    def machine_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "machine_type", value)

    @property
    @pulumi.getter(name="tpuTopology")
    def tpu_topology(self) -> Optional[pulumi.Input[str]]:
        """
        Immutable. The topology of the TPUs. Corresponds to the TPU topologies available from GKE. (Example: tpu_topology: "2x2x1").
        """
        return pulumi.get(self, "tpu_topology")

    @tpu_topology.setter
    def tpu_topology(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "tpu_topology", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ManualBatchTuningParametersArgs:
    def __init__(__self__, *,
                 batch_size: Optional[pulumi.Input[int]] = None):
        """
        Manual batch tuning parameters.
        :param pulumi.Input[int] batch_size: Immutable. The number of the records (e.g. instances) of the operation given in each batch to a machine replica. Machine type, and size of a single record should be considered when setting this parameter, higher value speeds up the batch operation's execution, but too high value will result in a whole batch not fitting in a machine's memory, and the whole operation will fail. The default value is 64.
        """
        if batch_size is not None:
            pulumi.set(__self__, "batch_size", batch_size)

    @property
    @pulumi.getter(name="batchSize")
    def batch_size(self) -> Optional[pulumi.Input[int]]:
        """
        Immutable. The number of the records (e.g. instances) of the operation given in each batch to a machine replica. Machine type, and size of a single record should be considered when setting this parameter, higher value speeds up the batch operation's execution, but too high value will result in a whole batch not fitting in a machine's memory, and the whole operation will fail. The default value is 64.
        """
        return pulumi.get(self, "batch_size")

    @batch_size.setter
    def batch_size(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "batch_size", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ModelContainerSpecArgs:
    def __init__(__self__, *,
                 image_uri: pulumi.Input[str],
                 args: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 command: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 deployment_timeout: Optional[pulumi.Input[str]] = None,
                 env: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1EnvVarArgs']]]] = None,
                 health_probe: Optional[pulumi.Input['GoogleCloudAiplatformV1ProbeArgs']] = None,
                 health_route: Optional[pulumi.Input[str]] = None,
                 ports: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1PortArgs']]]] = None,
                 predict_route: Optional[pulumi.Input[str]] = None,
                 shared_memory_size_mb: Optional[pulumi.Input[str]] = None,
                 startup_probe: Optional[pulumi.Input['GoogleCloudAiplatformV1ProbeArgs']] = None):
        """
        Specification of a container for serving predictions. Some fields in this message correspond to fields in the [Kubernetes Container v1 core specification](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        :param pulumi.Input[str] image_uri: Immutable. URI of the Docker image to be used as the custom container for serving predictions. This URI must identify an image in Artifact Registry or Container Registry. Learn more about the [container publishing requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#publishing), including permissions requirements for the Vertex AI Service Agent. The container image is ingested upon ModelService.UploadModel, stored internally, and this original path is afterwards not used. To learn about the requirements for the Docker image itself, see [Custom container requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#). You can use the URI to one of Vertex AI's [pre-built container images for prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers) in this field.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] args: Immutable. Specifies arguments for the command that runs when the container starts. This overrides the container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd). Specify this field as an array of executable and arguments, similar to a Docker `CMD`'s "default parameters" form. If you don't specify this field but do specify the command field, then the command from the `command` field runs without any additional arguments. See the [Kubernetes documentation about how the `command` and `args` fields interact with a container's `ENTRYPOINT` and `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). If you don't specify this field and don't specify the `command` field, then the container's [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#cmd) and `CMD` determine what runs based on their default behavior. See the Docker documentation about [how `CMD` and `ENTRYPOINT` interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). In this field, you can reference [environment variables set by Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $( VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME) This field corresponds to the `args` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        :param pulumi.Input[Sequence[pulumi.Input[str]]] command: Immutable. Specifies the command that runs when the container starts. This overrides the container's [ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint). Specify this field as an array of executable and arguments, similar to a Docker `ENTRYPOINT`'s "exec" form, not its "shell" form. If you do not specify this field, then the container's `ENTRYPOINT` runs, in conjunction with the args field or the container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd), if either exists. If this field is not specified and the container does not have an `ENTRYPOINT`, then refer to the Docker documentation about [how `CMD` and `ENTRYPOINT` interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). If you specify this field, then you can also specify the `args` field to provide additional arguments for this command. However, if you specify this field, then the container's `CMD` is ignored. See the [Kubernetes documentation about how the `command` and `args` fields interact with a container's `ENTRYPOINT` and `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). In this field, you can reference [environment variables set by Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $( VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME) This field corresponds to the `command` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        :param pulumi.Input[str] deployment_timeout: Immutable. Deployment timeout. TODO (b/306244185): Revise documentation before exposing.
        :param pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1EnvVarArgs']]] env: Immutable. List of environment variables to set in the container. After the container starts running, code running in the container can read these environment variables. Additionally, the command and args fields can reference these variables. Later entries in this list can also reference earlier entries. For example, the following example sets the variable `VAR_2` to have the value `foo bar`: ```json [ { "name": "VAR_1", "value": "foo" }, { "name": "VAR_2", "value": "$(VAR_1) bar" } ] ``` If you switch the order of the variables in the example, then the expansion does not occur. This field corresponds to the `env` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        :param pulumi.Input['GoogleCloudAiplatformV1ProbeArgs'] health_probe: Immutable. Specification for Kubernetes readiness probe. TODO (b/306244185): Revise documentation before exposing.
        :param pulumi.Input[str] health_route: Immutable. HTTP path on the container to send health checks to. Vertex AI intermittently sends GET requests to this path on the container's IP address and port to check that the container is healthy. Read more about [health checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health). For example, if you set this field to `/bar`, then Vertex AI intermittently sends a GET request to the `/bar` path on the port of your container specified by the first value of this `ModelContainerSpec`'s ports field. If you don't specify this field, it defaults to the following value when you deploy this Model to an Endpoint: /v1/endpoints/ENDPOINT/deployedModels/ DEPLOYED_MODEL:predict The placeholders in this value are replaced as follows: * ENDPOINT: The last segment (following `endpoints/`)of the Endpoint.name][] field of the Endpoint where this Model has been deployed. (Vertex AI makes this value available to your container code as the [`AIP_ENDPOINT_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).) * DEPLOYED_MODEL: DeployedModel.id of the `DeployedModel`. (Vertex AI makes this value available to your container code as the [`AIP_DEPLOYED_MODEL_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        :param pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1PortArgs']]] ports: Immutable. List of ports to expose from the container. Vertex AI sends any prediction requests that it receives to the first port on this list. Vertex AI also sends [liveness and health checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#liveness) to this port. If you do not specify this field, it defaults to following value: ```json [ { "containerPort": 8080 } ] ``` Vertex AI does not use ports other than the first one listed. This field corresponds to the `ports` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        :param pulumi.Input[str] predict_route: Immutable. HTTP path on the container to send prediction requests to. Vertex AI forwards requests sent using projects.locations.endpoints.predict to this path on the container's IP address and port. Vertex AI then returns the container's response in the API response. For example, if you set this field to `/foo`, then when Vertex AI receives a prediction request, it forwards the request body in a POST request to the `/foo` path on the port of your container specified by the first value of this `ModelContainerSpec`'s ports field. If you don't specify this field, it defaults to the following value when you deploy this Model to an Endpoint: /v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL:predict The placeholders in this value are replaced as follows: * ENDPOINT: The last segment (following `endpoints/`)of the Endpoint.name][] field of the Endpoint where this Model has been deployed. (Vertex AI makes this value available to your container code as the [`AIP_ENDPOINT_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).) * DEPLOYED_MODEL: DeployedModel.id of the `DeployedModel`. (Vertex AI makes this value available to your container code as the [`AIP_DEPLOYED_MODEL_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        :param pulumi.Input[str] shared_memory_size_mb: Immutable. The amount of the VM memory to reserve as the shared memory for the model in megabytes. TODO (b/306244185): Revise documentation before exposing.
        :param pulumi.Input['GoogleCloudAiplatformV1ProbeArgs'] startup_probe: Immutable. Specification for Kubernetes startup probe. TODO (b/306244185): Revise documentation before exposing.
        """
        pulumi.set(__self__, "image_uri", image_uri)
        if args is not None:
            pulumi.set(__self__, "args", args)
        if command is not None:
            pulumi.set(__self__, "command", command)
        if deployment_timeout is not None:
            pulumi.set(__self__, "deployment_timeout", deployment_timeout)
        if env is not None:
            pulumi.set(__self__, "env", env)
        if health_probe is not None:
            pulumi.set(__self__, "health_probe", health_probe)
        if health_route is not None:
            pulumi.set(__self__, "health_route", health_route)
        if ports is not None:
            pulumi.set(__self__, "ports", ports)
        if predict_route is not None:
            pulumi.set(__self__, "predict_route", predict_route)
        if shared_memory_size_mb is not None:
            pulumi.set(__self__, "shared_memory_size_mb", shared_memory_size_mb)
        if startup_probe is not None:
            pulumi.set(__self__, "startup_probe", startup_probe)

    @property
    @pulumi.getter(name="imageUri")
    def image_uri(self) -> pulumi.Input[str]:
        """
        Immutable. URI of the Docker image to be used as the custom container for serving predictions. This URI must identify an image in Artifact Registry or Container Registry. Learn more about the [container publishing requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#publishing), including permissions requirements for the Vertex AI Service Agent. The container image is ingested upon ModelService.UploadModel, stored internally, and this original path is afterwards not used. To learn about the requirements for the Docker image itself, see [Custom container requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#). You can use the URI to one of Vertex AI's [pre-built container images for prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers) in this field.
        """
        return pulumi.get(self, "image_uri")

    @image_uri.setter
    def image_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "image_uri", value)

    @property
    @pulumi.getter
    def args(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Immutable. Specifies arguments for the command that runs when the container starts. This overrides the container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd). Specify this field as an array of executable and arguments, similar to a Docker `CMD`'s "default parameters" form. If you don't specify this field but do specify the command field, then the command from the `command` field runs without any additional arguments. See the [Kubernetes documentation about how the `command` and `args` fields interact with a container's `ENTRYPOINT` and `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). If you don't specify this field and don't specify the `command` field, then the container's [`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#cmd) and `CMD` determine what runs based on their default behavior. See the Docker documentation about [how `CMD` and `ENTRYPOINT` interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). In this field, you can reference [environment variables set by Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $( VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME) This field corresponds to the `args` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        """
        return pulumi.get(self, "args")

    @args.setter
    def args(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "args", value)

    @property
    @pulumi.getter
    def command(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Immutable. Specifies the command that runs when the container starts. This overrides the container's [ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint). Specify this field as an array of executable and arguments, similar to a Docker `ENTRYPOINT`'s "exec" form, not its "shell" form. If you do not specify this field, then the container's `ENTRYPOINT` runs, in conjunction with the args field or the container's [`CMD`](https://docs.docker.com/engine/reference/builder/#cmd), if either exists. If this field is not specified and the container does not have an `ENTRYPOINT`, then refer to the Docker documentation about [how `CMD` and `ENTRYPOINT` interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). If you specify this field, then you can also specify the `args` field to provide additional arguments for this command. However, if you specify this field, then the container's `CMD` is ignored. See the [Kubernetes documentation about how the `command` and `args` fields interact with a container's `ENTRYPOINT` and `CMD`](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). In this field, you can reference [environment variables set by Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: $( VARIABLE_NAME) Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with `$$`; for example: $$(VARIABLE_NAME) This field corresponds to the `command` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        """
        return pulumi.get(self, "command")

    @command.setter
    def command(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "command", value)

    @property
    @pulumi.getter(name="deploymentTimeout")
    def deployment_timeout(self) -> Optional[pulumi.Input[str]]:
        """
        Immutable. Deployment timeout. TODO (b/306244185): Revise documentation before exposing.
        """
        return pulumi.get(self, "deployment_timeout")

    @deployment_timeout.setter
    def deployment_timeout(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "deployment_timeout", value)

    @property
    @pulumi.getter
    def env(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1EnvVarArgs']]]]:
        """
        Immutable. List of environment variables to set in the container. After the container starts running, code running in the container can read these environment variables. Additionally, the command and args fields can reference these variables. Later entries in this list can also reference earlier entries. For example, the following example sets the variable `VAR_2` to have the value `foo bar`: ```json [ { "name": "VAR_1", "value": "foo" }, { "name": "VAR_2", "value": "$(VAR_1) bar" } ] ``` If you switch the order of the variables in the example, then the expansion does not occur. This field corresponds to the `env` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        """
        return pulumi.get(self, "env")

    @env.setter
    def env(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1EnvVarArgs']]]]):
        pulumi.set(self, "env", value)

    @property
    @pulumi.getter(name="healthProbe")
    def health_probe(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ProbeArgs']]:
        """
        Immutable. Specification for Kubernetes readiness probe. TODO (b/306244185): Revise documentation before exposing.
        """
        return pulumi.get(self, "health_probe")

    @health_probe.setter
    def health_probe(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ProbeArgs']]):
        pulumi.set(self, "health_probe", value)

    @property
    @pulumi.getter(name="healthRoute")
    def health_route(self) -> Optional[pulumi.Input[str]]:
        """
        Immutable. HTTP path on the container to send health checks to. Vertex AI intermittently sends GET requests to this path on the container's IP address and port to check that the container is healthy. Read more about [health checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health). For example, if you set this field to `/bar`, then Vertex AI intermittently sends a GET request to the `/bar` path on the port of your container specified by the first value of this `ModelContainerSpec`'s ports field. If you don't specify this field, it defaults to the following value when you deploy this Model to an Endpoint: /v1/endpoints/ENDPOINT/deployedModels/ DEPLOYED_MODEL:predict The placeholders in this value are replaced as follows: * ENDPOINT: The last segment (following `endpoints/`)of the Endpoint.name][] field of the Endpoint where this Model has been deployed. (Vertex AI makes this value available to your container code as the [`AIP_ENDPOINT_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).) * DEPLOYED_MODEL: DeployedModel.id of the `DeployedModel`. (Vertex AI makes this value available to your container code as the [`AIP_DEPLOYED_MODEL_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        """
        return pulumi.get(self, "health_route")

    @health_route.setter
    def health_route(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "health_route", value)

    @property
    @pulumi.getter
    def ports(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1PortArgs']]]]:
        """
        Immutable. List of ports to expose from the container. Vertex AI sends any prediction requests that it receives to the first port on this list. Vertex AI also sends [liveness and health checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#liveness) to this port. If you do not specify this field, it defaults to following value: ```json [ { "containerPort": 8080 } ] ``` Vertex AI does not use ports other than the first one listed. This field corresponds to the `ports` field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).
        """
        return pulumi.get(self, "ports")

    @ports.setter
    def ports(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1PortArgs']]]]):
        pulumi.set(self, "ports", value)

    @property
    @pulumi.getter(name="predictRoute")
    def predict_route(self) -> Optional[pulumi.Input[str]]:
        """
        Immutable. HTTP path on the container to send prediction requests to. Vertex AI forwards requests sent using projects.locations.endpoints.predict to this path on the container's IP address and port. Vertex AI then returns the container's response in the API response. For example, if you set this field to `/foo`, then when Vertex AI receives a prediction request, it forwards the request body in a POST request to the `/foo` path on the port of your container specified by the first value of this `ModelContainerSpec`'s ports field. If you don't specify this field, it defaults to the following value when you deploy this Model to an Endpoint: /v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL:predict The placeholders in this value are replaced as follows: * ENDPOINT: The last segment (following `endpoints/`)of the Endpoint.name][] field of the Endpoint where this Model has been deployed. (Vertex AI makes this value available to your container code as the [`AIP_ENDPOINT_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).) * DEPLOYED_MODEL: DeployedModel.id of the `DeployedModel`. (Vertex AI makes this value available to your container code as the [`AIP_DEPLOYED_MODEL_ID` environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)
        """
        return pulumi.get(self, "predict_route")

    @predict_route.setter
    def predict_route(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "predict_route", value)

    @property
    @pulumi.getter(name="sharedMemorySizeMb")
    def shared_memory_size_mb(self) -> Optional[pulumi.Input[str]]:
        """
        Immutable. The amount of the VM memory to reserve as the shared memory for the model in megabytes. TODO (b/306244185): Revise documentation before exposing.
        """
        return pulumi.get(self, "shared_memory_size_mb")

    @shared_memory_size_mb.setter
    def shared_memory_size_mb(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "shared_memory_size_mb", value)

    @property
    @pulumi.getter(name="startupProbe")
    def startup_probe(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ProbeArgs']]:
        """
        Immutable. Specification for Kubernetes startup probe. TODO (b/306244185): Revise documentation before exposing.
        """
        return pulumi.get(self, "startup_probe")

    @startup_probe.setter
    def startup_probe(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ProbeArgs']]):
        pulumi.set(self, "startup_probe", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ModelDeploymentMonitoringObjectiveConfigArgs:
    def __init__(__self__, *,
                 deployed_model_id: Optional[pulumi.Input[str]] = None,
                 objective_config: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigArgs']] = None):
        """
        ModelDeploymentMonitoringObjectiveConfig contains the pair of deployed_model_id to ModelMonitoringObjectiveConfig.
        :param pulumi.Input[str] deployed_model_id: The DeployedModel ID of the objective config.
        :param pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigArgs'] objective_config: The objective config of for the modelmonitoring job of this deployed model.
        """
        if deployed_model_id is not None:
            pulumi.set(__self__, "deployed_model_id", deployed_model_id)
        if objective_config is not None:
            pulumi.set(__self__, "objective_config", objective_config)

    @property
    @pulumi.getter(name="deployedModelId")
    def deployed_model_id(self) -> Optional[pulumi.Input[str]]:
        """
        The DeployedModel ID of the objective config.
        """
        return pulumi.get(self, "deployed_model_id")

    @deployed_model_id.setter
    def deployed_model_id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "deployed_model_id", value)

    @property
    @pulumi.getter(name="objectiveConfig")
    def objective_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigArgs']]:
        """
        The objective config of for the modelmonitoring job of this deployed model.
        """
        return pulumi.get(self, "objective_config")

    @objective_config.setter
    def objective_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigArgs']]):
        pulumi.set(self, "objective_config", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ModelDeploymentMonitoringScheduleConfigArgs:
    def __init__(__self__, *,
                 monitor_interval: pulumi.Input[str],
                 monitor_window: Optional[pulumi.Input[str]] = None):
        """
        The config for scheduling monitoring job.
        :param pulumi.Input[str] monitor_interval: The model monitoring job scheduling interval. It will be rounded up to next full hour. This defines how often the monitoring jobs are triggered.
        :param pulumi.Input[str] monitor_window: The time window of the prediction data being included in each prediction dataset. This window specifies how long the data should be collected from historical model results for each run. If not set, ModelDeploymentMonitoringScheduleConfig.monitor_interval will be used. e.g. If currently the cutoff time is 2022-01-08 14:30:00 and the monitor_window is set to be 3600, then data from 2022-01-08 13:30:00 to 2022-01-08 14:30:00 will be retrieved and aggregated to calculate the monitoring statistics.
        """
        pulumi.set(__self__, "monitor_interval", monitor_interval)
        if monitor_window is not None:
            pulumi.set(__self__, "monitor_window", monitor_window)

    @property
    @pulumi.getter(name="monitorInterval")
    def monitor_interval(self) -> pulumi.Input[str]:
        """
        The model monitoring job scheduling interval. It will be rounded up to next full hour. This defines how often the monitoring jobs are triggered.
        """
        return pulumi.get(self, "monitor_interval")

    @monitor_interval.setter
    def monitor_interval(self, value: pulumi.Input[str]):
        pulumi.set(self, "monitor_interval", value)

    @property
    @pulumi.getter(name="monitorWindow")
    def monitor_window(self) -> Optional[pulumi.Input[str]]:
        """
        The time window of the prediction data being included in each prediction dataset. This window specifies how long the data should be collected from historical model results for each run. If not set, ModelDeploymentMonitoringScheduleConfig.monitor_interval will be used. e.g. If currently the cutoff time is 2022-01-08 14:30:00 and the monitor_window is set to be 3600, then data from 2022-01-08 13:30:00 to 2022-01-08 14:30:00 will be retrieved and aggregated to calculate the monitoring statistics.
        """
        return pulumi.get(self, "monitor_window")

    @monitor_window.setter
    def monitor_window(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "monitor_window", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigArgs:
    def __init__(__self__, *,
                 user_emails: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        The config for email alert.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] user_emails: The email addresses to send the alert.
        """
        if user_emails is not None:
            pulumi.set(__self__, "user_emails", user_emails)

    @property
    @pulumi.getter(name="userEmails")
    def user_emails(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        The email addresses to send the alert.
        """
        return pulumi.get(self, "user_emails")

    @user_emails.setter
    def user_emails(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "user_emails", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ModelMonitoringAlertConfigArgs:
    def __init__(__self__, *,
                 email_alert_config: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigArgs']] = None,
                 enable_logging: Optional[pulumi.Input[bool]] = None,
                 notification_channels: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        :param pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigArgs'] email_alert_config: Email alert config.
        :param pulumi.Input[bool] enable_logging: Dump the anomalies to Cloud Logging. The anomalies will be put to json payload encoded from proto google.cloud.aiplatform.logging.ModelMonitoringAnomaliesLogEntry. This can be further sinked to Pub/Sub or any other services supported by Cloud Logging.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] notification_channels: Resource names of the NotificationChannels to send alert. Must be of the format `projects//notificationChannels/`
        """
        if email_alert_config is not None:
            pulumi.set(__self__, "email_alert_config", email_alert_config)
        if enable_logging is not None:
            pulumi.set(__self__, "enable_logging", enable_logging)
        if notification_channels is not None:
            pulumi.set(__self__, "notification_channels", notification_channels)

    @property
    @pulumi.getter(name="emailAlertConfig")
    def email_alert_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigArgs']]:
        """
        Email alert config.
        """
        return pulumi.get(self, "email_alert_config")

    @email_alert_config.setter
    def email_alert_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringAlertConfigEmailAlertConfigArgs']]):
        pulumi.set(self, "email_alert_config", value)

    @property
    @pulumi.getter(name="enableLogging")
    def enable_logging(self) -> Optional[pulumi.Input[bool]]:
        """
        Dump the anomalies to Cloud Logging. The anomalies will be put to json payload encoded from proto google.cloud.aiplatform.logging.ModelMonitoringAnomaliesLogEntry. This can be further sinked to Pub/Sub or any other services supported by Cloud Logging.
        """
        return pulumi.get(self, "enable_logging")

    @enable_logging.setter
    def enable_logging(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_logging", value)

    @property
    @pulumi.getter(name="notificationChannels")
    def notification_channels(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Resource names of the NotificationChannels to send alert. Must be of the format `projects//notificationChannels/`
        """
        return pulumi.get(self, "notification_channels")

    @notification_channels.setter
    def notification_channels(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "notification_channels", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineArgs:
    def __init__(__self__, *,
                 bigquery: Optional[pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs']] = None,
                 gcs: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs']] = None,
                 prediction_format: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselinePredictionFormat']] = None):
        """
        Output from BatchPredictionJob for Model Monitoring baseline dataset, which can be used to generate baseline attribution scores.
        :param pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs'] bigquery: BigQuery location for BatchExplain output.
        :param pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs'] gcs: Cloud Storage location for BatchExplain output.
        :param pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselinePredictionFormat'] prediction_format: The storage format of the predictions generated BatchPrediction job.
        """
        if bigquery is not None:
            pulumi.set(__self__, "bigquery", bigquery)
        if gcs is not None:
            pulumi.set(__self__, "gcs", gcs)
        if prediction_format is not None:
            pulumi.set(__self__, "prediction_format", prediction_format)

    @property
    @pulumi.getter
    def bigquery(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs']]:
        """
        BigQuery location for BatchExplain output.
        """
        return pulumi.get(self, "bigquery")

    @bigquery.setter
    def bigquery(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs']]):
        pulumi.set(self, "bigquery", value)

    @property
    @pulumi.getter
    def gcs(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs']]:
        """
        Cloud Storage location for BatchExplain output.
        """
        return pulumi.get(self, "gcs")

    @gcs.setter
    def gcs(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsDestinationArgs']]):
        pulumi.set(self, "gcs", value)

    @property
    @pulumi.getter(name="predictionFormat")
    def prediction_format(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselinePredictionFormat']]:
        """
        The storage format of the predictions generated BatchPrediction job.
        """
        return pulumi.get(self, "prediction_format")

    @prediction_format.setter
    def prediction_format(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselinePredictionFormat']]):
        pulumi.set(self, "prediction_format", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigArgs:
    def __init__(__self__, *,
                 enable_feature_attributes: Optional[pulumi.Input[bool]] = None,
                 explanation_baseline: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineArgs']] = None):
        """
        The config for integrating with Vertex Explainable AI. Only applicable if the Model has explanation_spec populated.
        :param pulumi.Input[bool] enable_feature_attributes: If want to analyze the Vertex Explainable AI feature attribute scores or not. If set to true, Vertex AI will log the feature attributions from explain response and do the skew/drift detection for them.
        :param pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineArgs'] explanation_baseline: Predictions generated by the BatchPredictionJob using baseline dataset.
        """
        if enable_feature_attributes is not None:
            pulumi.set(__self__, "enable_feature_attributes", enable_feature_attributes)
        if explanation_baseline is not None:
            pulumi.set(__self__, "explanation_baseline", explanation_baseline)

    @property
    @pulumi.getter(name="enableFeatureAttributes")
    def enable_feature_attributes(self) -> Optional[pulumi.Input[bool]]:
        """
        If want to analyze the Vertex Explainable AI feature attribute scores or not. If set to true, Vertex AI will log the feature attributions from explain response and do the skew/drift detection for them.
        """
        return pulumi.get(self, "enable_feature_attributes")

    @enable_feature_attributes.setter
    def enable_feature_attributes(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_feature_attributes", value)

    @property
    @pulumi.getter(name="explanationBaseline")
    def explanation_baseline(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineArgs']]:
        """
        Predictions generated by the BatchPredictionJob using baseline dataset.
        """
        return pulumi.get(self, "explanation_baseline")

    @explanation_baseline.setter
    def explanation_baseline(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigExplanationBaselineArgs']]):
        pulumi.set(self, "explanation_baseline", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigArgs:
    def __init__(__self__, *,
                 attribution_score_drift_thresholds: Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]]] = None,
                 default_drift_threshold: Optional[pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']] = None,
                 drift_thresholds: Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]]] = None):
        """
        The config for Prediction data drift detection.
        :param pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]] attribution_score_drift_thresholds: Key is the feature name and value is the threshold. The threshold here is against attribution score distance between different time windows.
        :param pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs'] default_drift_threshold: Drift anomaly detection threshold used by all features. When the per-feature thresholds are not set, this field can be used to specify a threshold for all features.
        :param pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]] drift_thresholds: Key is the feature name and value is the threshold. If a feature needs to be monitored for drift, a value threshold must be configured for that feature. The threshold here is against feature distribution distance between different time windws.
        """
        if attribution_score_drift_thresholds is not None:
            pulumi.set(__self__, "attribution_score_drift_thresholds", attribution_score_drift_thresholds)
        if default_drift_threshold is not None:
            pulumi.set(__self__, "default_drift_threshold", default_drift_threshold)
        if drift_thresholds is not None:
            pulumi.set(__self__, "drift_thresholds", drift_thresholds)

    @property
    @pulumi.getter(name="attributionScoreDriftThresholds")
    def attribution_score_drift_thresholds(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]]]:
        """
        Key is the feature name and value is the threshold. The threshold here is against attribution score distance between different time windows.
        """
        return pulumi.get(self, "attribution_score_drift_thresholds")

    @attribution_score_drift_thresholds.setter
    def attribution_score_drift_thresholds(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]]]):
        pulumi.set(self, "attribution_score_drift_thresholds", value)

    @property
    @pulumi.getter(name="defaultDriftThreshold")
    def default_drift_threshold(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]:
        """
        Drift anomaly detection threshold used by all features. When the per-feature thresholds are not set, this field can be used to specify a threshold for all features.
        """
        return pulumi.get(self, "default_drift_threshold")

    @default_drift_threshold.setter
    def default_drift_threshold(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]):
        pulumi.set(self, "default_drift_threshold", value)

    @property
    @pulumi.getter(name="driftThresholds")
    def drift_thresholds(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]]]:
        """
        Key is the feature name and value is the threshold. If a feature needs to be monitored for drift, a value threshold must be configured for that feature. The threshold here is against feature distribution distance between different time windws.
        """
        return pulumi.get(self, "drift_thresholds")

    @drift_thresholds.setter
    def drift_thresholds(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]]]):
        pulumi.set(self, "drift_thresholds", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetArgs:
    def __init__(__self__, *,
                 bigquery_source: Optional[pulumi.Input['GoogleCloudAiplatformV1BigQuerySourceArgs']] = None,
                 data_format: Optional[pulumi.Input[str]] = None,
                 dataset: Optional[pulumi.Input[str]] = None,
                 gcs_source: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsSourceArgs']] = None,
                 logging_sampling_strategy: Optional[pulumi.Input['GoogleCloudAiplatformV1SamplingStrategyArgs']] = None,
                 target_field: Optional[pulumi.Input[str]] = None):
        """
        Training Dataset information.
        :param pulumi.Input['GoogleCloudAiplatformV1BigQuerySourceArgs'] bigquery_source: The BigQuery table of the unmanaged Dataset used to train this Model.
        :param pulumi.Input[str] data_format: Data format of the dataset, only applicable if the input is from Google Cloud Storage. The possible formats are: "tf-record" The source file is a TFRecord file. "csv" The source file is a CSV file. "jsonl" The source file is a JSONL file.
        :param pulumi.Input[str] dataset: The resource name of the Dataset used to train this Model.
        :param pulumi.Input['GoogleCloudAiplatformV1GcsSourceArgs'] gcs_source: The Google Cloud Storage uri of the unmanaged Dataset used to train this Model.
        :param pulumi.Input['GoogleCloudAiplatformV1SamplingStrategyArgs'] logging_sampling_strategy: Strategy to sample data from Training Dataset. If not set, we process the whole dataset.
        :param pulumi.Input[str] target_field: The target field name the model is to predict. This field will be excluded when doing Predict and (or) Explain for the training data.
        """
        if bigquery_source is not None:
            pulumi.set(__self__, "bigquery_source", bigquery_source)
        if data_format is not None:
            pulumi.set(__self__, "data_format", data_format)
        if dataset is not None:
            pulumi.set(__self__, "dataset", dataset)
        if gcs_source is not None:
            pulumi.set(__self__, "gcs_source", gcs_source)
        if logging_sampling_strategy is not None:
            pulumi.set(__self__, "logging_sampling_strategy", logging_sampling_strategy)
        if target_field is not None:
            pulumi.set(__self__, "target_field", target_field)

    @property
    @pulumi.getter(name="bigquerySource")
    def bigquery_source(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1BigQuerySourceArgs']]:
        """
        The BigQuery table of the unmanaged Dataset used to train this Model.
        """
        return pulumi.get(self, "bigquery_source")

    @bigquery_source.setter
    def bigquery_source(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1BigQuerySourceArgs']]):
        pulumi.set(self, "bigquery_source", value)

    @property
    @pulumi.getter(name="dataFormat")
    def data_format(self) -> Optional[pulumi.Input[str]]:
        """
        Data format of the dataset, only applicable if the input is from Google Cloud Storage. The possible formats are: "tf-record" The source file is a TFRecord file. "csv" The source file is a CSV file. "jsonl" The source file is a JSONL file.
        """
        return pulumi.get(self, "data_format")

    @data_format.setter
    def data_format(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "data_format", value)

    @property
    @pulumi.getter
    def dataset(self) -> Optional[pulumi.Input[str]]:
        """
        The resource name of the Dataset used to train this Model.
        """
        return pulumi.get(self, "dataset")

    @dataset.setter
    def dataset(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "dataset", value)

    @property
    @pulumi.getter(name="gcsSource")
    def gcs_source(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1GcsSourceArgs']]:
        """
        The Google Cloud Storage uri of the unmanaged Dataset used to train this Model.
        """
        return pulumi.get(self, "gcs_source")

    @gcs_source.setter
    def gcs_source(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1GcsSourceArgs']]):
        pulumi.set(self, "gcs_source", value)

    @property
    @pulumi.getter(name="loggingSamplingStrategy")
    def logging_sampling_strategy(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1SamplingStrategyArgs']]:
        """
        Strategy to sample data from Training Dataset. If not set, we process the whole dataset.
        """
        return pulumi.get(self, "logging_sampling_strategy")

    @logging_sampling_strategy.setter
    def logging_sampling_strategy(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1SamplingStrategyArgs']]):
        pulumi.set(self, "logging_sampling_strategy", value)

    @property
    @pulumi.getter(name="targetField")
    def target_field(self) -> Optional[pulumi.Input[str]]:
        """
        The target field name the model is to predict. This field will be excluded when doing Predict and (or) Explain for the training data.
        """
        return pulumi.get(self, "target_field")

    @target_field.setter
    def target_field(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "target_field", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigArgs:
    def __init__(__self__, *,
                 attribution_score_skew_thresholds: Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]]] = None,
                 default_skew_threshold: Optional[pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']] = None,
                 skew_thresholds: Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]]] = None):
        """
        The config for Training & Prediction data skew detection. It specifies the training dataset sources and the skew detection parameters.
        :param pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]] attribution_score_skew_thresholds: Key is the feature name and value is the threshold. The threshold here is against attribution score distance between the training and prediction feature.
        :param pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs'] default_skew_threshold: Skew anomaly detection threshold used by all features. When the per-feature thresholds are not set, this field can be used to specify a threshold for all features.
        :param pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]] skew_thresholds: Key is the feature name and value is the threshold. If a feature needs to be monitored for skew, a value threshold must be configured for that feature. The threshold here is against feature distribution distance between the training and prediction feature.
        """
        if attribution_score_skew_thresholds is not None:
            pulumi.set(__self__, "attribution_score_skew_thresholds", attribution_score_skew_thresholds)
        if default_skew_threshold is not None:
            pulumi.set(__self__, "default_skew_threshold", default_skew_threshold)
        if skew_thresholds is not None:
            pulumi.set(__self__, "skew_thresholds", skew_thresholds)

    @property
    @pulumi.getter(name="attributionScoreSkewThresholds")
    def attribution_score_skew_thresholds(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]]]:
        """
        Key is the feature name and value is the threshold. The threshold here is against attribution score distance between the training and prediction feature.
        """
        return pulumi.get(self, "attribution_score_skew_thresholds")

    @attribution_score_skew_thresholds.setter
    def attribution_score_skew_thresholds(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]]]):
        pulumi.set(self, "attribution_score_skew_thresholds", value)

    @property
    @pulumi.getter(name="defaultSkewThreshold")
    def default_skew_threshold(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]:
        """
        Skew anomaly detection threshold used by all features. When the per-feature thresholds are not set, this field can be used to specify a threshold for all features.
        """
        return pulumi.get(self, "default_skew_threshold")

    @default_skew_threshold.setter
    def default_skew_threshold(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]):
        pulumi.set(self, "default_skew_threshold", value)

    @property
    @pulumi.getter(name="skewThresholds")
    def skew_thresholds(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]]]:
        """
        Key is the feature name and value is the threshold. If a feature needs to be monitored for skew, a value threshold must be configured for that feature. The threshold here is against feature distribution distance between the training and prediction feature.
        """
        return pulumi.get(self, "skew_thresholds")

    @skew_thresholds.setter
    def skew_thresholds(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ThresholdConfigArgs']]]]):
        pulumi.set(self, "skew_thresholds", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigArgs:
    def __init__(__self__, *,
                 explanation_config: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigArgs']] = None,
                 prediction_drift_detection_config: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigArgs']] = None,
                 training_dataset: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetArgs']] = None,
                 training_prediction_skew_detection_config: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigArgs']] = None):
        """
        The objective configuration for model monitoring, including the information needed to detect anomalies for one particular model.
        :param pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigArgs'] explanation_config: The config for integrating with Vertex Explainable AI.
        :param pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigArgs'] prediction_drift_detection_config: The config for drift of prediction data.
        :param pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetArgs'] training_dataset: Training dataset for models. This field has to be set only if TrainingPredictionSkewDetectionConfig is specified.
        :param pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigArgs'] training_prediction_skew_detection_config: The config for skew between training data and prediction data.
        """
        if explanation_config is not None:
            pulumi.set(__self__, "explanation_config", explanation_config)
        if prediction_drift_detection_config is not None:
            pulumi.set(__self__, "prediction_drift_detection_config", prediction_drift_detection_config)
        if training_dataset is not None:
            pulumi.set(__self__, "training_dataset", training_dataset)
        if training_prediction_skew_detection_config is not None:
            pulumi.set(__self__, "training_prediction_skew_detection_config", training_prediction_skew_detection_config)

    @property
    @pulumi.getter(name="explanationConfig")
    def explanation_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigArgs']]:
        """
        The config for integrating with Vertex Explainable AI.
        """
        return pulumi.get(self, "explanation_config")

    @explanation_config.setter
    def explanation_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigExplanationConfigArgs']]):
        pulumi.set(self, "explanation_config", value)

    @property
    @pulumi.getter(name="predictionDriftDetectionConfig")
    def prediction_drift_detection_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigArgs']]:
        """
        The config for drift of prediction data.
        """
        return pulumi.get(self, "prediction_drift_detection_config")

    @prediction_drift_detection_config.setter
    def prediction_drift_detection_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigPredictionDriftDetectionConfigArgs']]):
        pulumi.set(self, "prediction_drift_detection_config", value)

    @property
    @pulumi.getter(name="trainingDataset")
    def training_dataset(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetArgs']]:
        """
        Training dataset for models. This field has to be set only if TrainingPredictionSkewDetectionConfig is specified.
        """
        return pulumi.get(self, "training_dataset")

    @training_dataset.setter
    def training_dataset(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingDatasetArgs']]):
        pulumi.set(self, "training_dataset", value)

    @property
    @pulumi.getter(name="trainingPredictionSkewDetectionConfig")
    def training_prediction_skew_detection_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigArgs']]:
        """
        The config for skew between training data and prediction data.
        """
        return pulumi.get(self, "training_prediction_skew_detection_config")

    @training_prediction_skew_detection_config.setter
    def training_prediction_skew_detection_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelMonitoringObjectiveConfigTrainingPredictionSkewDetectionConfigArgs']]):
        pulumi.set(self, "training_prediction_skew_detection_config", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ModelArgs:
    def __init__(__self__, *,
                 display_name: pulumi.Input[str],
                 artifact_uri: Optional[pulumi.Input[str]] = None,
                 container_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelContainerSpecArgs']] = None,
                 description: Optional[pulumi.Input[str]] = None,
                 encryption_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1EncryptionSpecArgs']] = None,
                 etag: Optional[pulumi.Input[str]] = None,
                 explanation_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationSpecArgs']] = None,
                 labels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 metadata: Optional[Any] = None,
                 metadata_schema_uri: Optional[pulumi.Input[str]] = None,
                 name: Optional[pulumi.Input[str]] = None,
                 pipeline_job: Optional[pulumi.Input[str]] = None,
                 predict_schemata: Optional[pulumi.Input['GoogleCloudAiplatformV1PredictSchemataArgs']] = None,
                 version_aliases: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 version_description: Optional[pulumi.Input[str]] = None):
        """
        A trained machine learning Model.
        :param pulumi.Input[str] display_name: The display name of the Model. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        :param pulumi.Input[str] artifact_uri: Immutable. The path to the directory containing the Model artifact and any of its supporting files. Not present for AutoML Models or Large Models.
        :param pulumi.Input['GoogleCloudAiplatformV1ModelContainerSpecArgs'] container_spec: Input only. The specification of the container that is to be used when deploying this Model. The specification is ingested upon ModelService.UploadModel, and all binaries it contains are copied and stored internally by Vertex AI. Not present for AutoML Models or Large Models.
        :param pulumi.Input[str] description: The description of the Model.
        :param pulumi.Input['GoogleCloudAiplatformV1EncryptionSpecArgs'] encryption_spec: Customer-managed encryption key spec for a Model. If set, this Model and all sub-resources of this Model will be secured by this key.
        :param pulumi.Input[str] etag: Used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
        :param pulumi.Input['GoogleCloudAiplatformV1ExplanationSpecArgs'] explanation_spec: The default explanation specification for this Model. The Model can be used for requesting explanation after being deployed if it is populated. The Model can be used for batch explanation if it is populated. All fields of the explanation_spec can be overridden by explanation_spec of DeployModelRequest.deployed_model, or explanation_spec of BatchPredictionJob. If the default explanation specification is not set for this Model, this Model can still be used for requesting explanation by setting explanation_spec of DeployModelRequest.deployed_model and for batch explanation by setting explanation_spec of BatchPredictionJob.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] labels: The labels with user-defined metadata to organize your Models. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
        :param Any metadata: Immutable. An additional information about the Model; the schema of the metadata can be found in metadata_schema. Unset if the Model does not have any additional information.
        :param pulumi.Input[str] metadata_schema_uri: Immutable. Points to a YAML file stored on Google Cloud Storage describing additional information about the Model, that is specific to it. Unset if the Model does not have any additional information. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI, if no additional metadata is needed, this field is set to an empty string. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        :param pulumi.Input[str] name: The resource name of the Model.
        :param pulumi.Input[str] pipeline_job: Optional. This field is populated if the model is produced by a pipeline job.
        :param pulumi.Input['GoogleCloudAiplatformV1PredictSchemataArgs'] predict_schemata: The schemata that describe formats of the Model's predictions and explanations as given and returned via PredictionService.Predict and PredictionService.Explain.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] version_aliases: User provided version aliases so that a model version can be referenced via alias (i.e. `projects/{project}/locations/{location}/models/{model_id}@{version_alias}` instead of auto-generated version id (i.e. `projects/{project}/locations/{location}/models/{model_id}@{version_id})`. The format is a-z{0,126}[a-z0-9] to distinguish from version_id. A default version alias will be created for the first version of the model, and there must be exactly one default version alias for a model.
        :param pulumi.Input[str] version_description: The description of this version.
        """
        pulumi.set(__self__, "display_name", display_name)
        if artifact_uri is not None:
            pulumi.set(__self__, "artifact_uri", artifact_uri)
        if container_spec is not None:
            pulumi.set(__self__, "container_spec", container_spec)
        if description is not None:
            pulumi.set(__self__, "description", description)
        if encryption_spec is not None:
            pulumi.set(__self__, "encryption_spec", encryption_spec)
        if etag is not None:
            pulumi.set(__self__, "etag", etag)
        if explanation_spec is not None:
            pulumi.set(__self__, "explanation_spec", explanation_spec)
        if labels is not None:
            pulumi.set(__self__, "labels", labels)
        if metadata is not None:
            pulumi.set(__self__, "metadata", metadata)
        if metadata_schema_uri is not None:
            pulumi.set(__self__, "metadata_schema_uri", metadata_schema_uri)
        if name is not None:
            pulumi.set(__self__, "name", name)
        if pipeline_job is not None:
            pulumi.set(__self__, "pipeline_job", pipeline_job)
        if predict_schemata is not None:
            pulumi.set(__self__, "predict_schemata", predict_schemata)
        if version_aliases is not None:
            pulumi.set(__self__, "version_aliases", version_aliases)
        if version_description is not None:
            pulumi.set(__self__, "version_description", version_description)

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> pulumi.Input[str]:
        """
        The display name of the Model. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        """
        return pulumi.get(self, "display_name")

    @display_name.setter
    def display_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "display_name", value)

    @property
    @pulumi.getter(name="artifactUri")
    def artifact_uri(self) -> Optional[pulumi.Input[str]]:
        """
        Immutable. The path to the directory containing the Model artifact and any of its supporting files. Not present for AutoML Models or Large Models.
        """
        return pulumi.get(self, "artifact_uri")

    @artifact_uri.setter
    def artifact_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "artifact_uri", value)

    @property
    @pulumi.getter(name="containerSpec")
    def container_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ModelContainerSpecArgs']]:
        """
        Input only. The specification of the container that is to be used when deploying this Model. The specification is ingested upon ModelService.UploadModel, and all binaries it contains are copied and stored internally by Vertex AI. Not present for AutoML Models or Large Models.
        """
        return pulumi.get(self, "container_spec")

    @container_spec.setter
    def container_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelContainerSpecArgs']]):
        pulumi.set(self, "container_spec", value)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        """
        The description of the Model.
        """
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)

    @property
    @pulumi.getter(name="encryptionSpec")
    def encryption_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1EncryptionSpecArgs']]:
        """
        Customer-managed encryption key spec for a Model. If set, this Model and all sub-resources of this Model will be secured by this key.
        """
        return pulumi.get(self, "encryption_spec")

    @encryption_spec.setter
    def encryption_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1EncryptionSpecArgs']]):
        pulumi.set(self, "encryption_spec", value)

    @property
    @pulumi.getter
    def etag(self) -> Optional[pulumi.Input[str]]:
        """
        Used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
        """
        return pulumi.get(self, "etag")

    @etag.setter
    def etag(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "etag", value)

    @property
    @pulumi.getter(name="explanationSpec")
    def explanation_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationSpecArgs']]:
        """
        The default explanation specification for this Model. The Model can be used for requesting explanation after being deployed if it is populated. The Model can be used for batch explanation if it is populated. All fields of the explanation_spec can be overridden by explanation_spec of DeployModelRequest.deployed_model, or explanation_spec of BatchPredictionJob. If the default explanation specification is not set for this Model, this Model can still be used for requesting explanation by setting explanation_spec of DeployModelRequest.deployed_model and for batch explanation by setting explanation_spec of BatchPredictionJob.
        """
        return pulumi.get(self, "explanation_spec")

    @explanation_spec.setter
    def explanation_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ExplanationSpecArgs']]):
        pulumi.set(self, "explanation_spec", value)

    @property
    @pulumi.getter
    def labels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The labels with user-defined metadata to organize your Models. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
        """
        return pulumi.get(self, "labels")

    @labels.setter
    def labels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "labels", value)

    @property
    @pulumi.getter
    def metadata(self) -> Optional[Any]:
        """
        Immutable. An additional information about the Model; the schema of the metadata can be found in metadata_schema. Unset if the Model does not have any additional information.
        """
        return pulumi.get(self, "metadata")

    @metadata.setter
    def metadata(self, value: Optional[Any]):
        pulumi.set(self, "metadata", value)

    @property
    @pulumi.getter(name="metadataSchemaUri")
    def metadata_schema_uri(self) -> Optional[pulumi.Input[str]]:
        """
        Immutable. Points to a YAML file stored on Google Cloud Storage describing additional information about the Model, that is specific to it. Unset if the Model does not have any additional information. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI, if no additional metadata is needed, this field is set to an empty string. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        """
        return pulumi.get(self, "metadata_schema_uri")

    @metadata_schema_uri.setter
    def metadata_schema_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "metadata_schema_uri", value)

    @property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[str]]:
        """
        The resource name of the Model.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "name", value)

    @property
    @pulumi.getter(name="pipelineJob")
    def pipeline_job(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. This field is populated if the model is produced by a pipeline job.
        """
        return pulumi.get(self, "pipeline_job")

    @pipeline_job.setter
    def pipeline_job(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "pipeline_job", value)

    @property
    @pulumi.getter(name="predictSchemata")
    def predict_schemata(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1PredictSchemataArgs']]:
        """
        The schemata that describe formats of the Model's predictions and explanations as given and returned via PredictionService.Predict and PredictionService.Explain.
        """
        return pulumi.get(self, "predict_schemata")

    @predict_schemata.setter
    def predict_schemata(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1PredictSchemataArgs']]):
        pulumi.set(self, "predict_schemata", value)

    @property
    @pulumi.getter(name="versionAliases")
    def version_aliases(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        User provided version aliases so that a model version can be referenced via alias (i.e. `projects/{project}/locations/{location}/models/{model_id}@{version_alias}` instead of auto-generated version id (i.e. `projects/{project}/locations/{location}/models/{model_id}@{version_id})`. The format is a-z{0,126}[a-z0-9] to distinguish from version_id. A default version alias will be created for the first version of the model, and there must be exactly one default version alias for a model.
        """
        return pulumi.get(self, "version_aliases")

    @version_aliases.setter
    def version_aliases(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "version_aliases", value)

    @property
    @pulumi.getter(name="versionDescription")
    def version_description(self) -> Optional[pulumi.Input[str]]:
        """
        The description of this version.
        """
        return pulumi.get(self, "version_description")

    @version_description.setter
    def version_description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "version_description", value)


@pulumi.input_type
class GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecArgs:
    def __init__(__self__, *,
                 goal: pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecGoal'],
                 metric_id: pulumi.Input[str]):
        """
        Represents a metric to optimize.
        :param pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecGoal'] goal: The optimization goal of the metric.
        :param pulumi.Input[str] metric_id: The ID of the metric. Must not contain whitespaces.
        """
        pulumi.set(__self__, "goal", goal)
        pulumi.set(__self__, "metric_id", metric_id)

    @property
    @pulumi.getter
    def goal(self) -> pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecGoal']:
        """
        The optimization goal of the metric.
        """
        return pulumi.get(self, "goal")

    @goal.setter
    def goal(self, value: pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecGoal']):
        pulumi.set(self, "goal", value)

    @property
    @pulumi.getter(name="metricId")
    def metric_id(self) -> pulumi.Input[str]:
        """
        The ID of the metric. Must not contain whitespaces.
        """
        return pulumi.get(self, "metric_id")

    @metric_id.setter
    def metric_id(self, value: pulumi.Input[str]):
        pulumi.set(self, "metric_id", value)


@pulumi.input_type
class GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecArgs:
    def __init__(__self__, *,
                 max_parallel_trial_count: pulumi.Input[int],
                 max_trial_count: pulumi.Input[int],
                 search_trial_job_spec: pulumi.Input['GoogleCloudAiplatformV1CustomJobSpecArgs'],
                 max_failed_trial_count: Optional[pulumi.Input[int]] = None):
        """
        Represent spec for search trials.
        :param pulumi.Input[int] max_parallel_trial_count: The maximum number of trials to run in parallel.
        :param pulumi.Input[int] max_trial_count: The maximum number of Neural Architecture Search (NAS) trials to run.
        :param pulumi.Input['GoogleCloudAiplatformV1CustomJobSpecArgs'] search_trial_job_spec: The spec of a search trial job. The same spec applies to all search trials.
        :param pulumi.Input[int] max_failed_trial_count: The number of failed trials that need to be seen before failing the NasJob. If set to 0, Vertex AI decides how many trials must fail before the whole job fails.
        """
        pulumi.set(__self__, "max_parallel_trial_count", max_parallel_trial_count)
        pulumi.set(__self__, "max_trial_count", max_trial_count)
        pulumi.set(__self__, "search_trial_job_spec", search_trial_job_spec)
        if max_failed_trial_count is not None:
            pulumi.set(__self__, "max_failed_trial_count", max_failed_trial_count)

    @property
    @pulumi.getter(name="maxParallelTrialCount")
    def max_parallel_trial_count(self) -> pulumi.Input[int]:
        """
        The maximum number of trials to run in parallel.
        """
        return pulumi.get(self, "max_parallel_trial_count")

    @max_parallel_trial_count.setter
    def max_parallel_trial_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "max_parallel_trial_count", value)

    @property
    @pulumi.getter(name="maxTrialCount")
    def max_trial_count(self) -> pulumi.Input[int]:
        """
        The maximum number of Neural Architecture Search (NAS) trials to run.
        """
        return pulumi.get(self, "max_trial_count")

    @max_trial_count.setter
    def max_trial_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "max_trial_count", value)

    @property
    @pulumi.getter(name="searchTrialJobSpec")
    def search_trial_job_spec(self) -> pulumi.Input['GoogleCloudAiplatformV1CustomJobSpecArgs']:
        """
        The spec of a search trial job. The same spec applies to all search trials.
        """
        return pulumi.get(self, "search_trial_job_spec")

    @search_trial_job_spec.setter
    def search_trial_job_spec(self, value: pulumi.Input['GoogleCloudAiplatformV1CustomJobSpecArgs']):
        pulumi.set(self, "search_trial_job_spec", value)

    @property
    @pulumi.getter(name="maxFailedTrialCount")
    def max_failed_trial_count(self) -> Optional[pulumi.Input[int]]:
        """
        The number of failed trials that need to be seen before failing the NasJob. If set to 0, Vertex AI decides how many trials must fail before the whole job fails.
        """
        return pulumi.get(self, "max_failed_trial_count")

    @max_failed_trial_count.setter
    def max_failed_trial_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_failed_trial_count", value)


@pulumi.input_type
class GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecArgs:
    def __init__(__self__, *,
                 frequency: pulumi.Input[int],
                 max_parallel_trial_count: pulumi.Input[int],
                 train_trial_job_spec: pulumi.Input['GoogleCloudAiplatformV1CustomJobSpecArgs']):
        """
        Represent spec for train trials.
        :param pulumi.Input[int] frequency: Frequency of search trials to start train stage. Top N [TrainTrialSpec.max_parallel_trial_count] search trials will be trained for every M [TrainTrialSpec.frequency] trials searched.
        :param pulumi.Input[int] max_parallel_trial_count: The maximum number of trials to run in parallel.
        :param pulumi.Input['GoogleCloudAiplatformV1CustomJobSpecArgs'] train_trial_job_spec: The spec of a train trial job. The same spec applies to all train trials.
        """
        pulumi.set(__self__, "frequency", frequency)
        pulumi.set(__self__, "max_parallel_trial_count", max_parallel_trial_count)
        pulumi.set(__self__, "train_trial_job_spec", train_trial_job_spec)

    @property
    @pulumi.getter
    def frequency(self) -> pulumi.Input[int]:
        """
        Frequency of search trials to start train stage. Top N [TrainTrialSpec.max_parallel_trial_count] search trials will be trained for every M [TrainTrialSpec.frequency] trials searched.
        """
        return pulumi.get(self, "frequency")

    @frequency.setter
    def frequency(self, value: pulumi.Input[int]):
        pulumi.set(self, "frequency", value)

    @property
    @pulumi.getter(name="maxParallelTrialCount")
    def max_parallel_trial_count(self) -> pulumi.Input[int]:
        """
        The maximum number of trials to run in parallel.
        """
        return pulumi.get(self, "max_parallel_trial_count")

    @max_parallel_trial_count.setter
    def max_parallel_trial_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "max_parallel_trial_count", value)

    @property
    @pulumi.getter(name="trainTrialJobSpec")
    def train_trial_job_spec(self) -> pulumi.Input['GoogleCloudAiplatformV1CustomJobSpecArgs']:
        """
        The spec of a train trial job. The same spec applies to all train trials.
        """
        return pulumi.get(self, "train_trial_job_spec")

    @train_trial_job_spec.setter
    def train_trial_job_spec(self, value: pulumi.Input['GoogleCloudAiplatformV1CustomJobSpecArgs']):
        pulumi.set(self, "train_trial_job_spec", value)


@pulumi.input_type
class GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecArgs:
    def __init__(__self__, *,
                 search_trial_spec: pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecArgs'],
                 metric: Optional[pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecArgs']] = None,
                 multi_trial_algorithm: Optional[pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMultiTrialAlgorithm']] = None,
                 train_trial_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecArgs']] = None):
        """
        The spec of multi-trial Neural Architecture Search (NAS).
        :param pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecArgs'] search_trial_spec: Spec for search trials.
        :param pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecArgs'] metric: Metric specs for the NAS job. Validation for this field is done at `multi_trial_algorithm_spec` field.
        :param pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMultiTrialAlgorithm'] multi_trial_algorithm: The multi-trial Neural Architecture Search (NAS) algorithm type. Defaults to `REINFORCEMENT_LEARNING`.
        :param pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecArgs'] train_trial_spec: Spec for train trials. Top N [TrainTrialSpec.max_parallel_trial_count] search trials will be trained for every M [TrainTrialSpec.frequency] trials searched.
        """
        pulumi.set(__self__, "search_trial_spec", search_trial_spec)
        if metric is not None:
            pulumi.set(__self__, "metric", metric)
        if multi_trial_algorithm is not None:
            pulumi.set(__self__, "multi_trial_algorithm", multi_trial_algorithm)
        if train_trial_spec is not None:
            pulumi.set(__self__, "train_trial_spec", train_trial_spec)

    @property
    @pulumi.getter(name="searchTrialSpec")
    def search_trial_spec(self) -> pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecArgs']:
        """
        Spec for search trials.
        """
        return pulumi.get(self, "search_trial_spec")

    @search_trial_spec.setter
    def search_trial_spec(self, value: pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecSearchTrialSpecArgs']):
        pulumi.set(self, "search_trial_spec", value)

    @property
    @pulumi.getter
    def metric(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecArgs']]:
        """
        Metric specs for the NAS job. Validation for this field is done at `multi_trial_algorithm_spec` field.
        """
        return pulumi.get(self, "metric")

    @metric.setter
    def metric(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMetricSpecArgs']]):
        pulumi.set(self, "metric", value)

    @property
    @pulumi.getter(name="multiTrialAlgorithm")
    def multi_trial_algorithm(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMultiTrialAlgorithm']]:
        """
        The multi-trial Neural Architecture Search (NAS) algorithm type. Defaults to `REINFORCEMENT_LEARNING`.
        """
        return pulumi.get(self, "multi_trial_algorithm")

    @multi_trial_algorithm.setter
    def multi_trial_algorithm(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecMultiTrialAlgorithm']]):
        pulumi.set(self, "multi_trial_algorithm", value)

    @property
    @pulumi.getter(name="trainTrialSpec")
    def train_trial_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecArgs']]:
        """
        Spec for train trials. Top N [TrainTrialSpec.max_parallel_trial_count] search trials will be trained for every M [TrainTrialSpec.frequency] trials searched.
        """
        return pulumi.get(self, "train_trial_spec")

    @train_trial_spec.setter
    def train_trial_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecTrainTrialSpecArgs']]):
        pulumi.set(self, "train_trial_spec", value)


@pulumi.input_type
class GoogleCloudAiplatformV1NasJobSpecArgs:
    def __init__(__self__, *,
                 multi_trial_algorithm_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecArgs']] = None,
                 resume_nas_job_id: Optional[pulumi.Input[str]] = None,
                 search_space_spec: Optional[pulumi.Input[str]] = None):
        """
        Represents the spec of a NasJob.
        :param pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecArgs'] multi_trial_algorithm_spec: The spec of multi-trial algorithms.
        :param pulumi.Input[str] resume_nas_job_id: The ID of the existing NasJob in the same Project and Location which will be used to resume search. search_space_spec and nas_algorithm_spec are obtained from previous NasJob hence should not provide them again for this NasJob.
        :param pulumi.Input[str] search_space_spec: It defines the search space for Neural Architecture Search (NAS).
        """
        if multi_trial_algorithm_spec is not None:
            pulumi.set(__self__, "multi_trial_algorithm_spec", multi_trial_algorithm_spec)
        if resume_nas_job_id is not None:
            pulumi.set(__self__, "resume_nas_job_id", resume_nas_job_id)
        if search_space_spec is not None:
            pulumi.set(__self__, "search_space_spec", search_space_spec)

    @property
    @pulumi.getter(name="multiTrialAlgorithmSpec")
    def multi_trial_algorithm_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecArgs']]:
        """
        The spec of multi-trial algorithms.
        """
        return pulumi.get(self, "multi_trial_algorithm_spec")

    @multi_trial_algorithm_spec.setter
    def multi_trial_algorithm_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1NasJobSpecMultiTrialAlgorithmSpecArgs']]):
        pulumi.set(self, "multi_trial_algorithm_spec", value)

    @property
    @pulumi.getter(name="resumeNasJobId")
    def resume_nas_job_id(self) -> Optional[pulumi.Input[str]]:
        """
        The ID of the existing NasJob in the same Project and Location which will be used to resume search. search_space_spec and nas_algorithm_spec are obtained from previous NasJob hence should not provide them again for this NasJob.
        """
        return pulumi.get(self, "resume_nas_job_id")

    @resume_nas_job_id.setter
    def resume_nas_job_id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "resume_nas_job_id", value)

    @property
    @pulumi.getter(name="searchSpaceSpec")
    def search_space_spec(self) -> Optional[pulumi.Input[str]]:
        """
        It defines the search space for Neural Architecture Search (NAS).
        """
        return pulumi.get(self, "search_space_spec")

    @search_space_spec.setter
    def search_space_spec(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "search_space_spec", value)


@pulumi.input_type
class GoogleCloudAiplatformV1NetworkSpecArgs:
    def __init__(__self__, *,
                 enable_internet_access: Optional[pulumi.Input[bool]] = None,
                 network: Optional[pulumi.Input[str]] = None,
                 subnetwork: Optional[pulumi.Input[str]] = None):
        """
        Network spec.
        :param pulumi.Input[bool] enable_internet_access: Whether to enable public internet access. Default false.
        :param pulumi.Input[str] network: The full name of the Google Compute Engine [network](https://cloud.google.com//compute/docs/networks-and-firewalls#networks)
        :param pulumi.Input[str] subnetwork: The name of the subnet that this instance is in. Format: `projects/{project_id_or_number}/regions/{region}/subnetworks/{subnetwork_id}`
        """
        if enable_internet_access is not None:
            pulumi.set(__self__, "enable_internet_access", enable_internet_access)
        if network is not None:
            pulumi.set(__self__, "network", network)
        if subnetwork is not None:
            pulumi.set(__self__, "subnetwork", subnetwork)

    @property
    @pulumi.getter(name="enableInternetAccess")
    def enable_internet_access(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether to enable public internet access. Default false.
        """
        return pulumi.get(self, "enable_internet_access")

    @enable_internet_access.setter
    def enable_internet_access(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enable_internet_access", value)

    @property
    @pulumi.getter
    def network(self) -> Optional[pulumi.Input[str]]:
        """
        The full name of the Google Compute Engine [network](https://cloud.google.com//compute/docs/networks-and-firewalls#networks)
        """
        return pulumi.get(self, "network")

    @network.setter
    def network(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "network", value)

    @property
    @pulumi.getter
    def subnetwork(self) -> Optional[pulumi.Input[str]]:
        """
        The name of the subnet that this instance is in. Format: `projects/{project_id_or_number}/regions/{region}/subnetworks/{subnetwork_id}`
        """
        return pulumi.get(self, "subnetwork")

    @subnetwork.setter
    def subnetwork(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "subnetwork", value)


@pulumi.input_type
class GoogleCloudAiplatformV1NfsMountArgs:
    def __init__(__self__, *,
                 mount_point: pulumi.Input[str],
                 path: pulumi.Input[str],
                 server: pulumi.Input[str]):
        """
        Represents a mount configuration for Network File System (NFS) to mount.
        :param pulumi.Input[str] mount_point: Destination mount path. The NFS will be mounted for the user under /mnt/nfs/
        :param pulumi.Input[str] path: Source path exported from NFS server. Has to start with '/', and combined with the ip address, it indicates the source mount path in the form of `server:path`
        :param pulumi.Input[str] server: IP address of the NFS server.
        """
        pulumi.set(__self__, "mount_point", mount_point)
        pulumi.set(__self__, "path", path)
        pulumi.set(__self__, "server", server)

    @property
    @pulumi.getter(name="mountPoint")
    def mount_point(self) -> pulumi.Input[str]:
        """
        Destination mount path. The NFS will be mounted for the user under /mnt/nfs/
        """
        return pulumi.get(self, "mount_point")

    @mount_point.setter
    def mount_point(self, value: pulumi.Input[str]):
        pulumi.set(self, "mount_point", value)

    @property
    @pulumi.getter
    def path(self) -> pulumi.Input[str]:
        """
        Source path exported from NFS server. Has to start with '/', and combined with the ip address, it indicates the source mount path in the form of `server:path`
        """
        return pulumi.get(self, "path")

    @path.setter
    def path(self, value: pulumi.Input[str]):
        pulumi.set(self, "path", value)

    @property
    @pulumi.getter
    def server(self) -> pulumi.Input[str]:
        """
        IP address of the NFS server.
        """
        return pulumi.get(self, "server")

    @server.setter
    def server(self, value: pulumi.Input[str]):
        pulumi.set(self, "server", value)


@pulumi.input_type
class GoogleCloudAiplatformV1NotebookEucConfigArgs:
    def __init__(__self__, *,
                 euc_disabled: Optional[pulumi.Input[bool]] = None):
        """
        The euc configuration of NotebookRuntimeTemplate.
        :param pulumi.Input[bool] euc_disabled: Input only. Whether EUC is disabled in this NotebookRuntimeTemplate. In proto3, the default value of a boolean is false. In this way, by default EUC will be enabled for NotebookRuntimeTemplate.
        """
        if euc_disabled is not None:
            pulumi.set(__self__, "euc_disabled", euc_disabled)

    @property
    @pulumi.getter(name="eucDisabled")
    def euc_disabled(self) -> Optional[pulumi.Input[bool]]:
        """
        Input only. Whether EUC is disabled in this NotebookRuntimeTemplate. In proto3, the default value of a boolean is false. In this way, by default EUC will be enabled for NotebookRuntimeTemplate.
        """
        return pulumi.get(self, "euc_disabled")

    @euc_disabled.setter
    def euc_disabled(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "euc_disabled", value)


@pulumi.input_type
class GoogleCloudAiplatformV1NotebookIdleShutdownConfigArgs:
    def __init__(__self__, *,
                 idle_timeout: pulumi.Input[str],
                 idle_shutdown_disabled: Optional[pulumi.Input[bool]] = None):
        """
        The idle shutdown configuration of NotebookRuntimeTemplate, which contains the idle_timeout as required field.
        :param pulumi.Input[str] idle_timeout: Duration is accurate to the second. In Notebook, Idle Timeout is accurate to minute so the range of idle_timeout (second) is: 10 * 60 ~ 1440 * 60.
        :param pulumi.Input[bool] idle_shutdown_disabled: Whether Idle Shutdown is disabled in this NotebookRuntimeTemplate.
        """
        pulumi.set(__self__, "idle_timeout", idle_timeout)
        if idle_shutdown_disabled is not None:
            pulumi.set(__self__, "idle_shutdown_disabled", idle_shutdown_disabled)

    @property
    @pulumi.getter(name="idleTimeout")
    def idle_timeout(self) -> pulumi.Input[str]:
        """
        Duration is accurate to the second. In Notebook, Idle Timeout is accurate to minute so the range of idle_timeout (second) is: 10 * 60 ~ 1440 * 60.
        """
        return pulumi.get(self, "idle_timeout")

    @idle_timeout.setter
    def idle_timeout(self, value: pulumi.Input[str]):
        pulumi.set(self, "idle_timeout", value)

    @property
    @pulumi.getter(name="idleShutdownDisabled")
    def idle_shutdown_disabled(self) -> Optional[pulumi.Input[bool]]:
        """
        Whether Idle Shutdown is disabled in this NotebookRuntimeTemplate.
        """
        return pulumi.get(self, "idle_shutdown_disabled")

    @idle_shutdown_disabled.setter
    def idle_shutdown_disabled(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "idle_shutdown_disabled", value)


@pulumi.input_type
class GoogleCloudAiplatformV1PersistentDiskSpecArgs:
    def __init__(__self__, *,
                 disk_size_gb: Optional[pulumi.Input[str]] = None,
                 disk_type: Optional[pulumi.Input[str]] = None):
        """
        Represents the spec of persistent disk options.
        :param pulumi.Input[str] disk_size_gb: Size in GB of the disk (default is 100GB).
        :param pulumi.Input[str] disk_type: Type of the disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) "pd-standard" (Persistent Disk Hard Disk Drive) "pd-balanced" (Balanced Persistent Disk) "pd-extreme" (Extreme Persistent Disk)
        """
        if disk_size_gb is not None:
            pulumi.set(__self__, "disk_size_gb", disk_size_gb)
        if disk_type is not None:
            pulumi.set(__self__, "disk_type", disk_type)

    @property
    @pulumi.getter(name="diskSizeGb")
    def disk_size_gb(self) -> Optional[pulumi.Input[str]]:
        """
        Size in GB of the disk (default is 100GB).
        """
        return pulumi.get(self, "disk_size_gb")

    @disk_size_gb.setter
    def disk_size_gb(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "disk_size_gb", value)

    @property
    @pulumi.getter(name="diskType")
    def disk_type(self) -> Optional[pulumi.Input[str]]:
        """
        Type of the disk (default is "pd-standard"). Valid values: "pd-ssd" (Persistent Disk Solid State Drive) "pd-standard" (Persistent Disk Hard Disk Drive) "pd-balanced" (Balanced Persistent Disk) "pd-extreme" (Extreme Persistent Disk)
        """
        return pulumi.get(self, "disk_type")

    @disk_type.setter
    def disk_type(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "disk_type", value)


@pulumi.input_type
class GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactArgs:
    def __init__(__self__, *,
                 artifact_id: Optional[pulumi.Input[str]] = None):
        """
        The type of an input artifact.
        :param pulumi.Input[str] artifact_id: Artifact resource id from MLMD. Which is the last portion of an artifact resource name: `projects/{project}/locations/{location}/metadataStores/default/artifacts/{artifact_id}`. The artifact must stay within the same project, location and default metadatastore as the pipeline.
        """
        if artifact_id is not None:
            pulumi.set(__self__, "artifact_id", artifact_id)

    @property
    @pulumi.getter(name="artifactId")
    def artifact_id(self) -> Optional[pulumi.Input[str]]:
        """
        Artifact resource id from MLMD. Which is the last portion of an artifact resource name: `projects/{project}/locations/{location}/metadataStores/default/artifacts/{artifact_id}`. The artifact must stay within the same project, location and default metadatastore as the pipeline.
        """
        return pulumi.get(self, "artifact_id")

    @artifact_id.setter
    def artifact_id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "artifact_id", value)


@pulumi.input_type
class GoogleCloudAiplatformV1PipelineJobRuntimeConfigArgs:
    def __init__(__self__, *,
                 gcs_output_directory: pulumi.Input[str],
                 failure_policy: Optional[pulumi.Input['GoogleCloudAiplatformV1PipelineJobRuntimeConfigFailurePolicy']] = None,
                 input_artifacts: Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactArgs']]]] = None,
                 parameter_values: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 parameters: Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ValueArgs']]]] = None):
        """
        The runtime config of a PipelineJob.
        :param pulumi.Input[str] gcs_output_directory: A path in a Cloud Storage bucket, which will be treated as the root output directory of the pipeline. It is used by the system to generate the paths of output artifacts. The artifact paths are generated with a sub-path pattern `{job_id}/{task_id}/{output_key}` under the specified output directory. The service account specified in this pipeline must have the `storage.objects.get` and `storage.objects.create` permissions for this bucket.
        :param pulumi.Input['GoogleCloudAiplatformV1PipelineJobRuntimeConfigFailurePolicy'] failure_policy: Represents the failure policy of a pipeline. Currently, the default of a pipeline is that the pipeline will continue to run until no more tasks can be executed, also known as PIPELINE_FAILURE_POLICY_FAIL_SLOW. However, if a pipeline is set to PIPELINE_FAILURE_POLICY_FAIL_FAST, it will stop scheduling any new tasks when a task has failed. Any scheduled tasks will continue to completion.
        :param pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactArgs']]] input_artifacts: The runtime artifacts of the PipelineJob. The key will be the input artifact name and the value would be one of the InputArtifact.
        :param pulumi.Input[Mapping[str, Any]] parameter_values: The runtime parameters of the PipelineJob. The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime. This field is used by pipelines built using `PipelineJob.pipeline_spec.schema_version` 2.1.0, such as pipelines built using Kubeflow Pipelines SDK 1.9 or higher and the v2 DSL.
        :param pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ValueArgs']]] parameters: Deprecated. Use RuntimeConfig.parameter_values instead. The runtime parameters of the PipelineJob. The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime. This field is used by pipelines built using `PipelineJob.pipeline_spec.schema_version` 2.0.0 or lower, such as pipelines built using Kubeflow Pipelines SDK 1.8 or lower.
        """
        pulumi.set(__self__, "gcs_output_directory", gcs_output_directory)
        if failure_policy is not None:
            pulumi.set(__self__, "failure_policy", failure_policy)
        if input_artifacts is not None:
            pulumi.set(__self__, "input_artifacts", input_artifacts)
        if parameter_values is not None:
            pulumi.set(__self__, "parameter_values", parameter_values)
        if parameters is not None:
            warnings.warn("""Deprecated. Use RuntimeConfig.parameter_values instead. The runtime parameters of the PipelineJob. The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime. This field is used by pipelines built using `PipelineJob.pipeline_spec.schema_version` 2.0.0 or lower, such as pipelines built using Kubeflow Pipelines SDK 1.8 or lower.""", DeprecationWarning)
            pulumi.log.warn("""parameters is deprecated: Deprecated. Use RuntimeConfig.parameter_values instead. The runtime parameters of the PipelineJob. The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime. This field is used by pipelines built using `PipelineJob.pipeline_spec.schema_version` 2.0.0 or lower, such as pipelines built using Kubeflow Pipelines SDK 1.8 or lower.""")
        if parameters is not None:
            pulumi.set(__self__, "parameters", parameters)

    @property
    @pulumi.getter(name="gcsOutputDirectory")
    def gcs_output_directory(self) -> pulumi.Input[str]:
        """
        A path in a Cloud Storage bucket, which will be treated as the root output directory of the pipeline. It is used by the system to generate the paths of output artifacts. The artifact paths are generated with a sub-path pattern `{job_id}/{task_id}/{output_key}` under the specified output directory. The service account specified in this pipeline must have the `storage.objects.get` and `storage.objects.create` permissions for this bucket.
        """
        return pulumi.get(self, "gcs_output_directory")

    @gcs_output_directory.setter
    def gcs_output_directory(self, value: pulumi.Input[str]):
        pulumi.set(self, "gcs_output_directory", value)

    @property
    @pulumi.getter(name="failurePolicy")
    def failure_policy(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1PipelineJobRuntimeConfigFailurePolicy']]:
        """
        Represents the failure policy of a pipeline. Currently, the default of a pipeline is that the pipeline will continue to run until no more tasks can be executed, also known as PIPELINE_FAILURE_POLICY_FAIL_SLOW. However, if a pipeline is set to PIPELINE_FAILURE_POLICY_FAIL_FAST, it will stop scheduling any new tasks when a task has failed. Any scheduled tasks will continue to completion.
        """
        return pulumi.get(self, "failure_policy")

    @failure_policy.setter
    def failure_policy(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1PipelineJobRuntimeConfigFailurePolicy']]):
        pulumi.set(self, "failure_policy", value)

    @property
    @pulumi.getter(name="inputArtifacts")
    def input_artifacts(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactArgs']]]]:
        """
        The runtime artifacts of the PipelineJob. The key will be the input artifact name and the value would be one of the InputArtifact.
        """
        return pulumi.get(self, "input_artifacts")

    @input_artifacts.setter
    def input_artifacts(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1PipelineJobRuntimeConfigInputArtifactArgs']]]]):
        pulumi.set(self, "input_artifacts", value)

    @property
    @pulumi.getter(name="parameterValues")
    def parameter_values(self) -> Optional[pulumi.Input[Mapping[str, Any]]]:
        """
        The runtime parameters of the PipelineJob. The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime. This field is used by pipelines built using `PipelineJob.pipeline_spec.schema_version` 2.1.0, such as pipelines built using Kubeflow Pipelines SDK 1.9 or higher and the v2 DSL.
        """
        return pulumi.get(self, "parameter_values")

    @parameter_values.setter
    def parameter_values(self, value: Optional[pulumi.Input[Mapping[str, Any]]]):
        pulumi.set(self, "parameter_values", value)

    @property
    @pulumi.getter
    @_utilities.deprecated("""Deprecated. Use RuntimeConfig.parameter_values instead. The runtime parameters of the PipelineJob. The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime. This field is used by pipelines built using `PipelineJob.pipeline_spec.schema_version` 2.0.0 or lower, such as pipelines built using Kubeflow Pipelines SDK 1.8 or lower.""")
    def parameters(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ValueArgs']]]]:
        """
        Deprecated. Use RuntimeConfig.parameter_values instead. The runtime parameters of the PipelineJob. The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime. This field is used by pipelines built using `PipelineJob.pipeline_spec.schema_version` 2.0.0 or lower, such as pipelines built using Kubeflow Pipelines SDK 1.8 or lower.
        """
        return pulumi.get(self, "parameters")

    @parameters.setter
    def parameters(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input['GoogleCloudAiplatformV1ValueArgs']]]]):
        pulumi.set(self, "parameters", value)


@pulumi.input_type
class GoogleCloudAiplatformV1PipelineJobArgs:
    def __init__(__self__, *,
                 display_name: Optional[pulumi.Input[str]] = None,
                 encryption_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1EncryptionSpecArgs']] = None,
                 labels: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]] = None,
                 network: Optional[pulumi.Input[str]] = None,
                 pipeline_spec: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 reserved_ip_ranges: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 runtime_config: Optional[pulumi.Input['GoogleCloudAiplatformV1PipelineJobRuntimeConfigArgs']] = None,
                 service_account: Optional[pulumi.Input[str]] = None,
                 template_uri: Optional[pulumi.Input[str]] = None):
        """
        An instance of a machine learning PipelineJob.
        :param pulumi.Input[str] display_name: The display name of the Pipeline. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        :param pulumi.Input['GoogleCloudAiplatformV1EncryptionSpecArgs'] encryption_spec: Customer-managed encryption key spec for a pipelineJob. If set, this PipelineJob and all of its sub-resources will be secured by this key.
        :param pulumi.Input[Mapping[str, pulumi.Input[str]]] labels: The labels with user-defined metadata to organize PipelineJob. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels. Note there is some reserved label key for Vertex AI Pipelines. - `vertex-ai-pipelines-run-billing-id`, user set value will get overrided.
        :param pulumi.Input[str] network: The full name of the Compute Engine [network](/compute/docs/networks-and-firewalls#networks) to which the Pipeline Job's workload should be peered. For example, `projects/12345/global/networks/myVPC`. [Format](/compute/docs/reference/rest/v1/networks/insert) is of the form `projects/{project}/global/networks/{network}`. Where {project} is a project number, as in `12345`, and {network} is a network name. Private services access must already be configured for the network. Pipeline job will apply the network configuration to the Google Cloud resources being launched, if applied, such as Vertex AI Training or Dataflow job. If left unspecified, the workload is not peered with any network.
        :param pulumi.Input[Mapping[str, Any]] pipeline_spec: The spec of the pipeline.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] reserved_ip_ranges: A list of names for the reserved ip ranges under the VPC network that can be used for this Pipeline Job's workload. If set, we will deploy the Pipeline Job's workload within the provided ip ranges. Otherwise, the job will be deployed to any ip ranges under the provided VPC network. Example: ['vertex-ai-ip-range'].
        :param pulumi.Input['GoogleCloudAiplatformV1PipelineJobRuntimeConfigArgs'] runtime_config: Runtime config of the pipeline.
        :param pulumi.Input[str] service_account: The service account that the pipeline workload runs as. If not specified, the Compute Engine default service account in the project will be used. See https://cloud.google.com/compute/docs/access/service-accounts#default_service_account Users starting the pipeline must have the `iam.serviceAccounts.actAs` permission on this service account.
        :param pulumi.Input[str] template_uri: A template uri from where the PipelineJob.pipeline_spec, if empty, will be downloaded. Currently, only uri from Vertex Template Registry & Gallery is supported. Reference to https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template.
        """
        if display_name is not None:
            pulumi.set(__self__, "display_name", display_name)
        if encryption_spec is not None:
            pulumi.set(__self__, "encryption_spec", encryption_spec)
        if labels is not None:
            pulumi.set(__self__, "labels", labels)
        if network is not None:
            pulumi.set(__self__, "network", network)
        if pipeline_spec is not None:
            pulumi.set(__self__, "pipeline_spec", pipeline_spec)
        if reserved_ip_ranges is not None:
            pulumi.set(__self__, "reserved_ip_ranges", reserved_ip_ranges)
        if runtime_config is not None:
            pulumi.set(__self__, "runtime_config", runtime_config)
        if service_account is not None:
            pulumi.set(__self__, "service_account", service_account)
        if template_uri is not None:
            pulumi.set(__self__, "template_uri", template_uri)

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> Optional[pulumi.Input[str]]:
        """
        The display name of the Pipeline. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        """
        return pulumi.get(self, "display_name")

    @display_name.setter
    def display_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "display_name", value)

    @property
    @pulumi.getter(name="encryptionSpec")
    def encryption_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1EncryptionSpecArgs']]:
        """
        Customer-managed encryption key spec for a pipelineJob. If set, this PipelineJob and all of its sub-resources will be secured by this key.
        """
        return pulumi.get(self, "encryption_spec")

    @encryption_spec.setter
    def encryption_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1EncryptionSpecArgs']]):
        pulumi.set(self, "encryption_spec", value)

    @property
    @pulumi.getter
    def labels(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]:
        """
        The labels with user-defined metadata to organize PipelineJob. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels. Note there is some reserved label key for Vertex AI Pipelines. - `vertex-ai-pipelines-run-billing-id`, user set value will get overrided.
        """
        return pulumi.get(self, "labels")

    @labels.setter
    def labels(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[str]]]]):
        pulumi.set(self, "labels", value)

    @property
    @pulumi.getter
    def network(self) -> Optional[pulumi.Input[str]]:
        """
        The full name of the Compute Engine [network](/compute/docs/networks-and-firewalls#networks) to which the Pipeline Job's workload should be peered. For example, `projects/12345/global/networks/myVPC`. [Format](/compute/docs/reference/rest/v1/networks/insert) is of the form `projects/{project}/global/networks/{network}`. Where {project} is a project number, as in `12345`, and {network} is a network name. Private services access must already be configured for the network. Pipeline job will apply the network configuration to the Google Cloud resources being launched, if applied, such as Vertex AI Training or Dataflow job. If left unspecified, the workload is not peered with any network.
        """
        return pulumi.get(self, "network")

    @network.setter
    def network(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "network", value)

    @property
    @pulumi.getter(name="pipelineSpec")
    def pipeline_spec(self) -> Optional[pulumi.Input[Mapping[str, Any]]]:
        """
        The spec of the pipeline.
        """
        return pulumi.get(self, "pipeline_spec")

    @pipeline_spec.setter
    def pipeline_spec(self, value: Optional[pulumi.Input[Mapping[str, Any]]]):
        pulumi.set(self, "pipeline_spec", value)

    @property
    @pulumi.getter(name="reservedIpRanges")
    def reserved_ip_ranges(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        A list of names for the reserved ip ranges under the VPC network that can be used for this Pipeline Job's workload. If set, we will deploy the Pipeline Job's workload within the provided ip ranges. Otherwise, the job will be deployed to any ip ranges under the provided VPC network. Example: ['vertex-ai-ip-range'].
        """
        return pulumi.get(self, "reserved_ip_ranges")

    @reserved_ip_ranges.setter
    def reserved_ip_ranges(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "reserved_ip_ranges", value)

    @property
    @pulumi.getter(name="runtimeConfig")
    def runtime_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1PipelineJobRuntimeConfigArgs']]:
        """
        Runtime config of the pipeline.
        """
        return pulumi.get(self, "runtime_config")

    @runtime_config.setter
    def runtime_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1PipelineJobRuntimeConfigArgs']]):
        pulumi.set(self, "runtime_config", value)

    @property
    @pulumi.getter(name="serviceAccount")
    def service_account(self) -> Optional[pulumi.Input[str]]:
        """
        The service account that the pipeline workload runs as. If not specified, the Compute Engine default service account in the project will be used. See https://cloud.google.com/compute/docs/access/service-accounts#default_service_account Users starting the pipeline must have the `iam.serviceAccounts.actAs` permission on this service account.
        """
        return pulumi.get(self, "service_account")

    @service_account.setter
    def service_account(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "service_account", value)

    @property
    @pulumi.getter(name="templateUri")
    def template_uri(self) -> Optional[pulumi.Input[str]]:
        """
        A template uri from where the PipelineJob.pipeline_spec, if empty, will be downloaded. Currently, only uri from Vertex Template Registry & Gallery is supported. Reference to https://cloud.google.com/vertex-ai/docs/pipelines/create-pipeline-template.
        """
        return pulumi.get(self, "template_uri")

    @template_uri.setter
    def template_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "template_uri", value)


@pulumi.input_type
class GoogleCloudAiplatformV1PortArgs:
    def __init__(__self__, *,
                 container_port: Optional[pulumi.Input[int]] = None):
        """
        Represents a network port in a container.
        :param pulumi.Input[int] container_port: The number of the port to expose on the pod's IP address. Must be a valid port number, between 1 and 65535 inclusive.
        """
        if container_port is not None:
            pulumi.set(__self__, "container_port", container_port)

    @property
    @pulumi.getter(name="containerPort")
    def container_port(self) -> Optional[pulumi.Input[int]]:
        """
        The number of the port to expose on the pod's IP address. Must be a valid port number, between 1 and 65535 inclusive.
        """
        return pulumi.get(self, "container_port")

    @container_port.setter
    def container_port(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "container_port", value)


@pulumi.input_type
class GoogleCloudAiplatformV1PredefinedSplitArgs:
    def __init__(__self__, *,
                 key: pulumi.Input[str]):
        """
        Assigns input data to training, validation, and test sets based on the value of a provided key. Supported only for tabular Datasets.
        :param pulumi.Input[str] key: The key is a name of one of the Dataset's data columns. The value of the key (either the label's value or value in the column) must be one of {`training`, `validation`, `test`}, and it defines to which set the given piece of data is assigned. If for a piece of data the key is not present or has an invalid value, that piece is ignored by the pipeline.
        """
        pulumi.set(__self__, "key", key)

    @property
    @pulumi.getter
    def key(self) -> pulumi.Input[str]:
        """
        The key is a name of one of the Dataset's data columns. The value of the key (either the label's value or value in the column) must be one of {`training`, `validation`, `test`}, and it defines to which set the given piece of data is assigned. If for a piece of data the key is not present or has an invalid value, that piece is ignored by the pipeline.
        """
        return pulumi.get(self, "key")

    @key.setter
    def key(self, value: pulumi.Input[str]):
        pulumi.set(self, "key", value)


@pulumi.input_type
class GoogleCloudAiplatformV1PredictRequestResponseLoggingConfigArgs:
    def __init__(__self__, *,
                 bigquery_destination: Optional[pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs']] = None,
                 enabled: Optional[pulumi.Input[bool]] = None,
                 sampling_rate: Optional[pulumi.Input[float]] = None):
        """
        Configuration for logging request-response to a BigQuery table.
        :param pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs'] bigquery_destination: BigQuery table for logging. If only given a project, a new dataset will be created with name `logging__` where will be made BigQuery-dataset-name compatible (e.g. most special characters will become underscores). If no table name is given, a new table will be created with name `request_response_logging`
        :param pulumi.Input[bool] enabled: If logging is enabled or not.
        :param pulumi.Input[float] sampling_rate: Percentage of requests to be logged, expressed as a fraction in range(0,1].
        """
        if bigquery_destination is not None:
            pulumi.set(__self__, "bigquery_destination", bigquery_destination)
        if enabled is not None:
            pulumi.set(__self__, "enabled", enabled)
        if sampling_rate is not None:
            pulumi.set(__self__, "sampling_rate", sampling_rate)

    @property
    @pulumi.getter(name="bigqueryDestination")
    def bigquery_destination(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs']]:
        """
        BigQuery table for logging. If only given a project, a new dataset will be created with name `logging__` where will be made BigQuery-dataset-name compatible (e.g. most special characters will become underscores). If no table name is given, a new table will be created with name `request_response_logging`
        """
        return pulumi.get(self, "bigquery_destination")

    @bigquery_destination.setter
    def bigquery_destination(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1BigQueryDestinationArgs']]):
        pulumi.set(self, "bigquery_destination", value)

    @property
    @pulumi.getter
    def enabled(self) -> Optional[pulumi.Input[bool]]:
        """
        If logging is enabled or not.
        """
        return pulumi.get(self, "enabled")

    @enabled.setter
    def enabled(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "enabled", value)

    @property
    @pulumi.getter(name="samplingRate")
    def sampling_rate(self) -> Optional[pulumi.Input[float]]:
        """
        Percentage of requests to be logged, expressed as a fraction in range(0,1].
        """
        return pulumi.get(self, "sampling_rate")

    @sampling_rate.setter
    def sampling_rate(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "sampling_rate", value)


@pulumi.input_type
class GoogleCloudAiplatformV1PredictSchemataArgs:
    def __init__(__self__, *,
                 instance_schema_uri: Optional[pulumi.Input[str]] = None,
                 parameters_schema_uri: Optional[pulumi.Input[str]] = None,
                 prediction_schema_uri: Optional[pulumi.Input[str]] = None):
        """
        Contains the schemata used in Model's predictions and explanations via PredictionService.Predict, PredictionService.Explain and BatchPredictionJob.
        :param pulumi.Input[str] instance_schema_uri: Immutable. Points to a YAML file stored on Google Cloud Storage describing the format of a single instance, which are used in PredictRequest.instances, ExplainRequest.instances and BatchPredictionJob.input_config. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        :param pulumi.Input[str] parameters_schema_uri: Immutable. Points to a YAML file stored on Google Cloud Storage describing the parameters of prediction and explanation via PredictRequest.parameters, ExplainRequest.parameters and BatchPredictionJob.model_parameters. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI, if no parameters are supported, then it is set to an empty string. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        :param pulumi.Input[str] prediction_schema_uri: Immutable. Points to a YAML file stored on Google Cloud Storage describing the format of a single prediction produced by this Model, which are returned via PredictResponse.predictions, ExplainResponse.explanations, and BatchPredictionJob.output_config. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        """
        if instance_schema_uri is not None:
            pulumi.set(__self__, "instance_schema_uri", instance_schema_uri)
        if parameters_schema_uri is not None:
            pulumi.set(__self__, "parameters_schema_uri", parameters_schema_uri)
        if prediction_schema_uri is not None:
            pulumi.set(__self__, "prediction_schema_uri", prediction_schema_uri)

    @property
    @pulumi.getter(name="instanceSchemaUri")
    def instance_schema_uri(self) -> Optional[pulumi.Input[str]]:
        """
        Immutable. Points to a YAML file stored on Google Cloud Storage describing the format of a single instance, which are used in PredictRequest.instances, ExplainRequest.instances and BatchPredictionJob.input_config. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        """
        return pulumi.get(self, "instance_schema_uri")

    @instance_schema_uri.setter
    def instance_schema_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "instance_schema_uri", value)

    @property
    @pulumi.getter(name="parametersSchemaUri")
    def parameters_schema_uri(self) -> Optional[pulumi.Input[str]]:
        """
        Immutable. Points to a YAML file stored on Google Cloud Storage describing the parameters of prediction and explanation via PredictRequest.parameters, ExplainRequest.parameters and BatchPredictionJob.model_parameters. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI, if no parameters are supported, then it is set to an empty string. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        """
        return pulumi.get(self, "parameters_schema_uri")

    @parameters_schema_uri.setter
    def parameters_schema_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "parameters_schema_uri", value)

    @property
    @pulumi.getter(name="predictionSchemaUri")
    def prediction_schema_uri(self) -> Optional[pulumi.Input[str]]:
        """
        Immutable. Points to a YAML file stored on Google Cloud Storage describing the format of a single prediction produced by this Model, which are returned via PredictResponse.predictions, ExplainResponse.explanations, and BatchPredictionJob.output_config. The schema is defined as an OpenAPI 3.0.2 [Schema Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject). AutoML Models always have this field populated by Vertex AI. Note: The URI given on output will be immutable and probably different, including the URI scheme, than the one given on input. The output URI will point to a location where the user only has a read access.
        """
        return pulumi.get(self, "prediction_schema_uri")

    @prediction_schema_uri.setter
    def prediction_schema_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "prediction_schema_uri", value)


@pulumi.input_type
class GoogleCloudAiplatformV1PresetsArgs:
    def __init__(__self__, *,
                 modality: Optional[pulumi.Input['GoogleCloudAiplatformV1PresetsModality']] = None,
                 query: Optional[pulumi.Input['GoogleCloudAiplatformV1PresetsQuery']] = None):
        """
        Preset configuration for example-based explanations
        :param pulumi.Input['GoogleCloudAiplatformV1PresetsModality'] modality: The modality of the uploaded model, which automatically configures the distance measurement and feature normalization for the underlying example index and queries. If your model does not precisely fit one of these types, it is okay to choose the closest type.
        :param pulumi.Input['GoogleCloudAiplatformV1PresetsQuery'] query: Preset option controlling parameters for speed-precision trade-off when querying for examples. If omitted, defaults to `PRECISE`.
        """
        if modality is not None:
            pulumi.set(__self__, "modality", modality)
        if query is not None:
            pulumi.set(__self__, "query", query)

    @property
    @pulumi.getter
    def modality(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1PresetsModality']]:
        """
        The modality of the uploaded model, which automatically configures the distance measurement and feature normalization for the underlying example index and queries. If your model does not precisely fit one of these types, it is okay to choose the closest type.
        """
        return pulumi.get(self, "modality")

    @modality.setter
    def modality(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1PresetsModality']]):
        pulumi.set(self, "modality", value)

    @property
    @pulumi.getter
    def query(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1PresetsQuery']]:
        """
        Preset option controlling parameters for speed-precision trade-off when querying for examples. If omitted, defaults to `PRECISE`.
        """
        return pulumi.get(self, "query")

    @query.setter
    def query(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1PresetsQuery']]):
        pulumi.set(self, "query", value)


@pulumi.input_type
class GoogleCloudAiplatformV1PrivateServiceConnectConfigArgs:
    def __init__(__self__, *,
                 enable_private_service_connect: pulumi.Input[bool],
                 project_allowlist: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        Represents configuration for private service connect.
        :param pulumi.Input[bool] enable_private_service_connect: If true, expose the IndexEndpoint via private service connect.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] project_allowlist: A list of Projects from which the forwarding rule will target the service attachment.
        """
        pulumi.set(__self__, "enable_private_service_connect", enable_private_service_connect)
        if project_allowlist is not None:
            pulumi.set(__self__, "project_allowlist", project_allowlist)

    @property
    @pulumi.getter(name="enablePrivateServiceConnect")
    def enable_private_service_connect(self) -> pulumi.Input[bool]:
        """
        If true, expose the IndexEndpoint via private service connect.
        """
        return pulumi.get(self, "enable_private_service_connect")

    @enable_private_service_connect.setter
    def enable_private_service_connect(self, value: pulumi.Input[bool]):
        pulumi.set(self, "enable_private_service_connect", value)

    @property
    @pulumi.getter(name="projectAllowlist")
    def project_allowlist(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        A list of Projects from which the forwarding rule will target the service attachment.
        """
        return pulumi.get(self, "project_allowlist")

    @project_allowlist.setter
    def project_allowlist(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "project_allowlist", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ProbeExecActionArgs:
    def __init__(__self__, *,
                 command: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None):
        """
        ExecAction specifies a command to execute.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] command: Command is the command line to execute inside the container, the working directory for the command is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
        """
        if command is not None:
            pulumi.set(__self__, "command", command)

    @property
    @pulumi.getter
    def command(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Command is the command line to execute inside the container, the working directory for the command is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy.
        """
        return pulumi.get(self, "command")

    @command.setter
    def command(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "command", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ProbeArgs:
    def __init__(__self__, *,
                 exec_: Optional[pulumi.Input['GoogleCloudAiplatformV1ProbeExecActionArgs']] = None,
                 period_seconds: Optional[pulumi.Input[int]] = None,
                 timeout_seconds: Optional[pulumi.Input[int]] = None):
        """
        Probe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic.
        :param pulumi.Input['GoogleCloudAiplatformV1ProbeExecActionArgs'] exec_: Exec specifies the action to take.
        :param pulumi.Input[int] period_seconds: How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Must be less than timeout_seconds. Maps to Kubernetes probe argument 'periodSeconds'.
        :param pulumi.Input[int] timeout_seconds: Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Must be greater or equal to period_seconds. Maps to Kubernetes probe argument 'timeoutSeconds'.
        """
        if exec_ is not None:
            pulumi.set(__self__, "exec_", exec_)
        if period_seconds is not None:
            pulumi.set(__self__, "period_seconds", period_seconds)
        if timeout_seconds is not None:
            pulumi.set(__self__, "timeout_seconds", timeout_seconds)

    @property
    @pulumi.getter(name="exec")
    def exec_(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ProbeExecActionArgs']]:
        """
        Exec specifies the action to take.
        """
        return pulumi.get(self, "exec_")

    @exec_.setter
    def exec_(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ProbeExecActionArgs']]):
        pulumi.set(self, "exec_", value)

    @property
    @pulumi.getter(name="periodSeconds")
    def period_seconds(self) -> Optional[pulumi.Input[int]]:
        """
        How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Must be less than timeout_seconds. Maps to Kubernetes probe argument 'periodSeconds'.
        """
        return pulumi.get(self, "period_seconds")

    @period_seconds.setter
    def period_seconds(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "period_seconds", value)

    @property
    @pulumi.getter(name="timeoutSeconds")
    def timeout_seconds(self) -> Optional[pulumi.Input[int]]:
        """
        Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Must be greater or equal to period_seconds. Maps to Kubernetes probe argument 'timeoutSeconds'.
        """
        return pulumi.get(self, "timeout_seconds")

    @timeout_seconds.setter
    def timeout_seconds(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "timeout_seconds", value)


@pulumi.input_type
class GoogleCloudAiplatformV1PythonPackageSpecArgs:
    def __init__(__self__, *,
                 executor_image_uri: pulumi.Input[str],
                 package_uris: pulumi.Input[Sequence[pulumi.Input[str]]],
                 python_module: pulumi.Input[str],
                 args: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 env: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1EnvVarArgs']]]] = None):
        """
        The spec of a Python packaged code.
        :param pulumi.Input[str] executor_image_uri: The URI of a container image in Artifact Registry that will run the provided Python package. Vertex AI provides a wide range of executor images with pre-installed packages to meet users' various use cases. See the list of [pre-built containers for training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers). You must use an image from this list.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] package_uris: The Google Cloud Storage location of the Python package files which are the training program and its dependent packages. The maximum number of package URIs is 100.
        :param pulumi.Input[str] python_module: The Python module name to run after installing the packages.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] args: Command line arguments to be passed to the Python task.
        :param pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1EnvVarArgs']]] env: Environment variables to be passed to the python module. Maximum limit is 100.
        """
        pulumi.set(__self__, "executor_image_uri", executor_image_uri)
        pulumi.set(__self__, "package_uris", package_uris)
        pulumi.set(__self__, "python_module", python_module)
        if args is not None:
            pulumi.set(__self__, "args", args)
        if env is not None:
            pulumi.set(__self__, "env", env)

    @property
    @pulumi.getter(name="executorImageUri")
    def executor_image_uri(self) -> pulumi.Input[str]:
        """
        The URI of a container image in Artifact Registry that will run the provided Python package. Vertex AI provides a wide range of executor images with pre-installed packages to meet users' various use cases. See the list of [pre-built containers for training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers). You must use an image from this list.
        """
        return pulumi.get(self, "executor_image_uri")

    @executor_image_uri.setter
    def executor_image_uri(self, value: pulumi.Input[str]):
        pulumi.set(self, "executor_image_uri", value)

    @property
    @pulumi.getter(name="packageUris")
    def package_uris(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        The Google Cloud Storage location of the Python package files which are the training program and its dependent packages. The maximum number of package URIs is 100.
        """
        return pulumi.get(self, "package_uris")

    @package_uris.setter
    def package_uris(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "package_uris", value)

    @property
    @pulumi.getter(name="pythonModule")
    def python_module(self) -> pulumi.Input[str]:
        """
        The Python module name to run after installing the packages.
        """
        return pulumi.get(self, "python_module")

    @python_module.setter
    def python_module(self, value: pulumi.Input[str]):
        pulumi.set(self, "python_module", value)

    @property
    @pulumi.getter
    def args(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Command line arguments to be passed to the Python task.
        """
        return pulumi.get(self, "args")

    @args.setter
    def args(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "args", value)

    @property
    @pulumi.getter
    def env(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1EnvVarArgs']]]]:
        """
        Environment variables to be passed to the python module. Maximum limit is 100.
        """
        return pulumi.get(self, "env")

    @env.setter
    def env(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1EnvVarArgs']]]]):
        pulumi.set(self, "env", value)


@pulumi.input_type
class GoogleCloudAiplatformV1SampleConfigArgs:
    def __init__(__self__, *,
                 following_batch_sample_percentage: Optional[pulumi.Input[int]] = None,
                 initial_batch_sample_percentage: Optional[pulumi.Input[int]] = None,
                 sample_strategy: Optional[pulumi.Input['GoogleCloudAiplatformV1SampleConfigSampleStrategy']] = None):
        """
        Active learning data sampling config. For every active learning labeling iteration, it will select a batch of data based on the sampling strategy.
        :param pulumi.Input[int] following_batch_sample_percentage: The percentage of data needed to be labeled in each following batch (except the first batch).
        :param pulumi.Input[int] initial_batch_sample_percentage: The percentage of data needed to be labeled in the first batch.
        :param pulumi.Input['GoogleCloudAiplatformV1SampleConfigSampleStrategy'] sample_strategy: Field to choose sampling strategy. Sampling strategy will decide which data should be selected for human labeling in every batch.
        """
        if following_batch_sample_percentage is not None:
            pulumi.set(__self__, "following_batch_sample_percentage", following_batch_sample_percentage)
        if initial_batch_sample_percentage is not None:
            pulumi.set(__self__, "initial_batch_sample_percentage", initial_batch_sample_percentage)
        if sample_strategy is not None:
            pulumi.set(__self__, "sample_strategy", sample_strategy)

    @property
    @pulumi.getter(name="followingBatchSamplePercentage")
    def following_batch_sample_percentage(self) -> Optional[pulumi.Input[int]]:
        """
        The percentage of data needed to be labeled in each following batch (except the first batch).
        """
        return pulumi.get(self, "following_batch_sample_percentage")

    @following_batch_sample_percentage.setter
    def following_batch_sample_percentage(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "following_batch_sample_percentage", value)

    @property
    @pulumi.getter(name="initialBatchSamplePercentage")
    def initial_batch_sample_percentage(self) -> Optional[pulumi.Input[int]]:
        """
        The percentage of data needed to be labeled in the first batch.
        """
        return pulumi.get(self, "initial_batch_sample_percentage")

    @initial_batch_sample_percentage.setter
    def initial_batch_sample_percentage(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "initial_batch_sample_percentage", value)

    @property
    @pulumi.getter(name="sampleStrategy")
    def sample_strategy(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1SampleConfigSampleStrategy']]:
        """
        Field to choose sampling strategy. Sampling strategy will decide which data should be selected for human labeling in every batch.
        """
        return pulumi.get(self, "sample_strategy")

    @sample_strategy.setter
    def sample_strategy(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1SampleConfigSampleStrategy']]):
        pulumi.set(self, "sample_strategy", value)


@pulumi.input_type
class GoogleCloudAiplatformV1SampledShapleyAttributionArgs:
    def __init__(__self__, *,
                 path_count: pulumi.Input[int]):
        """
        An attribution method that approximates Shapley values for features that contribute to the label being predicted. A sampling strategy is used to approximate the value rather than considering all subsets of features.
        :param pulumi.Input[int] path_count: The number of feature permutations to consider when approximating the Shapley values. Valid range of its value is [1, 50], inclusively.
        """
        pulumi.set(__self__, "path_count", path_count)

    @property
    @pulumi.getter(name="pathCount")
    def path_count(self) -> pulumi.Input[int]:
        """
        The number of feature permutations to consider when approximating the Shapley values. Valid range of its value is [1, 50], inclusively.
        """
        return pulumi.get(self, "path_count")

    @path_count.setter
    def path_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "path_count", value)


@pulumi.input_type
class GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigArgs:
    def __init__(__self__, *,
                 sample_rate: Optional[pulumi.Input[float]] = None):
        """
        Requests are randomly selected.
        :param pulumi.Input[float] sample_rate: Sample rate (0, 1]
        """
        if sample_rate is not None:
            pulumi.set(__self__, "sample_rate", sample_rate)

    @property
    @pulumi.getter(name="sampleRate")
    def sample_rate(self) -> Optional[pulumi.Input[float]]:
        """
        Sample rate (0, 1]
        """
        return pulumi.get(self, "sample_rate")

    @sample_rate.setter
    def sample_rate(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "sample_rate", value)


@pulumi.input_type
class GoogleCloudAiplatformV1SamplingStrategyArgs:
    def __init__(__self__, *,
                 random_sample_config: Optional[pulumi.Input['GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigArgs']] = None):
        """
        Sampling Strategy for logging, can be for both training and prediction dataset.
        :param pulumi.Input['GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigArgs'] random_sample_config: Random sample config. Will support more sampling strategies later.
        """
        if random_sample_config is not None:
            pulumi.set(__self__, "random_sample_config", random_sample_config)

    @property
    @pulumi.getter(name="randomSampleConfig")
    def random_sample_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigArgs']]:
        """
        Random sample config. Will support more sampling strategies later.
        """
        return pulumi.get(self, "random_sample_config")

    @random_sample_config.setter
    def random_sample_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1SamplingStrategyRandomSampleConfigArgs']]):
        pulumi.set(self, "random_sample_config", value)


@pulumi.input_type
class GoogleCloudAiplatformV1SavedQueryArgs:
    def __init__(__self__, *,
                 display_name: pulumi.Input[str],
                 problem_type: pulumi.Input[str],
                 etag: Optional[pulumi.Input[str]] = None,
                 metadata: Optional[Any] = None):
        """
        A SavedQuery is a view of the dataset. It references a subset of annotations by problem type and filters.
        :param pulumi.Input[str] display_name: The user-defined name of the SavedQuery. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        :param pulumi.Input[str] problem_type: Problem type of the SavedQuery. Allowed values: * IMAGE_CLASSIFICATION_SINGLE_LABEL * IMAGE_CLASSIFICATION_MULTI_LABEL * IMAGE_BOUNDING_POLY * IMAGE_BOUNDING_BOX * TEXT_CLASSIFICATION_SINGLE_LABEL * TEXT_CLASSIFICATION_MULTI_LABEL * TEXT_EXTRACTION * TEXT_SENTIMENT * VIDEO_CLASSIFICATION * VIDEO_OBJECT_TRACKING
        :param pulumi.Input[str] etag: Used to perform a consistent read-modify-write update. If not set, a blind "overwrite" update happens.
        :param Any metadata: Some additional information about the SavedQuery.
        """
        pulumi.set(__self__, "display_name", display_name)
        pulumi.set(__self__, "problem_type", problem_type)
        if etag is not None:
            pulumi.set(__self__, "etag", etag)
        if metadata is not None:
            pulumi.set(__self__, "metadata", metadata)

    @property
    @pulumi.getter(name="displayName")
    def display_name(self) -> pulumi.Input[str]:
        """
        The user-defined name of the SavedQuery. The name can be up to 128 characters long and can consist of any UTF-8 characters.
        """
        return pulumi.get(self, "display_name")

    @display_name.setter
    def display_name(self, value: pulumi.Input[str]):
        pulumi.set(self, "display_name", value)

    @property
    @pulumi.getter(name="problemType")
    def problem_type(self) -> pulumi.Input[str]:
        """
        Problem type of the SavedQuery. Allowed values: * IMAGE_CLASSIFICATION_SINGLE_LABEL * IMAGE_CLASSIFICATION_MULTI_LABEL * IMAGE_BOUNDING_POLY * IMAGE_BOUNDING_BOX * TEXT_CLASSIFICATION_SINGLE_LABEL * TEXT_CLASSIFICATION_MULTI_LABEL * TEXT_EXTRACTION * TEXT_SENTIMENT * VIDEO_CLASSIFICATION * VIDEO_OBJECT_TRACKING
        """
        return pulumi.get(self, "problem_type")

    @problem_type.setter
    def problem_type(self, value: pulumi.Input[str]):
        pulumi.set(self, "problem_type", value)

    @property
    @pulumi.getter
    def etag(self) -> Optional[pulumi.Input[str]]:
        """
        Used to perform a consistent read-modify-write update. If not set, a blind "overwrite" update happens.
        """
        return pulumi.get(self, "etag")

    @etag.setter
    def etag(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "etag", value)

    @property
    @pulumi.getter
    def metadata(self) -> Optional[Any]:
        """
        Some additional information about the SavedQuery.
        """
        return pulumi.get(self, "metadata")

    @metadata.setter
    def metadata(self, value: Optional[Any]):
        pulumi.set(self, "metadata", value)


@pulumi.input_type
class GoogleCloudAiplatformV1SchedulingArgs:
    def __init__(__self__, *,
                 disable_retries: Optional[pulumi.Input[bool]] = None,
                 restart_job_on_worker_restart: Optional[pulumi.Input[bool]] = None,
                 timeout: Optional[pulumi.Input[str]] = None):
        """
        All parameters related to queuing and scheduling of custom jobs.
        :param pulumi.Input[bool] disable_retries: Optional. Indicates if the job should retry for internal errors after the job starts running. If true, overrides `Scheduling.restart_job_on_worker_restart` to false.
        :param pulumi.Input[bool] restart_job_on_worker_restart: Restarts the entire CustomJob if a worker gets restarted. This feature can be used by distributed training jobs that are not resilient to workers leaving and joining a job.
        :param pulumi.Input[str] timeout: The maximum job running time. The default is 7 days.
        """
        if disable_retries is not None:
            pulumi.set(__self__, "disable_retries", disable_retries)
        if restart_job_on_worker_restart is not None:
            pulumi.set(__self__, "restart_job_on_worker_restart", restart_job_on_worker_restart)
        if timeout is not None:
            pulumi.set(__self__, "timeout", timeout)

    @property
    @pulumi.getter(name="disableRetries")
    def disable_retries(self) -> Optional[pulumi.Input[bool]]:
        """
        Optional. Indicates if the job should retry for internal errors after the job starts running. If true, overrides `Scheduling.restart_job_on_worker_restart` to false.
        """
        return pulumi.get(self, "disable_retries")

    @disable_retries.setter
    def disable_retries(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "disable_retries", value)

    @property
    @pulumi.getter(name="restartJobOnWorkerRestart")
    def restart_job_on_worker_restart(self) -> Optional[pulumi.Input[bool]]:
        """
        Restarts the entire CustomJob if a worker gets restarted. This feature can be used by distributed training jobs that are not resilient to workers leaving and joining a job.
        """
        return pulumi.get(self, "restart_job_on_worker_restart")

    @restart_job_on_worker_restart.setter
    def restart_job_on_worker_restart(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "restart_job_on_worker_restart", value)

    @property
    @pulumi.getter
    def timeout(self) -> Optional[pulumi.Input[str]]:
        """
        The maximum job running time. The default is 7 days.
        """
        return pulumi.get(self, "timeout")

    @timeout.setter
    def timeout(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "timeout", value)


@pulumi.input_type
class GoogleCloudAiplatformV1SmoothGradConfigArgs:
    def __init__(__self__, *,
                 feature_noise_sigma: Optional[pulumi.Input['GoogleCloudAiplatformV1FeatureNoiseSigmaArgs']] = None,
                 noise_sigma: Optional[pulumi.Input[float]] = None,
                 noisy_sample_count: Optional[pulumi.Input[int]] = None):
        """
        Config for SmoothGrad approximation of gradients. When enabled, the gradients are approximated by averaging the gradients from noisy samples in the vicinity of the inputs. Adding noise can help improve the computed gradients. Refer to this paper for more details: https://arxiv.org/pdf/1706.03825.pdf
        :param pulumi.Input['GoogleCloudAiplatformV1FeatureNoiseSigmaArgs'] feature_noise_sigma: This is similar to noise_sigma, but provides additional flexibility. A separate noise sigma can be provided for each feature, which is useful if their distributions are different. No noise is added to features that are not set. If this field is unset, noise_sigma will be used for all features.
        :param pulumi.Input[float] noise_sigma: This is a single float value and will be used to add noise to all the features. Use this field when all features are normalized to have the same distribution: scale to range [0, 1], [-1, 1] or z-scoring, where features are normalized to have 0-mean and 1-variance. Learn more about [normalization](https://developers.google.com/machine-learning/data-prep/transform/normalization). For best results the recommended value is about 10% - 20% of the standard deviation of the input feature. Refer to section 3.2 of the SmoothGrad paper: https://arxiv.org/pdf/1706.03825.pdf. Defaults to 0.1. If the distribution is different per feature, set feature_noise_sigma instead for each feature.
        :param pulumi.Input[int] noisy_sample_count: The number of gradient samples to use for approximation. The higher this number, the more accurate the gradient is, but the runtime complexity increases by this factor as well. Valid range of its value is [1, 50]. Defaults to 3.
        """
        if feature_noise_sigma is not None:
            pulumi.set(__self__, "feature_noise_sigma", feature_noise_sigma)
        if noise_sigma is not None:
            pulumi.set(__self__, "noise_sigma", noise_sigma)
        if noisy_sample_count is not None:
            pulumi.set(__self__, "noisy_sample_count", noisy_sample_count)

    @property
    @pulumi.getter(name="featureNoiseSigma")
    def feature_noise_sigma(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1FeatureNoiseSigmaArgs']]:
        """
        This is similar to noise_sigma, but provides additional flexibility. A separate noise sigma can be provided for each feature, which is useful if their distributions are different. No noise is added to features that are not set. If this field is unset, noise_sigma will be used for all features.
        """
        return pulumi.get(self, "feature_noise_sigma")

    @feature_noise_sigma.setter
    def feature_noise_sigma(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1FeatureNoiseSigmaArgs']]):
        pulumi.set(self, "feature_noise_sigma", value)

    @property
    @pulumi.getter(name="noiseSigma")
    def noise_sigma(self) -> Optional[pulumi.Input[float]]:
        """
        This is a single float value and will be used to add noise to all the features. Use this field when all features are normalized to have the same distribution: scale to range [0, 1], [-1, 1] or z-scoring, where features are normalized to have 0-mean and 1-variance. Learn more about [normalization](https://developers.google.com/machine-learning/data-prep/transform/normalization). For best results the recommended value is about 10% - 20% of the standard deviation of the input feature. Refer to section 3.2 of the SmoothGrad paper: https://arxiv.org/pdf/1706.03825.pdf. Defaults to 0.1. If the distribution is different per feature, set feature_noise_sigma instead for each feature.
        """
        return pulumi.get(self, "noise_sigma")

    @noise_sigma.setter
    def noise_sigma(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "noise_sigma", value)

    @property
    @pulumi.getter(name="noisySampleCount")
    def noisy_sample_count(self) -> Optional[pulumi.Input[int]]:
        """
        The number of gradient samples to use for approximation. The higher this number, the more accurate the gradient is, but the runtime complexity increases by this factor as well. Valid range of its value is [1, 50]. Defaults to 3.
        """
        return pulumi.get(self, "noisy_sample_count")

    @noisy_sample_count.setter
    def noisy_sample_count(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "noisy_sample_count", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StratifiedSplitArgs:
    def __init__(__self__, *,
                 key: pulumi.Input[str],
                 test_fraction: Optional[pulumi.Input[float]] = None,
                 training_fraction: Optional[pulumi.Input[float]] = None,
                 validation_fraction: Optional[pulumi.Input[float]] = None):
        """
        Assigns input data to the training, validation, and test sets so that the distribution of values found in the categorical column (as specified by the `key` field) is mirrored within each split. The fraction values determine the relative sizes of the splits. For example, if the specified column has three values, with 50% of the rows having value "A", 25% value "B", and 25% value "C", and the split fractions are specified as 80/10/10, then the training set will constitute 80% of the training data, with about 50% of the training set rows having the value "A" for the specified column, about 25% having the value "B", and about 25% having the value "C". Only the top 500 occurring values are used; any values not in the top 500 values are randomly assigned to a split. If less than three rows contain a specific value, those rows are randomly assigned. Supported only for tabular Datasets.
        :param pulumi.Input[str] key: The key is a name of one of the Dataset's data columns. The key provided must be for a categorical column.
        :param pulumi.Input[float] test_fraction: The fraction of the input data that is to be used to evaluate the Model.
        :param pulumi.Input[float] training_fraction: The fraction of the input data that is to be used to train the Model.
        :param pulumi.Input[float] validation_fraction: The fraction of the input data that is to be used to validate the Model.
        """
        pulumi.set(__self__, "key", key)
        if test_fraction is not None:
            pulumi.set(__self__, "test_fraction", test_fraction)
        if training_fraction is not None:
            pulumi.set(__self__, "training_fraction", training_fraction)
        if validation_fraction is not None:
            pulumi.set(__self__, "validation_fraction", validation_fraction)

    @property
    @pulumi.getter
    def key(self) -> pulumi.Input[str]:
        """
        The key is a name of one of the Dataset's data columns. The key provided must be for a categorical column.
        """
        return pulumi.get(self, "key")

    @key.setter
    def key(self, value: pulumi.Input[str]):
        pulumi.set(self, "key", value)

    @property
    @pulumi.getter(name="testFraction")
    def test_fraction(self) -> Optional[pulumi.Input[float]]:
        """
        The fraction of the input data that is to be used to evaluate the Model.
        """
        return pulumi.get(self, "test_fraction")

    @test_fraction.setter
    def test_fraction(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "test_fraction", value)

    @property
    @pulumi.getter(name="trainingFraction")
    def training_fraction(self) -> Optional[pulumi.Input[float]]:
        """
        The fraction of the input data that is to be used to train the Model.
        """
        return pulumi.get(self, "training_fraction")

    @training_fraction.setter
    def training_fraction(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "training_fraction", value)

    @property
    @pulumi.getter(name="validationFraction")
    def validation_fraction(self) -> Optional[pulumi.Input[float]]:
        """
        The fraction of the input data that is to be used to validate the Model.
        """
        return pulumi.get(self, "validation_fraction")

    @validation_fraction.setter
    def validation_fraction(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "validation_fraction", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecArgs:
    def __init__(__self__, *,
                 learning_rate_parameter_name: Optional[pulumi.Input[str]] = None,
                 max_step_count: Optional[pulumi.Input[str]] = None,
                 min_measurement_count: Optional[pulumi.Input[str]] = None,
                 min_step_count: Optional[pulumi.Input[str]] = None,
                 update_all_stopped_trials: Optional[pulumi.Input[bool]] = None,
                 use_elapsed_duration: Optional[pulumi.Input[bool]] = None):
        """
        Configuration for ConvexAutomatedStoppingSpec. When there are enough completed trials (configured by min_measurement_count), for pending trials with enough measurements and steps, the policy first computes an overestimate of the objective value at max_num_steps according to the slope of the incomplete objective value curve. No prediction can be made if the curve is completely flat. If the overestimation is worse than the best objective value of the completed trials, this pending trial will be early-stopped, but a last measurement will be added to the pending trial with max_num_steps and predicted objective value from the autoregression model.
        :param pulumi.Input[str] learning_rate_parameter_name: The hyper-parameter name used in the tuning job that stands for learning rate. Leave it blank if learning rate is not in a parameter in tuning. The learning_rate is used to estimate the objective value of the ongoing trial.
        :param pulumi.Input[str] max_step_count: Steps used in predicting the final objective for early stopped trials. In general, it's set to be the same as the defined steps in training / tuning. If not defined, it will learn it from the completed trials. When use_steps is false, this field is set to the maximum elapsed seconds.
        :param pulumi.Input[str] min_measurement_count: The minimal number of measurements in a Trial. Early-stopping checks will not trigger if less than min_measurement_count+1 completed trials or pending trials with less than min_measurement_count measurements. If not defined, the default value is 5.
        :param pulumi.Input[str] min_step_count: Minimum number of steps for a trial to complete. Trials which do not have a measurement with step_count > min_step_count won't be considered for early stopping. It's ok to set it to 0, and a trial can be early stopped at any stage. By default, min_step_count is set to be one-tenth of the max_step_count. When use_elapsed_duration is true, this field is set to the minimum elapsed seconds.
        :param pulumi.Input[bool] update_all_stopped_trials: ConvexAutomatedStoppingSpec by default only updates the trials that needs to be early stopped using a newly trained auto-regressive model. When this flag is set to True, all stopped trials from the beginning are potentially updated in terms of their `final_measurement`. Also, note that the training logic of autoregressive models is different in this case. Enabling this option has shown better results and this may be the default option in the future.
        :param pulumi.Input[bool] use_elapsed_duration: This bool determines whether or not the rule is applied based on elapsed_secs or steps. If use_elapsed_duration==false, the early stopping decision is made according to the predicted objective values according to the target steps. If use_elapsed_duration==true, elapsed_secs is used instead of steps. Also, in this case, the parameters max_num_steps and min_num_steps are overloaded to contain max_elapsed_seconds and min_elapsed_seconds.
        """
        if learning_rate_parameter_name is not None:
            pulumi.set(__self__, "learning_rate_parameter_name", learning_rate_parameter_name)
        if max_step_count is not None:
            pulumi.set(__self__, "max_step_count", max_step_count)
        if min_measurement_count is not None:
            pulumi.set(__self__, "min_measurement_count", min_measurement_count)
        if min_step_count is not None:
            pulumi.set(__self__, "min_step_count", min_step_count)
        if update_all_stopped_trials is not None:
            pulumi.set(__self__, "update_all_stopped_trials", update_all_stopped_trials)
        if use_elapsed_duration is not None:
            pulumi.set(__self__, "use_elapsed_duration", use_elapsed_duration)

    @property
    @pulumi.getter(name="learningRateParameterName")
    def learning_rate_parameter_name(self) -> Optional[pulumi.Input[str]]:
        """
        The hyper-parameter name used in the tuning job that stands for learning rate. Leave it blank if learning rate is not in a parameter in tuning. The learning_rate is used to estimate the objective value of the ongoing trial.
        """
        return pulumi.get(self, "learning_rate_parameter_name")

    @learning_rate_parameter_name.setter
    def learning_rate_parameter_name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "learning_rate_parameter_name", value)

    @property
    @pulumi.getter(name="maxStepCount")
    def max_step_count(self) -> Optional[pulumi.Input[str]]:
        """
        Steps used in predicting the final objective for early stopped trials. In general, it's set to be the same as the defined steps in training / tuning. If not defined, it will learn it from the completed trials. When use_steps is false, this field is set to the maximum elapsed seconds.
        """
        return pulumi.get(self, "max_step_count")

    @max_step_count.setter
    def max_step_count(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "max_step_count", value)

    @property
    @pulumi.getter(name="minMeasurementCount")
    def min_measurement_count(self) -> Optional[pulumi.Input[str]]:
        """
        The minimal number of measurements in a Trial. Early-stopping checks will not trigger if less than min_measurement_count+1 completed trials or pending trials with less than min_measurement_count measurements. If not defined, the default value is 5.
        """
        return pulumi.get(self, "min_measurement_count")

    @min_measurement_count.setter
    def min_measurement_count(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "min_measurement_count", value)

    @property
    @pulumi.getter(name="minStepCount")
    def min_step_count(self) -> Optional[pulumi.Input[str]]:
        """
        Minimum number of steps for a trial to complete. Trials which do not have a measurement with step_count > min_step_count won't be considered for early stopping. It's ok to set it to 0, and a trial can be early stopped at any stage. By default, min_step_count is set to be one-tenth of the max_step_count. When use_elapsed_duration is true, this field is set to the minimum elapsed seconds.
        """
        return pulumi.get(self, "min_step_count")

    @min_step_count.setter
    def min_step_count(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "min_step_count", value)

    @property
    @pulumi.getter(name="updateAllStoppedTrials")
    def update_all_stopped_trials(self) -> Optional[pulumi.Input[bool]]:
        """
        ConvexAutomatedStoppingSpec by default only updates the trials that needs to be early stopped using a newly trained auto-regressive model. When this flag is set to True, all stopped trials from the beginning are potentially updated in terms of their `final_measurement`. Also, note that the training logic of autoregressive models is different in this case. Enabling this option has shown better results and this may be the default option in the future.
        """
        return pulumi.get(self, "update_all_stopped_trials")

    @update_all_stopped_trials.setter
    def update_all_stopped_trials(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "update_all_stopped_trials", value)

    @property
    @pulumi.getter(name="useElapsedDuration")
    def use_elapsed_duration(self) -> Optional[pulumi.Input[bool]]:
        """
        This bool determines whether or not the rule is applied based on elapsed_secs or steps. If use_elapsed_duration==false, the early stopping decision is made according to the predicted objective values according to the target steps. If use_elapsed_duration==true, elapsed_secs is used instead of steps. Also, in this case, the parameters max_num_steps and min_num_steps are overloaded to contain max_elapsed_seconds and min_elapsed_seconds.
        """
        return pulumi.get(self, "use_elapsed_duration")

    @use_elapsed_duration.setter
    def use_elapsed_duration(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "use_elapsed_duration", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecArgs:
    def __init__(__self__, *,
                 use_elapsed_duration: Optional[pulumi.Input[bool]] = None):
        """
        The decay curve automated stopping rule builds a Gaussian Process Regressor to predict the final objective value of a Trial based on the already completed Trials and the intermediate measurements of the current Trial. Early stopping is requested for the current Trial if there is very low probability to exceed the optimal value found so far.
        :param pulumi.Input[bool] use_elapsed_duration: True if Measurement.elapsed_duration is used as the x-axis of each Trials Decay Curve. Otherwise, Measurement.step_count will be used as the x-axis.
        """
        if use_elapsed_duration is not None:
            pulumi.set(__self__, "use_elapsed_duration", use_elapsed_duration)

    @property
    @pulumi.getter(name="useElapsedDuration")
    def use_elapsed_duration(self) -> Optional[pulumi.Input[bool]]:
        """
        True if Measurement.elapsed_duration is used as the x-axis of each Trials Decay Curve. Otherwise, Measurement.step_count will be used as the x-axis.
        """
        return pulumi.get(self, "use_elapsed_duration")

    @use_elapsed_duration.setter
    def use_elapsed_duration(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "use_elapsed_duration", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecArgs:
    def __init__(__self__, *,
                 use_elapsed_duration: Optional[pulumi.Input[bool]] = None):
        """
        The median automated stopping rule stops a pending Trial if the Trial's best objective_value is strictly below the median 'performance' of all completed Trials reported up to the Trial's last measurement. Currently, 'performance' refers to the running average of the objective values reported by the Trial in each measurement.
        :param pulumi.Input[bool] use_elapsed_duration: True if median automated stopping rule applies on Measurement.elapsed_duration. It means that elapsed_duration field of latest measurement of current Trial is used to compute median objective value for each completed Trials.
        """
        if use_elapsed_duration is not None:
            pulumi.set(__self__, "use_elapsed_duration", use_elapsed_duration)

    @property
    @pulumi.getter(name="useElapsedDuration")
    def use_elapsed_duration(self) -> Optional[pulumi.Input[bool]]:
        """
        True if median automated stopping rule applies on Measurement.elapsed_duration. It means that elapsed_duration field of latest measurement of current Trial is used to compute median objective value for each completed Trials.
        """
        return pulumi.get(self, "use_elapsed_duration")

    @use_elapsed_duration.setter
    def use_elapsed_duration(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "use_elapsed_duration", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigArgs:
    def __init__(__self__, *,
                 desired_min_safe_trials_fraction: Optional[pulumi.Input[float]] = None,
                 safety_threshold: Optional[pulumi.Input[float]] = None):
        """
        Used in safe optimization to specify threshold levels and risk tolerance.
        :param pulumi.Input[float] desired_min_safe_trials_fraction: Desired minimum fraction of safe trials (over total number of trials) that should be targeted by the algorithm at any time during the study (best effort). This should be between 0.0 and 1.0 and a value of 0.0 means that there is no minimum and an algorithm proceeds without targeting any specific fraction. A value of 1.0 means that the algorithm attempts to only Suggest safe Trials.
        :param pulumi.Input[float] safety_threshold: Safety threshold (boundary value between safe and unsafe). NOTE that if you leave SafetyMetricConfig unset, a default value of 0 will be used.
        """
        if desired_min_safe_trials_fraction is not None:
            pulumi.set(__self__, "desired_min_safe_trials_fraction", desired_min_safe_trials_fraction)
        if safety_threshold is not None:
            pulumi.set(__self__, "safety_threshold", safety_threshold)

    @property
    @pulumi.getter(name="desiredMinSafeTrialsFraction")
    def desired_min_safe_trials_fraction(self) -> Optional[pulumi.Input[float]]:
        """
        Desired minimum fraction of safe trials (over total number of trials) that should be targeted by the algorithm at any time during the study (best effort). This should be between 0.0 and 1.0 and a value of 0.0 means that there is no minimum and an algorithm proceeds without targeting any specific fraction. A value of 1.0 means that the algorithm attempts to only Suggest safe Trials.
        """
        return pulumi.get(self, "desired_min_safe_trials_fraction")

    @desired_min_safe_trials_fraction.setter
    def desired_min_safe_trials_fraction(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "desired_min_safe_trials_fraction", value)

    @property
    @pulumi.getter(name="safetyThreshold")
    def safety_threshold(self) -> Optional[pulumi.Input[float]]:
        """
        Safety threshold (boundary value between safe and unsafe). NOTE that if you leave SafetyMetricConfig unset, a default value of 0 will be used.
        """
        return pulumi.get(self, "safety_threshold")

    @safety_threshold.setter
    def safety_threshold(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "safety_threshold", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecMetricSpecArgs:
    def __init__(__self__, *,
                 goal: pulumi.Input['GoogleCloudAiplatformV1StudySpecMetricSpecGoal'],
                 metric_id: pulumi.Input[str],
                 safety_config: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigArgs']] = None):
        """
        Represents a metric to optimize.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecMetricSpecGoal'] goal: The optimization goal of the metric.
        :param pulumi.Input[str] metric_id: The ID of the metric. Must not contain whitespaces and must be unique amongst all MetricSpecs.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigArgs'] safety_config: Used for safe search. In the case, the metric will be a safety metric. You must provide a separate metric for objective metric.
        """
        pulumi.set(__self__, "goal", goal)
        pulumi.set(__self__, "metric_id", metric_id)
        if safety_config is not None:
            pulumi.set(__self__, "safety_config", safety_config)

    @property
    @pulumi.getter
    def goal(self) -> pulumi.Input['GoogleCloudAiplatformV1StudySpecMetricSpecGoal']:
        """
        The optimization goal of the metric.
        """
        return pulumi.get(self, "goal")

    @goal.setter
    def goal(self, value: pulumi.Input['GoogleCloudAiplatformV1StudySpecMetricSpecGoal']):
        pulumi.set(self, "goal", value)

    @property
    @pulumi.getter(name="metricId")
    def metric_id(self) -> pulumi.Input[str]:
        """
        The ID of the metric. Must not contain whitespaces and must be unique amongst all MetricSpecs.
        """
        return pulumi.get(self, "metric_id")

    @metric_id.setter
    def metric_id(self, value: pulumi.Input[str]):
        pulumi.set(self, "metric_id", value)

    @property
    @pulumi.getter(name="safetyConfig")
    def safety_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigArgs']]:
        """
        Used for safe search. In the case, the metric will be a safety metric. You must provide a separate metric for objective metric.
        """
        return pulumi.get(self, "safety_config")

    @safety_config.setter
    def safety_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecMetricSpecSafetyMetricConfigArgs']]):
        pulumi.set(self, "safety_config", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecArgs:
    def __init__(__self__, *,
                 values: pulumi.Input[Sequence[pulumi.Input[str]]],
                 default_value: Optional[pulumi.Input[str]] = None):
        """
        Value specification for a parameter in `CATEGORICAL` type.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] values: The list of possible categories.
        :param pulumi.Input[str] default_value: A default value for a `CATEGORICAL` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        pulumi.set(__self__, "values", values)
        if default_value is not None:
            pulumi.set(__self__, "default_value", default_value)

    @property
    @pulumi.getter
    def values(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        The list of possible categories.
        """
        return pulumi.get(self, "values")

    @values.setter
    def values(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "values", value)

    @property
    @pulumi.getter(name="defaultValue")
    def default_value(self) -> Optional[pulumi.Input[str]]:
        """
        A default value for a `CATEGORICAL` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        return pulumi.get(self, "default_value")

    @default_value.setter
    def default_value(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "default_value", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecCategoricalValueConditionArgs:
    def __init__(__self__, *,
                 values: pulumi.Input[Sequence[pulumi.Input[str]]]):
        """
        Represents the spec to match categorical values from parent parameter.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] values: Matches values of the parent parameter of 'CATEGORICAL' type. All values must exist in `categorical_value_spec` of parent parameter.
        """
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter
    def values(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Matches values of the parent parameter of 'CATEGORICAL' type. All values must exist in `categorical_value_spec` of parent parameter.
        """
        return pulumi.get(self, "values")

    @values.setter
    def values(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "values", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecDiscreteValueConditionArgs:
    def __init__(__self__, *,
                 values: pulumi.Input[Sequence[pulumi.Input[float]]]):
        """
        Represents the spec to match discrete values from parent parameter.
        :param pulumi.Input[Sequence[pulumi.Input[float]]] values: Matches values of the parent parameter of 'DISCRETE' type. All values must exist in `discrete_value_spec` of parent parameter. The Epsilon of the value matching is 1e-10.
        """
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter
    def values(self) -> pulumi.Input[Sequence[pulumi.Input[float]]]:
        """
        Matches values of the parent parameter of 'DISCRETE' type. All values must exist in `discrete_value_spec` of parent parameter. The Epsilon of the value matching is 1e-10.
        """
        return pulumi.get(self, "values")

    @values.setter
    def values(self, value: pulumi.Input[Sequence[pulumi.Input[float]]]):
        pulumi.set(self, "values", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecIntValueConditionArgs:
    def __init__(__self__, *,
                 values: pulumi.Input[Sequence[pulumi.Input[str]]]):
        """
        Represents the spec to match integer values from parent parameter.
        :param pulumi.Input[Sequence[pulumi.Input[str]]] values: Matches values of the parent parameter of 'INTEGER' type. All values must lie in `integer_value_spec` of parent parameter.
        """
        pulumi.set(__self__, "values", values)

    @property
    @pulumi.getter
    def values(self) -> pulumi.Input[Sequence[pulumi.Input[str]]]:
        """
        Matches values of the parent parameter of 'INTEGER' type. All values must lie in `integer_value_spec` of parent parameter.
        """
        return pulumi.get(self, "values")

    @values.setter
    def values(self, value: pulumi.Input[Sequence[pulumi.Input[str]]]):
        pulumi.set(self, "values", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecArgs:
    def __init__(__self__, *,
                 parameter_spec: pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecArgs'],
                 parent_categorical_values: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecCategoricalValueConditionArgs']] = None,
                 parent_discrete_values: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecDiscreteValueConditionArgs']] = None,
                 parent_int_values: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecIntValueConditionArgs']] = None):
        """
        Represents a parameter spec with condition from its parent parameter.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecArgs'] parameter_spec: The spec for a conditional parameter.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecCategoricalValueConditionArgs'] parent_categorical_values: The spec for matching values from a parent parameter of `CATEGORICAL` type.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecDiscreteValueConditionArgs'] parent_discrete_values: The spec for matching values from a parent parameter of `DISCRETE` type.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecIntValueConditionArgs'] parent_int_values: The spec for matching values from a parent parameter of `INTEGER` type.
        """
        pulumi.set(__self__, "parameter_spec", parameter_spec)
        if parent_categorical_values is not None:
            pulumi.set(__self__, "parent_categorical_values", parent_categorical_values)
        if parent_discrete_values is not None:
            pulumi.set(__self__, "parent_discrete_values", parent_discrete_values)
        if parent_int_values is not None:
            pulumi.set(__self__, "parent_int_values", parent_int_values)

    @property
    @pulumi.getter(name="parameterSpec")
    def parameter_spec(self) -> pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecArgs']:
        """
        The spec for a conditional parameter.
        """
        return pulumi.get(self, "parameter_spec")

    @parameter_spec.setter
    def parameter_spec(self, value: pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecArgs']):
        pulumi.set(self, "parameter_spec", value)

    @property
    @pulumi.getter(name="parentCategoricalValues")
    def parent_categorical_values(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecCategoricalValueConditionArgs']]:
        """
        The spec for matching values from a parent parameter of `CATEGORICAL` type.
        """
        return pulumi.get(self, "parent_categorical_values")

    @parent_categorical_values.setter
    def parent_categorical_values(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecCategoricalValueConditionArgs']]):
        pulumi.set(self, "parent_categorical_values", value)

    @property
    @pulumi.getter(name="parentDiscreteValues")
    def parent_discrete_values(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecDiscreteValueConditionArgs']]:
        """
        The spec for matching values from a parent parameter of `DISCRETE` type.
        """
        return pulumi.get(self, "parent_discrete_values")

    @parent_discrete_values.setter
    def parent_discrete_values(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecDiscreteValueConditionArgs']]):
        pulumi.set(self, "parent_discrete_values", value)

    @property
    @pulumi.getter(name="parentIntValues")
    def parent_int_values(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecIntValueConditionArgs']]:
        """
        The spec for matching values from a parent parameter of `INTEGER` type.
        """
        return pulumi.get(self, "parent_int_values")

    @parent_int_values.setter
    def parent_int_values(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecIntValueConditionArgs']]):
        pulumi.set(self, "parent_int_values", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecArgs:
    def __init__(__self__, *,
                 values: pulumi.Input[Sequence[pulumi.Input[float]]],
                 default_value: Optional[pulumi.Input[float]] = None):
        """
        Value specification for a parameter in `DISCRETE` type.
        :param pulumi.Input[Sequence[pulumi.Input[float]]] values: A list of possible values. The list should be in increasing order and at least 1e-10 apart. For instance, this parameter might have possible settings of 1.5, 2.5, and 4.0. This list should not contain more than 1,000 values.
        :param pulumi.Input[float] default_value: A default value for a `DISCRETE` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. It automatically rounds to the nearest feasible discrete point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        pulumi.set(__self__, "values", values)
        if default_value is not None:
            pulumi.set(__self__, "default_value", default_value)

    @property
    @pulumi.getter
    def values(self) -> pulumi.Input[Sequence[pulumi.Input[float]]]:
        """
        A list of possible values. The list should be in increasing order and at least 1e-10 apart. For instance, this parameter might have possible settings of 1.5, 2.5, and 4.0. This list should not contain more than 1,000 values.
        """
        return pulumi.get(self, "values")

    @values.setter
    def values(self, value: pulumi.Input[Sequence[pulumi.Input[float]]]):
        pulumi.set(self, "values", value)

    @property
    @pulumi.getter(name="defaultValue")
    def default_value(self) -> Optional[pulumi.Input[float]]:
        """
        A default value for a `DISCRETE` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. It automatically rounds to the nearest feasible discrete point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        return pulumi.get(self, "default_value")

    @default_value.setter
    def default_value(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "default_value", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecArgs:
    def __init__(__self__, *,
                 max_value: pulumi.Input[float],
                 min_value: pulumi.Input[float],
                 default_value: Optional[pulumi.Input[float]] = None):
        """
        Value specification for a parameter in `DOUBLE` type.
        :param pulumi.Input[float] max_value: Inclusive maximum value of the parameter.
        :param pulumi.Input[float] min_value: Inclusive minimum value of the parameter.
        :param pulumi.Input[float] default_value: A default value for a `DOUBLE` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        pulumi.set(__self__, "max_value", max_value)
        pulumi.set(__self__, "min_value", min_value)
        if default_value is not None:
            pulumi.set(__self__, "default_value", default_value)

    @property
    @pulumi.getter(name="maxValue")
    def max_value(self) -> pulumi.Input[float]:
        """
        Inclusive maximum value of the parameter.
        """
        return pulumi.get(self, "max_value")

    @max_value.setter
    def max_value(self, value: pulumi.Input[float]):
        pulumi.set(self, "max_value", value)

    @property
    @pulumi.getter(name="minValue")
    def min_value(self) -> pulumi.Input[float]:
        """
        Inclusive minimum value of the parameter.
        """
        return pulumi.get(self, "min_value")

    @min_value.setter
    def min_value(self, value: pulumi.Input[float]):
        pulumi.set(self, "min_value", value)

    @property
    @pulumi.getter(name="defaultValue")
    def default_value(self) -> Optional[pulumi.Input[float]]:
        """
        A default value for a `DOUBLE` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        return pulumi.get(self, "default_value")

    @default_value.setter
    def default_value(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "default_value", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecArgs:
    def __init__(__self__, *,
                 max_value: pulumi.Input[str],
                 min_value: pulumi.Input[str],
                 default_value: Optional[pulumi.Input[str]] = None):
        """
        Value specification for a parameter in `INTEGER` type.
        :param pulumi.Input[str] max_value: Inclusive maximum value of the parameter.
        :param pulumi.Input[str] min_value: Inclusive minimum value of the parameter.
        :param pulumi.Input[str] default_value: A default value for an `INTEGER` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        pulumi.set(__self__, "max_value", max_value)
        pulumi.set(__self__, "min_value", min_value)
        if default_value is not None:
            pulumi.set(__self__, "default_value", default_value)

    @property
    @pulumi.getter(name="maxValue")
    def max_value(self) -> pulumi.Input[str]:
        """
        Inclusive maximum value of the parameter.
        """
        return pulumi.get(self, "max_value")

    @max_value.setter
    def max_value(self, value: pulumi.Input[str]):
        pulumi.set(self, "max_value", value)

    @property
    @pulumi.getter(name="minValue")
    def min_value(self) -> pulumi.Input[str]:
        """
        Inclusive minimum value of the parameter.
        """
        return pulumi.get(self, "min_value")

    @min_value.setter
    def min_value(self, value: pulumi.Input[str]):
        pulumi.set(self, "min_value", value)

    @property
    @pulumi.getter(name="defaultValue")
    def default_value(self) -> Optional[pulumi.Input[str]]:
        """
        A default value for an `INTEGER` parameter that is assumed to be a relatively good starting point. Unset value signals that there is no offered starting point. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        return pulumi.get(self, "default_value")

    @default_value.setter
    def default_value(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "default_value", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecParameterSpecArgs:
    def __init__(__self__, *,
                 parameter_id: pulumi.Input[str],
                 categorical_value_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecArgs']] = None,
                 conditional_parameter_specs: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecArgs']]]] = None,
                 discrete_value_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecArgs']] = None,
                 double_value_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecArgs']] = None,
                 integer_value_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecArgs']] = None,
                 scale_type: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecScaleType']] = None):
        """
        Represents a single parameter to optimize.
        :param pulumi.Input[str] parameter_id: The ID of the parameter. Must not contain whitespaces and must be unique amongst all ParameterSpecs.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecArgs'] categorical_value_spec: The value spec for a 'CATEGORICAL' parameter.
        :param pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecArgs']]] conditional_parameter_specs: A conditional parameter node is active if the parameter's value matches the conditional node's parent_value_condition. If two items in conditional_parameter_specs have the same name, they must have disjoint parent_value_condition.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecArgs'] discrete_value_spec: The value spec for a 'DISCRETE' parameter.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecArgs'] double_value_spec: The value spec for a 'DOUBLE' parameter.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecArgs'] integer_value_spec: The value spec for an 'INTEGER' parameter.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecScaleType'] scale_type: How the parameter should be scaled. Leave unset for `CATEGORICAL` parameters.
        """
        pulumi.set(__self__, "parameter_id", parameter_id)
        if categorical_value_spec is not None:
            pulumi.set(__self__, "categorical_value_spec", categorical_value_spec)
        if conditional_parameter_specs is not None:
            pulumi.set(__self__, "conditional_parameter_specs", conditional_parameter_specs)
        if discrete_value_spec is not None:
            pulumi.set(__self__, "discrete_value_spec", discrete_value_spec)
        if double_value_spec is not None:
            pulumi.set(__self__, "double_value_spec", double_value_spec)
        if integer_value_spec is not None:
            pulumi.set(__self__, "integer_value_spec", integer_value_spec)
        if scale_type is not None:
            pulumi.set(__self__, "scale_type", scale_type)

    @property
    @pulumi.getter(name="parameterId")
    def parameter_id(self) -> pulumi.Input[str]:
        """
        The ID of the parameter. Must not contain whitespaces and must be unique amongst all ParameterSpecs.
        """
        return pulumi.get(self, "parameter_id")

    @parameter_id.setter
    def parameter_id(self, value: pulumi.Input[str]):
        pulumi.set(self, "parameter_id", value)

    @property
    @pulumi.getter(name="categoricalValueSpec")
    def categorical_value_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecArgs']]:
        """
        The value spec for a 'CATEGORICAL' parameter.
        """
        return pulumi.get(self, "categorical_value_spec")

    @categorical_value_spec.setter
    def categorical_value_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecCategoricalValueSpecArgs']]):
        pulumi.set(self, "categorical_value_spec", value)

    @property
    @pulumi.getter(name="conditionalParameterSpecs")
    def conditional_parameter_specs(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecArgs']]]]:
        """
        A conditional parameter node is active if the parameter's value matches the conditional node's parent_value_condition. If two items in conditional_parameter_specs have the same name, they must have disjoint parent_value_condition.
        """
        return pulumi.get(self, "conditional_parameter_specs")

    @conditional_parameter_specs.setter
    def conditional_parameter_specs(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecConditionalParameterSpecArgs']]]]):
        pulumi.set(self, "conditional_parameter_specs", value)

    @property
    @pulumi.getter(name="discreteValueSpec")
    def discrete_value_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecArgs']]:
        """
        The value spec for a 'DISCRETE' parameter.
        """
        return pulumi.get(self, "discrete_value_spec")

    @discrete_value_spec.setter
    def discrete_value_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecDiscreteValueSpecArgs']]):
        pulumi.set(self, "discrete_value_spec", value)

    @property
    @pulumi.getter(name="doubleValueSpec")
    def double_value_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecArgs']]:
        """
        The value spec for a 'DOUBLE' parameter.
        """
        return pulumi.get(self, "double_value_spec")

    @double_value_spec.setter
    def double_value_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecDoubleValueSpecArgs']]):
        pulumi.set(self, "double_value_spec", value)

    @property
    @pulumi.getter(name="integerValueSpec")
    def integer_value_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecArgs']]:
        """
        The value spec for an 'INTEGER' parameter.
        """
        return pulumi.get(self, "integer_value_spec")

    @integer_value_spec.setter
    def integer_value_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecIntegerValueSpecArgs']]):
        pulumi.set(self, "integer_value_spec", value)

    @property
    @pulumi.getter(name="scaleType")
    def scale_type(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecScaleType']]:
        """
        How the parameter should be scaled. Leave unset for `CATEGORICAL` parameters.
        """
        return pulumi.get(self, "scale_type")

    @scale_type.setter
    def scale_type(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecScaleType']]):
        pulumi.set(self, "scale_type", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecStudyStoppingConfigArgs:
    def __init__(__self__, *,
                 max_duration_no_progress: Optional[pulumi.Input[str]] = None,
                 max_num_trials: Optional[pulumi.Input[int]] = None,
                 max_num_trials_no_progress: Optional[pulumi.Input[int]] = None,
                 maximum_runtime_constraint: Optional[pulumi.Input['GoogleCloudAiplatformV1StudyTimeConstraintArgs']] = None,
                 min_num_trials: Optional[pulumi.Input[int]] = None,
                 minimum_runtime_constraint: Optional[pulumi.Input['GoogleCloudAiplatformV1StudyTimeConstraintArgs']] = None,
                 should_stop_asap: Optional[pulumi.Input[bool]] = None):
        """
        The configuration (stopping conditions) for automated stopping of a Study. Conditions include trial budgets, time budgets, and convergence detection.
        :param pulumi.Input[str] max_duration_no_progress: If the objective value has not improved for this much time, stop the study. WARNING: Effective only for single-objective studies.
        :param pulumi.Input[int] max_num_trials: If there are more than this many trials, stop the study.
        :param pulumi.Input[int] max_num_trials_no_progress: If the objective value has not improved for this many consecutive trials, stop the study. WARNING: Effective only for single-objective studies.
        :param pulumi.Input['GoogleCloudAiplatformV1StudyTimeConstraintArgs'] maximum_runtime_constraint: If the specified time or duration has passed, stop the study.
        :param pulumi.Input[int] min_num_trials: If there are fewer than this many COMPLETED trials, do not stop the study.
        :param pulumi.Input['GoogleCloudAiplatformV1StudyTimeConstraintArgs'] minimum_runtime_constraint: Each "stopping rule" in this proto specifies an "if" condition. Before Vizier would generate a new suggestion, it first checks each specified stopping rule, from top to bottom in this list. Note that the first few rules (e.g. minimum_runtime_constraint, min_num_trials) will prevent other stopping rules from being evaluated until they are met. For example, setting `min_num_trials=5` and `always_stop_after= 1 hour` means that the Study will ONLY stop after it has 5 COMPLETED trials, even if more than an hour has passed since its creation. It follows the first applicable rule (whose "if" condition is satisfied) to make a stopping decision. If none of the specified rules are applicable, then Vizier decides that the study should not stop. If Vizier decides that the study should stop, the study enters STOPPING state (or STOPPING_ASAP if should_stop_asap = true). IMPORTANT: The automatic study state transition happens precisely as described above; that is, deleting trials or updating StudyConfig NEVER automatically moves the study state back to ACTIVE. If you want to _resume_ a Study that was stopped, 1) change the stopping conditions if necessary, 2) activate the study, and then 3) ask for suggestions. If the specified time or duration has not passed, do not stop the study.
        :param pulumi.Input[bool] should_stop_asap: If true, a Study enters STOPPING_ASAP whenever it would normally enters STOPPING state. The bottom line is: set to true if you want to interrupt on-going evaluations of Trials as soon as the study stopping condition is met. (Please see Study.State documentation for the source of truth).
        """
        if max_duration_no_progress is not None:
            pulumi.set(__self__, "max_duration_no_progress", max_duration_no_progress)
        if max_num_trials is not None:
            pulumi.set(__self__, "max_num_trials", max_num_trials)
        if max_num_trials_no_progress is not None:
            pulumi.set(__self__, "max_num_trials_no_progress", max_num_trials_no_progress)
        if maximum_runtime_constraint is not None:
            pulumi.set(__self__, "maximum_runtime_constraint", maximum_runtime_constraint)
        if min_num_trials is not None:
            pulumi.set(__self__, "min_num_trials", min_num_trials)
        if minimum_runtime_constraint is not None:
            pulumi.set(__self__, "minimum_runtime_constraint", minimum_runtime_constraint)
        if should_stop_asap is not None:
            pulumi.set(__self__, "should_stop_asap", should_stop_asap)

    @property
    @pulumi.getter(name="maxDurationNoProgress")
    def max_duration_no_progress(self) -> Optional[pulumi.Input[str]]:
        """
        If the objective value has not improved for this much time, stop the study. WARNING: Effective only for single-objective studies.
        """
        return pulumi.get(self, "max_duration_no_progress")

    @max_duration_no_progress.setter
    def max_duration_no_progress(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "max_duration_no_progress", value)

    @property
    @pulumi.getter(name="maxNumTrials")
    def max_num_trials(self) -> Optional[pulumi.Input[int]]:
        """
        If there are more than this many trials, stop the study.
        """
        return pulumi.get(self, "max_num_trials")

    @max_num_trials.setter
    def max_num_trials(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_num_trials", value)

    @property
    @pulumi.getter(name="maxNumTrialsNoProgress")
    def max_num_trials_no_progress(self) -> Optional[pulumi.Input[int]]:
        """
        If the objective value has not improved for this many consecutive trials, stop the study. WARNING: Effective only for single-objective studies.
        """
        return pulumi.get(self, "max_num_trials_no_progress")

    @max_num_trials_no_progress.setter
    def max_num_trials_no_progress(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_num_trials_no_progress", value)

    @property
    @pulumi.getter(name="maximumRuntimeConstraint")
    def maximum_runtime_constraint(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudyTimeConstraintArgs']]:
        """
        If the specified time or duration has passed, stop the study.
        """
        return pulumi.get(self, "maximum_runtime_constraint")

    @maximum_runtime_constraint.setter
    def maximum_runtime_constraint(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudyTimeConstraintArgs']]):
        pulumi.set(self, "maximum_runtime_constraint", value)

    @property
    @pulumi.getter(name="minNumTrials")
    def min_num_trials(self) -> Optional[pulumi.Input[int]]:
        """
        If there are fewer than this many COMPLETED trials, do not stop the study.
        """
        return pulumi.get(self, "min_num_trials")

    @min_num_trials.setter
    def min_num_trials(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "min_num_trials", value)

    @property
    @pulumi.getter(name="minimumRuntimeConstraint")
    def minimum_runtime_constraint(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudyTimeConstraintArgs']]:
        """
        Each "stopping rule" in this proto specifies an "if" condition. Before Vizier would generate a new suggestion, it first checks each specified stopping rule, from top to bottom in this list. Note that the first few rules (e.g. minimum_runtime_constraint, min_num_trials) will prevent other stopping rules from being evaluated until they are met. For example, setting `min_num_trials=5` and `always_stop_after= 1 hour` means that the Study will ONLY stop after it has 5 COMPLETED trials, even if more than an hour has passed since its creation. It follows the first applicable rule (whose "if" condition is satisfied) to make a stopping decision. If none of the specified rules are applicable, then Vizier decides that the study should not stop. If Vizier decides that the study should stop, the study enters STOPPING state (or STOPPING_ASAP if should_stop_asap = true). IMPORTANT: The automatic study state transition happens precisely as described above; that is, deleting trials or updating StudyConfig NEVER automatically moves the study state back to ACTIVE. If you want to _resume_ a Study that was stopped, 1) change the stopping conditions if necessary, 2) activate the study, and then 3) ask for suggestions. If the specified time or duration has not passed, do not stop the study.
        """
        return pulumi.get(self, "minimum_runtime_constraint")

    @minimum_runtime_constraint.setter
    def minimum_runtime_constraint(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudyTimeConstraintArgs']]):
        pulumi.set(self, "minimum_runtime_constraint", value)

    @property
    @pulumi.getter(name="shouldStopAsap")
    def should_stop_asap(self) -> Optional[pulumi.Input[bool]]:
        """
        If true, a Study enters STOPPING_ASAP whenever it would normally enters STOPPING state. The bottom line is: set to true if you want to interrupt on-going evaluations of Trials as soon as the study stopping condition is met. (Please see Study.State documentation for the source of truth).
        """
        return pulumi.get(self, "should_stop_asap")

    @should_stop_asap.setter
    def should_stop_asap(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "should_stop_asap", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudySpecArgs:
    def __init__(__self__, *,
                 metrics: pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1StudySpecMetricSpecArgs']]],
                 parameters: pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecArgs']]],
                 algorithm: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecAlgorithm']] = None,
                 convex_automated_stopping_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecArgs']] = None,
                 decay_curve_stopping_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecArgs']] = None,
                 measurement_selection_type: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecMeasurementSelectionType']] = None,
                 median_automated_stopping_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecArgs']] = None,
                 observation_noise: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecObservationNoise']] = None,
                 study_stopping_config: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecStudyStoppingConfigArgs']] = None):
        """
        Represents specification of a Study.
        :param pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1StudySpecMetricSpecArgs']]] metrics: Metric specs for the Study.
        :param pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecArgs']]] parameters: The set of parameters to tune.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecAlgorithm'] algorithm: The search algorithm specified for the Study.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecArgs'] convex_automated_stopping_spec: The automated early stopping spec using convex stopping rule.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecArgs'] decay_curve_stopping_spec: The automated early stopping spec using decay curve rule.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecMeasurementSelectionType'] measurement_selection_type: Describe which measurement selection type will be used
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecArgs'] median_automated_stopping_spec: The automated early stopping spec using median rule.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecObservationNoise'] observation_noise: The observation noise level of the study. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        :param pulumi.Input['GoogleCloudAiplatformV1StudySpecStudyStoppingConfigArgs'] study_stopping_config: Conditions for automated stopping of a Study. Enable automated stopping by configuring at least one condition.
        """
        pulumi.set(__self__, "metrics", metrics)
        pulumi.set(__self__, "parameters", parameters)
        if algorithm is not None:
            pulumi.set(__self__, "algorithm", algorithm)
        if convex_automated_stopping_spec is not None:
            pulumi.set(__self__, "convex_automated_stopping_spec", convex_automated_stopping_spec)
        if decay_curve_stopping_spec is not None:
            pulumi.set(__self__, "decay_curve_stopping_spec", decay_curve_stopping_spec)
        if measurement_selection_type is not None:
            pulumi.set(__self__, "measurement_selection_type", measurement_selection_type)
        if median_automated_stopping_spec is not None:
            pulumi.set(__self__, "median_automated_stopping_spec", median_automated_stopping_spec)
        if observation_noise is not None:
            pulumi.set(__self__, "observation_noise", observation_noise)
        if study_stopping_config is not None:
            pulumi.set(__self__, "study_stopping_config", study_stopping_config)

    @property
    @pulumi.getter
    def metrics(self) -> pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1StudySpecMetricSpecArgs']]]:
        """
        Metric specs for the Study.
        """
        return pulumi.get(self, "metrics")

    @metrics.setter
    def metrics(self, value: pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1StudySpecMetricSpecArgs']]]):
        pulumi.set(self, "metrics", value)

    @property
    @pulumi.getter
    def parameters(self) -> pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecArgs']]]:
        """
        The set of parameters to tune.
        """
        return pulumi.get(self, "parameters")

    @parameters.setter
    def parameters(self, value: pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1StudySpecParameterSpecArgs']]]):
        pulumi.set(self, "parameters", value)

    @property
    @pulumi.getter
    def algorithm(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecAlgorithm']]:
        """
        The search algorithm specified for the Study.
        """
        return pulumi.get(self, "algorithm")

    @algorithm.setter
    def algorithm(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecAlgorithm']]):
        pulumi.set(self, "algorithm", value)

    @property
    @pulumi.getter(name="convexAutomatedStoppingSpec")
    def convex_automated_stopping_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecArgs']]:
        """
        The automated early stopping spec using convex stopping rule.
        """
        return pulumi.get(self, "convex_automated_stopping_spec")

    @convex_automated_stopping_spec.setter
    def convex_automated_stopping_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecConvexAutomatedStoppingSpecArgs']]):
        pulumi.set(self, "convex_automated_stopping_spec", value)

    @property
    @pulumi.getter(name="decayCurveStoppingSpec")
    def decay_curve_stopping_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecArgs']]:
        """
        The automated early stopping spec using decay curve rule.
        """
        return pulumi.get(self, "decay_curve_stopping_spec")

    @decay_curve_stopping_spec.setter
    def decay_curve_stopping_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecDecayCurveAutomatedStoppingSpecArgs']]):
        pulumi.set(self, "decay_curve_stopping_spec", value)

    @property
    @pulumi.getter(name="measurementSelectionType")
    def measurement_selection_type(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecMeasurementSelectionType']]:
        """
        Describe which measurement selection type will be used
        """
        return pulumi.get(self, "measurement_selection_type")

    @measurement_selection_type.setter
    def measurement_selection_type(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecMeasurementSelectionType']]):
        pulumi.set(self, "measurement_selection_type", value)

    @property
    @pulumi.getter(name="medianAutomatedStoppingSpec")
    def median_automated_stopping_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecArgs']]:
        """
        The automated early stopping spec using median rule.
        """
        return pulumi.get(self, "median_automated_stopping_spec")

    @median_automated_stopping_spec.setter
    def median_automated_stopping_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecMedianAutomatedStoppingSpecArgs']]):
        pulumi.set(self, "median_automated_stopping_spec", value)

    @property
    @pulumi.getter(name="observationNoise")
    def observation_noise(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecObservationNoise']]:
        """
        The observation noise level of the study. Currently only supported by the Vertex AI Vizier service. Not supported by HyperparameterTuningJob or TrainingPipeline.
        """
        return pulumi.get(self, "observation_noise")

    @observation_noise.setter
    def observation_noise(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecObservationNoise']]):
        pulumi.set(self, "observation_noise", value)

    @property
    @pulumi.getter(name="studyStoppingConfig")
    def study_stopping_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecStudyStoppingConfigArgs']]:
        """
        Conditions for automated stopping of a Study. Enable automated stopping by configuring at least one condition.
        """
        return pulumi.get(self, "study_stopping_config")

    @study_stopping_config.setter
    def study_stopping_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1StudySpecStudyStoppingConfigArgs']]):
        pulumi.set(self, "study_stopping_config", value)


@pulumi.input_type
class GoogleCloudAiplatformV1StudyTimeConstraintArgs:
    def __init__(__self__, *,
                 end_time: Optional[pulumi.Input[str]] = None,
                 max_duration: Optional[pulumi.Input[str]] = None):
        """
        Time-based Constraint for Study
        :param pulumi.Input[str] end_time: Compares the wallclock time to this time. Must use UTC timezone.
        :param pulumi.Input[str] max_duration: Counts the wallclock time passed since the creation of this Study.
        """
        if end_time is not None:
            pulumi.set(__self__, "end_time", end_time)
        if max_duration is not None:
            pulumi.set(__self__, "max_duration", max_duration)

    @property
    @pulumi.getter(name="endTime")
    def end_time(self) -> Optional[pulumi.Input[str]]:
        """
        Compares the wallclock time to this time. Must use UTC timezone.
        """
        return pulumi.get(self, "end_time")

    @end_time.setter
    def end_time(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "end_time", value)

    @property
    @pulumi.getter(name="maxDuration")
    def max_duration(self) -> Optional[pulumi.Input[str]]:
        """
        Counts the wallclock time passed since the creation of this Study.
        """
        return pulumi.get(self, "max_duration")

    @max_duration.setter
    def max_duration(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "max_duration", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ThresholdConfigArgs:
    def __init__(__self__, *,
                 value: Optional[pulumi.Input[float]] = None):
        """
        The config for feature monitoring threshold.
        :param pulumi.Input[float] value: Specify a threshold value that can trigger the alert. If this threshold config is for feature distribution distance: 1. For categorical feature, the distribution distance is calculated by L-inifinity norm. 2. For numerical feature, the distribution distance is calculated by JensenShannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature.
        """
        if value is not None:
            pulumi.set(__self__, "value", value)

    @property
    @pulumi.getter
    def value(self) -> Optional[pulumi.Input[float]]:
        """
        Specify a threshold value that can trigger the alert. If this threshold config is for feature distribution distance: 1. For categorical feature, the distribution distance is calculated by L-inifinity norm. 2. For numerical feature, the distribution distance is calculated by JensenShannon divergence. Each feature must have a non-zero threshold if they need to be monitored. Otherwise no alert will be triggered for that feature.
        """
        return pulumi.get(self, "value")

    @value.setter
    def value(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "value", value)


@pulumi.input_type
class GoogleCloudAiplatformV1TimestampSplitArgs:
    def __init__(__self__, *,
                 key: pulumi.Input[str],
                 test_fraction: Optional[pulumi.Input[float]] = None,
                 training_fraction: Optional[pulumi.Input[float]] = None,
                 validation_fraction: Optional[pulumi.Input[float]] = None):
        """
        Assigns input data to training, validation, and test sets based on a provided timestamps. The youngest data pieces are assigned to training set, next to validation set, and the oldest to the test set. Supported only for tabular Datasets.
        :param pulumi.Input[str] key: The key is a name of one of the Dataset's data columns. The values of the key (the values in the column) must be in RFC 3339 `date-time` format, where `time-offset` = `"Z"` (e.g. 1985-04-12T23:20:50.52Z). If for a piece of data the key is not present or has an invalid value, that piece is ignored by the pipeline.
        :param pulumi.Input[float] test_fraction: The fraction of the input data that is to be used to evaluate the Model.
        :param pulumi.Input[float] training_fraction: The fraction of the input data that is to be used to train the Model.
        :param pulumi.Input[float] validation_fraction: The fraction of the input data that is to be used to validate the Model.
        """
        pulumi.set(__self__, "key", key)
        if test_fraction is not None:
            pulumi.set(__self__, "test_fraction", test_fraction)
        if training_fraction is not None:
            pulumi.set(__self__, "training_fraction", training_fraction)
        if validation_fraction is not None:
            pulumi.set(__self__, "validation_fraction", validation_fraction)

    @property
    @pulumi.getter
    def key(self) -> pulumi.Input[str]:
        """
        The key is a name of one of the Dataset's data columns. The values of the key (the values in the column) must be in RFC 3339 `date-time` format, where `time-offset` = `"Z"` (e.g. 1985-04-12T23:20:50.52Z). If for a piece of data the key is not present or has an invalid value, that piece is ignored by the pipeline.
        """
        return pulumi.get(self, "key")

    @key.setter
    def key(self, value: pulumi.Input[str]):
        pulumi.set(self, "key", value)

    @property
    @pulumi.getter(name="testFraction")
    def test_fraction(self) -> Optional[pulumi.Input[float]]:
        """
        The fraction of the input data that is to be used to evaluate the Model.
        """
        return pulumi.get(self, "test_fraction")

    @test_fraction.setter
    def test_fraction(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "test_fraction", value)

    @property
    @pulumi.getter(name="trainingFraction")
    def training_fraction(self) -> Optional[pulumi.Input[float]]:
        """
        The fraction of the input data that is to be used to train the Model.
        """
        return pulumi.get(self, "training_fraction")

    @training_fraction.setter
    def training_fraction(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "training_fraction", value)

    @property
    @pulumi.getter(name="validationFraction")
    def validation_fraction(self) -> Optional[pulumi.Input[float]]:
        """
        The fraction of the input data that is to be used to validate the Model.
        """
        return pulumi.get(self, "validation_fraction")

    @validation_fraction.setter
    def validation_fraction(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "validation_fraction", value)


@pulumi.input_type
class GoogleCloudAiplatformV1TrainingConfigArgs:
    def __init__(__self__, *,
                 timeout_training_milli_hours: Optional[pulumi.Input[str]] = None):
        """
        CMLE training config. For every active learning labeling iteration, system will train a machine learning model on CMLE. The trained model will be used by data sampling algorithm to select DataItems.
        :param pulumi.Input[str] timeout_training_milli_hours: The timeout hours for the CMLE training job, expressed in milli hours i.e. 1,000 value in this field means 1 hour.
        """
        if timeout_training_milli_hours is not None:
            pulumi.set(__self__, "timeout_training_milli_hours", timeout_training_milli_hours)

    @property
    @pulumi.getter(name="timeoutTrainingMilliHours")
    def timeout_training_milli_hours(self) -> Optional[pulumi.Input[str]]:
        """
        The timeout hours for the CMLE training job, expressed in milli hours i.e. 1,000 value in this field means 1 hour.
        """
        return pulumi.get(self, "timeout_training_milli_hours")

    @timeout_training_milli_hours.setter
    def timeout_training_milli_hours(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "timeout_training_milli_hours", value)


@pulumi.input_type
class GoogleCloudAiplatformV1UnmanagedContainerModelArgs:
    def __init__(__self__, *,
                 artifact_uri: Optional[pulumi.Input[str]] = None,
                 container_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelContainerSpecArgs']] = None,
                 predict_schemata: Optional[pulumi.Input['GoogleCloudAiplatformV1PredictSchemataArgs']] = None):
        """
        Contains model information necessary to perform batch prediction without requiring a full model import.
        :param pulumi.Input[str] artifact_uri: The path to the directory containing the Model artifact and any of its supporting files.
        :param pulumi.Input['GoogleCloudAiplatformV1ModelContainerSpecArgs'] container_spec: Input only. The specification of the container that is to be used when deploying this Model.
        :param pulumi.Input['GoogleCloudAiplatformV1PredictSchemataArgs'] predict_schemata: Contains the schemata used in Model's predictions and explanations
        """
        if artifact_uri is not None:
            pulumi.set(__self__, "artifact_uri", artifact_uri)
        if container_spec is not None:
            pulumi.set(__self__, "container_spec", container_spec)
        if predict_schemata is not None:
            pulumi.set(__self__, "predict_schemata", predict_schemata)

    @property
    @pulumi.getter(name="artifactUri")
    def artifact_uri(self) -> Optional[pulumi.Input[str]]:
        """
        The path to the directory containing the Model artifact and any of its supporting files.
        """
        return pulumi.get(self, "artifact_uri")

    @artifact_uri.setter
    def artifact_uri(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "artifact_uri", value)

    @property
    @pulumi.getter(name="containerSpec")
    def container_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ModelContainerSpecArgs']]:
        """
        Input only. The specification of the container that is to be used when deploying this Model.
        """
        return pulumi.get(self, "container_spec")

    @container_spec.setter
    def container_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ModelContainerSpecArgs']]):
        pulumi.set(self, "container_spec", value)

    @property
    @pulumi.getter(name="predictSchemata")
    def predict_schemata(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1PredictSchemataArgs']]:
        """
        Contains the schemata used in Model's predictions and explanations
        """
        return pulumi.get(self, "predict_schemata")

    @predict_schemata.setter
    def predict_schemata(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1PredictSchemataArgs']]):
        pulumi.set(self, "predict_schemata", value)


@pulumi.input_type
class GoogleCloudAiplatformV1ValueArgs:
    def __init__(__self__, *,
                 double_value: Optional[pulumi.Input[float]] = None,
                 int_value: Optional[pulumi.Input[str]] = None,
                 string_value: Optional[pulumi.Input[str]] = None):
        """
        Value is the value of the field.
        :param pulumi.Input[float] double_value: A double value.
        :param pulumi.Input[str] int_value: An integer value.
        :param pulumi.Input[str] string_value: A string value.
        """
        if double_value is not None:
            pulumi.set(__self__, "double_value", double_value)
        if int_value is not None:
            pulumi.set(__self__, "int_value", int_value)
        if string_value is not None:
            pulumi.set(__self__, "string_value", string_value)

    @property
    @pulumi.getter(name="doubleValue")
    def double_value(self) -> Optional[pulumi.Input[float]]:
        """
        A double value.
        """
        return pulumi.get(self, "double_value")

    @double_value.setter
    def double_value(self, value: Optional[pulumi.Input[float]]):
        pulumi.set(self, "double_value", value)

    @property
    @pulumi.getter(name="intValue")
    def int_value(self) -> Optional[pulumi.Input[str]]:
        """
        An integer value.
        """
        return pulumi.get(self, "int_value")

    @int_value.setter
    def int_value(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "int_value", value)

    @property
    @pulumi.getter(name="stringValue")
    def string_value(self) -> Optional[pulumi.Input[str]]:
        """
        A string value.
        """
        return pulumi.get(self, "string_value")

    @string_value.setter
    def string_value(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "string_value", value)


@pulumi.input_type
class GoogleCloudAiplatformV1WorkerPoolSpecArgs:
    def __init__(__self__, *,
                 container_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1ContainerSpecArgs']] = None,
                 disk_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1DiskSpecArgs']] = None,
                 machine_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1MachineSpecArgs']] = None,
                 nfs_mounts: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1NfsMountArgs']]]] = None,
                 python_package_spec: Optional[pulumi.Input['GoogleCloudAiplatformV1PythonPackageSpecArgs']] = None,
                 replica_count: Optional[pulumi.Input[str]] = None):
        """
        Represents the spec of a worker pool in a job.
        :param pulumi.Input['GoogleCloudAiplatformV1ContainerSpecArgs'] container_spec: The custom container task.
        :param pulumi.Input['GoogleCloudAiplatformV1DiskSpecArgs'] disk_spec: Disk spec.
        :param pulumi.Input['GoogleCloudAiplatformV1MachineSpecArgs'] machine_spec: Optional. Immutable. The specification of a single machine.
        :param pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1NfsMountArgs']]] nfs_mounts: Optional. List of NFS mount spec.
        :param pulumi.Input['GoogleCloudAiplatformV1PythonPackageSpecArgs'] python_package_spec: The Python packaged task.
        :param pulumi.Input[str] replica_count: Optional. The number of worker replicas to use for this worker pool.
        """
        if container_spec is not None:
            pulumi.set(__self__, "container_spec", container_spec)
        if disk_spec is not None:
            pulumi.set(__self__, "disk_spec", disk_spec)
        if machine_spec is not None:
            pulumi.set(__self__, "machine_spec", machine_spec)
        if nfs_mounts is not None:
            pulumi.set(__self__, "nfs_mounts", nfs_mounts)
        if python_package_spec is not None:
            pulumi.set(__self__, "python_package_spec", python_package_spec)
        if replica_count is not None:
            pulumi.set(__self__, "replica_count", replica_count)

    @property
    @pulumi.getter(name="containerSpec")
    def container_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1ContainerSpecArgs']]:
        """
        The custom container task.
        """
        return pulumi.get(self, "container_spec")

    @container_spec.setter
    def container_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1ContainerSpecArgs']]):
        pulumi.set(self, "container_spec", value)

    @property
    @pulumi.getter(name="diskSpec")
    def disk_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1DiskSpecArgs']]:
        """
        Disk spec.
        """
        return pulumi.get(self, "disk_spec")

    @disk_spec.setter
    def disk_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1DiskSpecArgs']]):
        pulumi.set(self, "disk_spec", value)

    @property
    @pulumi.getter(name="machineSpec")
    def machine_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1MachineSpecArgs']]:
        """
        Optional. Immutable. The specification of a single machine.
        """
        return pulumi.get(self, "machine_spec")

    @machine_spec.setter
    def machine_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1MachineSpecArgs']]):
        pulumi.set(self, "machine_spec", value)

    @property
    @pulumi.getter(name="nfsMounts")
    def nfs_mounts(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1NfsMountArgs']]]]:
        """
        Optional. List of NFS mount spec.
        """
        return pulumi.get(self, "nfs_mounts")

    @nfs_mounts.setter
    def nfs_mounts(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['GoogleCloudAiplatformV1NfsMountArgs']]]]):
        pulumi.set(self, "nfs_mounts", value)

    @property
    @pulumi.getter(name="pythonPackageSpec")
    def python_package_spec(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1PythonPackageSpecArgs']]:
        """
        The Python packaged task.
        """
        return pulumi.get(self, "python_package_spec")

    @python_package_spec.setter
    def python_package_spec(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1PythonPackageSpecArgs']]):
        pulumi.set(self, "python_package_spec", value)

    @property
    @pulumi.getter(name="replicaCount")
    def replica_count(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. The number of worker replicas to use for this worker pool.
        """
        return pulumi.get(self, "replica_count")

    @replica_count.setter
    def replica_count(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "replica_count", value)


@pulumi.input_type
class GoogleCloudAiplatformV1XraiAttributionArgs:
    def __init__(__self__, *,
                 step_count: pulumi.Input[int],
                 blur_baseline_config: Optional[pulumi.Input['GoogleCloudAiplatformV1BlurBaselineConfigArgs']] = None,
                 smooth_grad_config: Optional[pulumi.Input['GoogleCloudAiplatformV1SmoothGradConfigArgs']] = None):
        """
        An explanation method that redistributes Integrated Gradients attributions to segmented regions, taking advantage of the model's fully differentiable structure. Refer to this paper for more details: https://arxiv.org/abs/1906.02825 Supported only by image Models.
        :param pulumi.Input[int] step_count: The number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is met within the desired error range. Valid range of its value is [1, 100], inclusively.
        :param pulumi.Input['GoogleCloudAiplatformV1BlurBaselineConfigArgs'] blur_baseline_config: Config for XRAI with blur baseline. When enabled, a linear path from the maximally blurred image to the input image is created. Using a blurred baseline instead of zero (black image) is motivated by the BlurIG approach explained here: https://arxiv.org/abs/2004.03383
        :param pulumi.Input['GoogleCloudAiplatformV1SmoothGradConfigArgs'] smooth_grad_config: Config for SmoothGrad approximation of gradients. When enabled, the gradients are approximated by averaging the gradients from noisy samples in the vicinity of the inputs. Adding noise can help improve the computed gradients. Refer to this paper for more details: https://arxiv.org/pdf/1706.03825.pdf
        """
        pulumi.set(__self__, "step_count", step_count)
        if blur_baseline_config is not None:
            pulumi.set(__self__, "blur_baseline_config", blur_baseline_config)
        if smooth_grad_config is not None:
            pulumi.set(__self__, "smooth_grad_config", smooth_grad_config)

    @property
    @pulumi.getter(name="stepCount")
    def step_count(self) -> pulumi.Input[int]:
        """
        The number of steps for approximating the path integral. A good value to start is 50 and gradually increase until the sum to diff property is met within the desired error range. Valid range of its value is [1, 100], inclusively.
        """
        return pulumi.get(self, "step_count")

    @step_count.setter
    def step_count(self, value: pulumi.Input[int]):
        pulumi.set(self, "step_count", value)

    @property
    @pulumi.getter(name="blurBaselineConfig")
    def blur_baseline_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1BlurBaselineConfigArgs']]:
        """
        Config for XRAI with blur baseline. When enabled, a linear path from the maximally blurred image to the input image is created. Using a blurred baseline instead of zero (black image) is motivated by the BlurIG approach explained here: https://arxiv.org/abs/2004.03383
        """
        return pulumi.get(self, "blur_baseline_config")

    @blur_baseline_config.setter
    def blur_baseline_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1BlurBaselineConfigArgs']]):
        pulumi.set(self, "blur_baseline_config", value)

    @property
    @pulumi.getter(name="smoothGradConfig")
    def smooth_grad_config(self) -> Optional[pulumi.Input['GoogleCloudAiplatformV1SmoothGradConfigArgs']]:
        """
        Config for SmoothGrad approximation of gradients. When enabled, the gradients are approximated by averaging the gradients from noisy samples in the vicinity of the inputs. Adding noise can help improve the computed gradients. Refer to this paper for more details: https://arxiv.org/pdf/1706.03825.pdf
        """
        return pulumi.get(self, "smooth_grad_config")

    @smooth_grad_config.setter
    def smooth_grad_config(self, value: Optional[pulumi.Input['GoogleCloudAiplatformV1SmoothGradConfigArgs']]):
        pulumi.set(self, "smooth_grad_config", value)


@pulumi.input_type
class GoogleIamV1BindingArgs:
    def __init__(__self__, *,
                 condition: Optional[pulumi.Input['GoogleTypeExprArgs']] = None,
                 members: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]] = None,
                 role: Optional[pulumi.Input[str]] = None):
        """
        Associates `members`, or principals, with a `role`.
        :param pulumi.Input['GoogleTypeExprArgs'] condition: The condition that is associated with this binding. If the condition evaluates to `true`, then this binding applies to the current request. If the condition evaluates to `false`, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding. To learn which resources support conditions in their IAM policies, see the [IAM documentation](https://cloud.google.com/iam/help/conditions/resource-policies).
        :param pulumi.Input[Sequence[pulumi.Input[str]]] members: Specifies the principals requesting access for a Google Cloud resource. `members` can have the following values: * `allUsers`: A special identifier that represents anyone who is on the internet; with or without a Google account. * `allAuthenticatedUsers`: A special identifier that represents anyone who is authenticated with a Google account or a service account. Does not include identities that come from external identity providers (IdPs) through identity federation. * `user:{emailid}`: An email address that represents a specific Google account. For example, `alice@example.com` . * `serviceAccount:{emailid}`: An email address that represents a Google service account. For example, `my-other-app@appspot.gserviceaccount.com`. * `serviceAccount:{projectid}.svc.id.goog[{namespace}/{kubernetes-sa}]`: An identifier for a [Kubernetes service account](https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts). For example, `my-project.svc.id.goog[my-namespace/my-kubernetes-sa]`. * `group:{emailid}`: An email address that represents a Google group. For example, `admins@example.com`. * `domain:{domain}`: The G Suite domain (primary) that represents all the users of that domain. For example, `google.com` or `example.com`. * `deleted:user:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a user that has been recently deleted. For example, `alice@example.com?uid=123456789012345678901`. If the user is recovered, this value reverts to `user:{emailid}` and the recovered user retains the role in the binding. * `deleted:serviceAccount:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, `my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901`. If the service account is undeleted, this value reverts to `serviceAccount:{emailid}` and the undeleted service account retains the role in the binding. * `deleted:group:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, `admins@example.com?uid=123456789012345678901`. If the group is recovered, this value reverts to `group:{emailid}` and the recovered group retains the role in the binding.
        :param pulumi.Input[str] role: Role that is assigned to the list of `members`, or principals. For example, `roles/viewer`, `roles/editor`, or `roles/owner`.
        """
        if condition is not None:
            pulumi.set(__self__, "condition", condition)
        if members is not None:
            pulumi.set(__self__, "members", members)
        if role is not None:
            pulumi.set(__self__, "role", role)

    @property
    @pulumi.getter
    def condition(self) -> Optional[pulumi.Input['GoogleTypeExprArgs']]:
        """
        The condition that is associated with this binding. If the condition evaluates to `true`, then this binding applies to the current request. If the condition evaluates to `false`, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding. To learn which resources support conditions in their IAM policies, see the [IAM documentation](https://cloud.google.com/iam/help/conditions/resource-policies).
        """
        return pulumi.get(self, "condition")

    @condition.setter
    def condition(self, value: Optional[pulumi.Input['GoogleTypeExprArgs']]):
        pulumi.set(self, "condition", value)

    @property
    @pulumi.getter
    def members(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]:
        """
        Specifies the principals requesting access for a Google Cloud resource. `members` can have the following values: * `allUsers`: A special identifier that represents anyone who is on the internet; with or without a Google account. * `allAuthenticatedUsers`: A special identifier that represents anyone who is authenticated with a Google account or a service account. Does not include identities that come from external identity providers (IdPs) through identity federation. * `user:{emailid}`: An email address that represents a specific Google account. For example, `alice@example.com` . * `serviceAccount:{emailid}`: An email address that represents a Google service account. For example, `my-other-app@appspot.gserviceaccount.com`. * `serviceAccount:{projectid}.svc.id.goog[{namespace}/{kubernetes-sa}]`: An identifier for a [Kubernetes service account](https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts). For example, `my-project.svc.id.goog[my-namespace/my-kubernetes-sa]`. * `group:{emailid}`: An email address that represents a Google group. For example, `admins@example.com`. * `domain:{domain}`: The G Suite domain (primary) that represents all the users of that domain. For example, `google.com` or `example.com`. * `deleted:user:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a user that has been recently deleted. For example, `alice@example.com?uid=123456789012345678901`. If the user is recovered, this value reverts to `user:{emailid}` and the recovered user retains the role in the binding. * `deleted:serviceAccount:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, `my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901`. If the service account is undeleted, this value reverts to `serviceAccount:{emailid}` and the undeleted service account retains the role in the binding. * `deleted:group:{emailid}?uid={uniqueid}`: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, `admins@example.com?uid=123456789012345678901`. If the group is recovered, this value reverts to `group:{emailid}` and the recovered group retains the role in the binding.
        """
        return pulumi.get(self, "members")

    @members.setter
    def members(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[str]]]]):
        pulumi.set(self, "members", value)

    @property
    @pulumi.getter
    def role(self) -> Optional[pulumi.Input[str]]:
        """
        Role that is assigned to the list of `members`, or principals. For example, `roles/viewer`, `roles/editor`, or `roles/owner`.
        """
        return pulumi.get(self, "role")

    @role.setter
    def role(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "role", value)


@pulumi.input_type
class GoogleTypeExprArgs:
    def __init__(__self__, *,
                 description: Optional[pulumi.Input[str]] = None,
                 expression: Optional[pulumi.Input[str]] = None,
                 location: Optional[pulumi.Input[str]] = None,
                 title: Optional[pulumi.Input[str]] = None):
        """
        Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec. Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
        :param pulumi.Input[str] description: Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
        :param pulumi.Input[str] expression: Textual representation of an expression in Common Expression Language syntax.
        :param pulumi.Input[str] location: Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
        :param pulumi.Input[str] title: Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
        """
        if description is not None:
            pulumi.set(__self__, "description", description)
        if expression is not None:
            pulumi.set(__self__, "expression", expression)
        if location is not None:
            pulumi.set(__self__, "location", location)
        if title is not None:
            pulumi.set(__self__, "title", title)

    @property
    @pulumi.getter
    def description(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
        """
        return pulumi.get(self, "description")

    @description.setter
    def description(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "description", value)

    @property
    @pulumi.getter
    def expression(self) -> Optional[pulumi.Input[str]]:
        """
        Textual representation of an expression in Common Expression Language syntax.
        """
        return pulumi.get(self, "expression")

    @expression.setter
    def expression(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "expression", value)

    @property
    @pulumi.getter
    def location(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
        """
        return pulumi.get(self, "location")

    @location.setter
    def location(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "location", value)

    @property
    @pulumi.getter
    def title(self) -> Optional[pulumi.Input[str]]:
        """
        Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
        """
        return pulumi.get(self, "title")

    @title.setter
    def title(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "title", value)


